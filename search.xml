<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[完全没时间复习の编译原理笔记]]></title>
    <url>%2F2020%2F07%2F02%2F%E5%AE%8C%E5%85%A8%E6%B2%A1%E6%97%B6%E9%97%B4%E5%A4%8D%E4%B9%A0%E3%81%AE%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Cp1，2 符号串集合A的闭包：A*=A做若干次笛卡尔积产生的结果 推导：顺着箭头推理；规约：逆着箭头推理 句型：一个文法可以推导出的中间结果 句子：文法可以推导出的一个最终结果（终结符构成） 语言：L(G)，句子集合 文法等价：可以推导出的句子等价 0型文法：所有推导规则中，左部都含有非终结符。 1型文法（上下文有关文法）：规则左部都含非终结符，且左部长度小于等于右部（短-&gt;长） 2型文法（上下文无关文法）：规则的左部固定是一个非终结符 右线性3型文法：规则格式为：A-&gt;aB | a（A，B是非终结符） 左线性3型文法：A -&gt; Ba | a 最左推导：每一步推导都选择最左边的非终结符展开。又叫规范推导。 无二义性 = 最左、最右推导唯一 自上而下分析：推导；自下而上：归约。都需要回溯。 判断短语、句柄：第二章ppt43~45页 如果一个文法不会产生一个单独的\(\epsilon\)，则\(\epsilon\)规则没有必要存在。 小结 第一章重点介绍了编译程序定义、编译过程、编译方式、编译程序结构和高级程序语言的分类以及相关的应用。提出的基本概念是源语言、源程序、目标语言、目标程序、程序等价、翻译方式、遍/趟、翻译程序、汇编程序、编译程序和解释程序。 重点掌握的概念：①编译程序；②编译过程；③编译程序结构；④编译程序生成方法。 第二章重点： ① 设计一个已知语言的文法； ② 确定已知文法定义的语言； ③ 求句型的短语、直接短语和句柄。 ④ 文法二义性判定。 Cp3 词法分析 单词可以用正规文法（3型文法，即A-&gt;aB | a或A-&gt;Ba | a）表示 等价于正则表达式（正规式） 正规式、正规文法转换：ppt第三章p11, 12 正规式转DFA（正规式转NFA，NFA转DFA，DFA最小化）：https://www.jianshu.com/p/de84d27264cc 最小化DFA时，第一步的集合划分是划分为终态和非终态 Cp4 自顶向下语法分析 求first(A)：看A能推导出的所有句子中 ，最左边的第一个字符是啥。 如果所有非终结符右部的FIRST集合两两相交为空，可以使用确定的最左推导。 求follow(A)：看在该文法的所有句型中，A的右边可以出现哪个终结符。 如果对非终结符A，有一条空规则，则A的FOLLOW集合和A的非空右部的FIRST集合两两相交为空，可以使用确定的最左推导。 画递归子程序图：第四章p31 求select集：看LL(0)分析表中，某一规则的所在的列名，就是该规则的select集中的项目 LL(1)分析流程： 判断是否是LL(1) ：求fisrt，follow，select集 做出分析表 根据分析表，做出分析栈进行分析。具体看 LR(0) / SLR(1)分析表中，Sn表示shift到状态n；Rn表示使用规则n归约。 四种分析方法：https://www.cnblogs.com/henuliulei/p/10872483.html LR(0)：一遇到终态就归约，一遇到First集就移进。可能出现移进/归约冲突 SLR(1)：如果LR(0)出现冲突，但可以通过follow集解决，则满足SLR(1)。见到First集就移进，见到终态先看Follow集，与Follow集对应的项目归约，其它报错。 LR(1)：比起LR(0)多增加了向前搜索符号。 LALR(1)：利用同心集合并的办法，不会产生“移进－归约”冲突 ，但会产生“归约－归约”冲突。 处理能力上，LR(0) &lt; SLR(1) &lt; LALR(1) &lt; LR(1) 总体关系：LR(0) –&gt;SLR(1); LR(1) –&gt; LALR(1) 重点！分析过程：需要状态栈（初始0）、符号栈（初始#）。 看当前状态i下，遇到当前输入串的字符，做什么动作。 如果是归约（Rn），则符号栈出栈右部，入栈左部 如果是移进（Sn），则符号栈入栈输入串元素。状态栈入栈n Cp7. 语法制导语义分析 属性含义： 综合属性从下到上，继承属性从上到下 从上往下的：in 从下往上的：type，val 语义计算：指的是画出带标注的语法树 分析方法：可以分别基于L、S属性，分别进行自顶向下（LL1）或自底向上（LR）的分析方法 L属性自顶向下（第七章p35） L属性自底向上（第七章p37） S属性自顶向下（第七章p35） S属性自顶向下（第七章p35） Cp10. 代码优化 局部优化：先划分基本块，然后在块内优化]]></content>
  </entry>
  <entry>
    <title><![CDATA[简简单单のSQL笔记！]]></title>
    <url>%2F2020%2F04%2F12%2F%E7%AE%80%E7%AE%80%E5%8D%95%E5%8D%95%E3%81%AESQL%E7%AC%94%E8%AE%B0%EF%BC%81%2F</url>
    <content type="text"><![CDATA[### 安装 sudo apt-get install mysql-server 创建 建立数据库：CREATE DATABASE database_name; 选择数据库：use database_name; 创建表：CREATE TABLE &lt;表名&gt; (字段1 类型1, 字段2 类型2...) 其中，常用类型可以是：INT(N), CHAR(N), VARCHAR(N) 增删查改 假设现在有一学生表students，班级表classes： 查询 SELECT &lt;字段&gt; FROM &lt;表名&gt;;，其中如果查询全部字段，用*表示：SELECT * FROM students; 不查询全部列（字段）：SELECT 列1, 列2, 列3 FROM &lt;表名&gt;;。这种查询又称为投影查询。 在查询结果中为列重命名：SELECT 列1 新名字1, 列2 新名字2, 列3 新名字3 FROM &lt;表名&gt;; SELECT &lt;表达式&gt;;可以直接计算出表达式的值，这种语句可用来测试对数据库的链接。例如SELECT 1; 条件查询：SELECT &lt;字段&gt; FROM &lt;表名&gt; WHERE &lt;条件1&gt; AND/OR &lt;条件2&gt; ...;。例如：SELECT * FROM students WHERE score&gt;=80 AND sex='M'; 上述条件语句中，可以使用AND, OR, NOT表示逻辑关系 相等：=，不等：&lt;&gt;，相似：LIKE ‘abc%’，其中%表示通配符 可以使用WHERE BETWEEN 50 AND 100表示在50~100之间 为查询结果排序：SELECT ... ORDER BY &lt;字段名&gt;;（ORDER必须放在WHERE后面） 倒序：SELECT ... ORDER BY &lt;字段名&gt; DESC; 指定多个排序依据：SELECT ... ORDER BY 字段1, 字段2;，同样，对每个排序依据都可以指定是否倒序。 长语句可以拆开为多行： 1234SELECT id, name, gender, scoreFROM studentsWHERE class_id = 1 AND score BETWEEN 50 AND 100ORDER BY score DESC, gender; 把查询结果分页显示：SELECT ... LIMIT 3 OFFSET 0，表示从第0条查询结果开始，显示3条记录。可以简写为：SELECT ... LIMIT 0, 3 聚合函数 sql提供了内建函数，可以辅助查询。例如想知道表的长度，可以使用：SELECT COUNT(*) FROM students。这里COUNT就是一个聚合函数。 其他类似函数还有：SUM, AVG, MAX, MIN 查询某一个字段，不同值对应的记录数目：SELECT COUNT(*) FROM students GROUP BY class_id。这里GROUP BY相当于把students根据class_id的不同值，拆分（分组）成若干张表，再用COUNT计算各自的数目 查询多张表 SELECT * FROM 表1, 表2;，这样会首先对两张表做笛卡尔积再查询。 如果两个表里有同名字段，默认将其合并 如果不想合并，想当做不同的字段处理，可以使用为两张表的同名字段，各自设置别名的办法。当然，这里表名也可以设置别名。 12345678SELECT s.id sid, s.name, s.gender, s.score, c.id cid, c.name cnameFROM students s, classes c 连接多表内容并查询：太长了，直接看这里8 修改数据 插入一条记录：INSERT INTO &lt;表名&gt; (列1，列2，列3) VALUES (值1，值2，值3); 插入多条：INSERT INTO &lt;表名&gt; (列1，列2，列3) VALUES (值1，值2，值3) (值4，值5，值6) ...; 修改符合条件的记录：UPDATE &lt;表名&gt; SET 字段1=值1, 字段2=值2, ... WHERE &lt;条件&gt;; 删除符合条件的记录：DELETE FROM &lt;表名&gt; WHERE &lt;条件&gt;;]]></content>
      <tags>
        <tag>编程</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学完可能就再也不看了的sml学习笔记！]]></title>
    <url>%2F2020%2F03%2F31%2F%E5%AD%A6%E5%AE%8C%E5%8F%AF%E8%83%BD%E5%B0%B1%E5%86%8D%E4%B9%9F%E4%B8%8D%E7%9C%8B%E4%BA%86%E7%9A%84sml%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%81%2F</url>
    <content type="text"><![CDATA[与其他语言不同的 声明变量：val x= 0: int; 负号用的是~而不是- 整除是div而不是// 不等于是&lt;&gt;而不是!= 字符串连接用^而不是+ char型变量是#&quot;a&quot;而不是'a' 用x::L表示数组L前加一个元素x 用L1 @ L2表示两个数组连接 类型与模式匹配 浮点型保留字是real而不是float 数组各个元素必须相同，元组各个元素可以不同 每个变量都有类型，例如int，real等。复杂结构的变量也有类型，例如int list是整形数组，int list list是二维数组 元组的类型表示：例如(&quot;string&quot;, 123, 123)的类型为string * int * int。即各个元素类型使用*连接 进一步，例如上述元组组成的数组类型为(string * int * int) list 模式匹配可以理解为类型判断。 例如val x::L = [1, 2, 3]进行了模式匹配，判断将1赋值给x，[2, 3]赋值给L。所有赋值语句都会进行模式匹配的操作。 函数 函数调用在无歧义的情况下，可以不加括号（例如只传一个参数时） 语句以分号结尾，但是函数定义时，函数体内无分号。即整个函数只有一条语句，只不过为了方便拆成多行。 函数示例： 123456fun function_name (args1: int, args2: int) : int = let val local_varible1 = 0 val local_varible1 = 1 in .... (* 这条语句的值就是返回值*) end 此外，函数也可以采取递归定义的方式。具体来说，就是对函数传递不同参数的情况分别进行定义。典型的例子是斐波那契数列： 123fun fibo 0 = 0 | fibo 1 = 1 | fibo n:int = fibo (n-1) + fibo (n-2); 这里注意，函数仍然只有一条语句，只有在最后有一个分号。|的作用表示或，这种方式相当于用|代替了if sml中，函数尽可能地写成递归形式，非常简洁。如果出现无法使用一条语句完成函数的情况，仍然可以使用let...in...end。例如： 1 匿名函数：用fn和=&gt;标志。例如：fn (a:int, b:int) =&gt; a+b]]></content>
      <tags>
        <tag>编程</tag>
        <tag>函数式编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简简单单的java笔记！]]></title>
    <url>%2F2020%2F03%2F20%2F%E7%AE%80%E7%AE%80%E5%8D%95%E5%8D%95%E7%9A%84java%E7%AC%94%E8%AE%B0%EF%BC%81%2F</url>
    <content type="text"><![CDATA[踩过的坑 .length用来求数组长，.length()用来求字符串长 二维数组中，每一行长度可以不同：例如char [][] arr = new char [3][]只初始化了二维数组长度为3，但没有限制每个子数组的长度。 不能基于函数返回类型来重载函数，即函数名和形参相同，但是return不同不算重载。也就是说，只要形参和 一个java文件中只能有一个public class，该类中的public static void main方法是该程序入口 变长参数：如果若干是同一类型，长度可变，可以使用变长参数：int func(String... args)，变长参数接收后，会直接将其包装为数组。 数组 Arrays包含了许多数组方法，需要额外导入：import java.util.Arrays 快速打印一位数组：System.out.println(Arrays.toString(arr))。 快速打印多维数组：System.out.println(Arrays.deepToString(arr)) 排序：Arrays.sort(arr)；查找：Arrays.binarySearch(arr, value) String 有三种类：String, StringBuffer, StringBuilder，其中StringBuider最快，但不适用于多线程；String最慢。 替换字符或子串：s.replace(&quot;old string&quot;, &quot;new string&quot;) 替换符合正则表达式的部分：s.replaceAll(&quot;regex&quot;, &quot;newstring&quot;) replaceAll支持正则表达式的分组，例如： 123String s = "hello world";s.replaceAll("(he)llo", "***$1***"); //使用$1引用"he"。顺便，$0始终代表匹配到的整个表达式// 输出结果为："***he*** world" 类和对象 修饰符 类方法修饰符： public：可被外部类访问； private：不允许外部类访问（包括自己的子类），只允许在该类内部访问 static：该方法无需实例化类就可调用。因此static方法内不能使用this protected：只允许同一个包内访问,但是跨包时允许子类访问 修饰符也可以修饰除了方法之外的成员变量，表示的含义不变。例如static变量可以通过Class.member访问。 也可以通过修饰符修饰块，并在块中执行语句：static { ... }。 静态块的特点是只执行一次，可以用来提高运行效率。但是在块中声明的变量无法被外部发现，因此尽量在静态块外部声明static变量，在块内部赋值： 123456class A &#123; static int v; static &#123; v = 1; &#125;&#125; 一个类的静态变量，被其所有实例共享 构造函数 构造函数不能有返回值，直接定义为ClassName(){...}，若有返回值（哪怕是void类型），java都会认为该方法是一个普通方法，而不是构造函数。例如void ClassName(){...} 继承 继承：class Son extends Father java不支持继承多个父类 子类的构造方法开头会自动调用super()来首先构造父类。但是如果父类的默认构造方法有参数，则需要手动调用super(args) 一个父类类型的变量也可以指向它儿子的实例。但是这种情况下，只能调用父类包含的那些方法。 覆盖 如果子类想重写覆盖父类方法，使用@Override装饰。 其实不用@Override也可以覆盖，但是为了可读性，仍然建议使用 final, static, private方法不可被覆盖（但可以重新定义一个同名的） 即使父类方法被覆盖，仍然可以通过super.method()调用被覆盖的方法 父类引用可以指向子类。但是此时，如果父子类中有同名方法，会优先调用父类的方法。 抽象 带有抽象方法的必须是抽象类，抽象类不能被实例化 static方法不能是抽象方法 接口 可以认为是一组成员变量和抽象方法的集合，接口里的方法默认全都是抽象的，只描述了方法的原型，借口可以理解为一种“约定”或“规范”。用interface {...}定义。 接口的作用是用来被类实现。class A implemants interface1, interface2 {...}。只有A中包含了接口要求的变量和方法，才算成功实现。 接口也可以继承。 一个接口的类型也可以指向实现了它的对象。但是这种情况下，只能调用该对象中，该接口包含的那些方法。 异常处理 捕获异常：try {...} catch (Exception1 e1) {...} catch (Exception2 e2) {...} finally {...}，其中finally中的语句一定会被执行。 finally一定会执行，甚至是在别的块中已经有return语句的情况下，在返回之前也会先执行finally。但是，实际上方法已经准备返回了，在finally中更改返回值是无效的。 抛出异常：throw new Exception 如果一个方法可能抛出异常，需要在方法头中声明：int method() throws Exception1, Exception2 自定义的异常需要继承java.lang.Exception 文件IO File f = new File(&quot;filename&quot;)，类比文件指针 读取：Scanner s = new Scanner(f); s.nextLine(); 写入：PrintWriter o = new PrintWriter(f); o.println(&quot;xxxx&quot;); File, PrintWriter都在java.io中 也可以直接使用FileReader(filename), FileWriter(filename) 泛型 泛型就是类似List &lt;int&gt;这样的类型，不再是一个特定的类型，而是也可以接受“参数”（这里的&lt;int&gt;） 泛型的写法一定程度上是为了可读性。在编译过程中会进行去泛型化。 定义: 123class Myclass &lt;T&gt; &#123; private T key;&#125; 这里的T（称为泛型参数），在实例化的时候，就会被替换为一个具体的类型，例如new Mycalss &lt;String&gt; (); 泛型参数不能是基本类型（即不能是开头小写的类型例如int, float） 泛型的实例不能创建数组 T不能被static修饰（不能在static field中出现）： 即不能作为static方法的返回类型，以及static变量的类型 可以有多个泛型参数。class Myclass &lt;T, K&gt; 泛型实例化：Myclass c &lt;String&gt; = new Mycalss &lt;String&gt; (); 实例化时，可以为T添加约束，例如： 允许任意类作为泛型参数：Myclass c &lt;?&gt; = new Mycalss &lt;String&gt; ();其中?是通配符。 只允许String的父类作为泛型参数：Myclass c &lt;? super String&gt; = new Mycalss &lt;String&gt; (); 只允许String的子类作为泛型参数：Myclass c &lt;? extends String&gt; = new Mycalss &lt;String&gt; (); 泛型方法 写法：例如：static &lt;T&gt; void method(List &lt;T&gt; list) {} 与泛型类无关，普通类里也可以有泛型方法。 和泛型类类似，在调用时，传递的参数会将T具体化（有一点像函数柯里化和闭包），例如a.method(new List&lt;String&gt;())，执行时T会被自动替换为String 可以针对不同的类型调用不同的参数，但只需要同一套代码 实现类似python的写法时可以用泛型。例如，一个方法可以打印数组中国所有元素，而不必关心数组是什么类型，这时就可以使用泛型。 反射 所有类都有一个Class对象，通过Class clz = a.getClass()获取。 也可以通过类名创建：Class.forName(&quot;java.lang.String&quot;) 使用Method[] ms = clz.getMethods();获取该类的全部方法 使用Constructor[] ms = clz.getConstructors();获取该类的全部构造方法 疑惑]]></content>
      <tags>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[whyの学习笔记:Fast.ai]]></title>
    <url>%2F2020%2F01%2F11%2Fwhy%E3%81%AE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Fast-ai%2F</url>
    <content type="text"><![CDATA[番外篇 - FastAI学习记录 概览 导入：使用from fastai import *或者from fastai.vision import *的形式：导入所有内容，并自动导入numpy as np, pandas as pd, matplotlib.pyplot as plt等。 使用from fastai.basics import *：导入主要功能。 基本包之间的依赖关系： 训练 Learner Learner：控制整个训练流程的类，将PyTorch模型，数据集，优化器和损失函数绑定在一起。 定义：函数原型见官方文档。其中定义需要传入的数据是DataBunch，模型是nn.Module 参数说明：带wd字样是与权重衰减有关的参数。true_wd=True时，将会使用wd的衰减率进行权重衰减（否则使用L2正则化进行权重衰减）；bn_wd表示是否对bn层权重进行衰减（默认衰减）。path是模型文件保存路径，train_bn=True时，始终训练bn层中的参数（即使网络冻结） metrics可以传入一个列表，fastai有许多预定义的metric，可以看这里 重要方法 Learner相关操作 Learner.init(func)：使用func初始化模型参数。可以直接从torch.nn.init模块中获取。 Learner.save：保存模型和优化器参数。 Learner.load：从文件加载模型和优化器，只能在使用了save之后使用。 Learner.export：保存learner的当前状态到文件。 Learner.load_learner：从文件learner，只能在使用了export之后使用。 model_summary(n)：使用n个字符宽度的表格打印出Learner中模型的摘要。 Learner.freeze(), Learner.unfreeze()：冻结、解冻整个模型。freeze时网络的最后一层不冻结。 Learner.freeze_to(n)：冻结网络的前n层，并解冻之后的所有层。 训练 Learner.fit在start_lr和end_lr之间搜索合适的学习率，如果stop_div，当loss开始发散时停止查找。 1Learner.lr_find(start_lr=1e-07, end_lr=10, num_it=100, stop_div=True, wd=None) Learner.fit：用于拟合数据。传参callbacks=C，来修改训练过程，其中C是Callback类，指定了训练中运行的代码。 Learner.fit_one_cycle 1Learner.fit_one_cycle(cyc_len, max_lr, moms=(0.95, 0.85), div_factor=25.0, pct_start:float=0.3, final_div:float=None, wd:float=None, callbacks:Optional[Collection[Callback]]=None, tot_epochs:int=None, start_epoch:int=None) 使用OneCycle策略进行训练：lr首先从0增大到max_lr，然后逐渐衰减。其中动量参数moms，分割因子div_factor，权重衰减wd。 使用mixup策略：.mixup()即在视觉任务中将不同类别的图像混合进行训练，可以大大提高效率。原理和技术细节看这里 clip_grad(clip:float=0.1)：在该Learner训练时允许使用梯度裁剪。 参数使用16位精度，以提高运算效率（32位同理）： 1to_fp16(loss_scale:float=None, max_noskip:int=1000, dynamic:bool=True, clip:float=None, flat_master:bool=False, max_scale:float=16777216) 观察内部信息：以分类任务为例，使用interp = ClassificationInterpretation.from_learner(learn)建立一个Interpretation，然后使用interp.plot_top_losses展示出loss最高的几个样本。除此之外，还可以画出混淆矩阵。 数据相关操作 Learner.dl(DataType)：获得Learner中指定数据集的迭代器 获得单个样本输出：Learner.predict(item) 获得Learner中的整个数据集输出： 1Learner.get_preds(ds_type:DatasetType=&lt;DatasetType.Valid: 2&gt;, with_loss:bool=False, n_batch:Optional[int]=None, pbar:Union[MasterBar, ProgressBar, NoneType]=None) → List[Tensor] ，其中dstype是保存在Learner中要预测的数据集类型，可以取DatasetType中的值，默认是验证集。 - 获得`Learner`中某个数据集中一个`batch`的输出： 1pred_batch(ds_type:DatasetType=&lt;DatasetType.Valid: 2&gt;, batch:Tuple=None, reconstruct:bool=False) → List[Tensor] 获得任意数据集输出： 1Learner.validate(dl=None, callbacks=None, metrics=None) 获得Learner中数据集真实的labels（如果有）： 1Learner.show_results(ds_type=&lt;DatasetType.Valid: 2&gt;, rows:int=5, **kwargs) 内存管理：康这里 重要成员 Learner.model：使用的模型 Learner.data：使用的数据集 子类 cnn_learner：用于迁移学习的Learner，导入内置的预训练模型并进行微调。定义看这里 Recorder 是一个Callback类，用于记录训练信息。创建Learner时会自动创建Learner.recorder 绘图：Recorder.plot(skip_start=10,skip_end=5,suggestion=False,return_fig=None)一般用来在lr_find执行后调用来绘制学习率。 绘制loss：Recorder.plot_losses(skip_start=10, skip_end=5, return_fig=None)。 绘制评测标准：Recorder.plot_metrics(skip_start=10, skip_end=5, return_fig=None)。 绘制lr：Recorder.plot-lr(show_moms, skip_start=10, skip_end=5, return_fig=None)。使用show_moms来绘制动量参数的图像。 使用回调函数实现更多功能，在每个epoch/batch训练开始/结束时执行指定的回调函数：康这里 内建函数 手动fit：fit(epochs, learner, callbacks:Optional[Collection[Callback]], metrics:OptMetrics=None)手动fit一个Learner，需要自己创建优化器（而使用Learner.fit会自动创建）。 训练一个epoch：train_epoch(model:Module, dl:DataLoader, opt:Optimizer, loss_func:LossFunction 验证模型： 1validate(model:Module, dl:DataLoader, loss_func:OptLossFunc=None, cb_handler:Optional[CallbackHandler]=None, pbar:Union[MasterBar, ProgressBar, NoneType]=None, average=True, n_batch:Optional[int]=None) → Iterator[Tuple[IntOrTensor, Ellipsis]] Callback 在可以传递Callback的地方（例如定义learner时）重构Callback类，可以完成在指定的时间实现指定的操作。 Callback常用的定义方式： 123456789@dataclassclass MyCallback(Callback): learn:Learnerdef on_train_begin: passdef on_epoch_begin: pass 即以Learner作为参数。这样的Callback可以直接在定义一个Learner时作为callback_fns参数传递，这样，该Learner即可在每个epoch/train开始时，调用该Callback中相应的方法。更多定制的方法康这里 DataBunch 用于传递给Learner进行训练的数据块。 - 定义：康这里。其中传入的train_dl, valid_dl等，就是pytorch中的DataLoader 重要方法 show_batch(rows, ds_type, DatasetType)：以rows行显示出一个batch中的数据 此外，也内置了返回数据集中一个batch或一个item的方法。 模型保存与加载：save('xxx.pkl');load_data(path, file, bs, val_bs, num_workers...) 导入数据 图像分类数据：ImageDataBunch.from_folder(path)，约定path下有train, valid, test三个文件夹，并且train和valid中的每个子文件夹均以label命名。（同pytroch中的ImageFolder）， 用于分类任务 使用csv存储label信息的图像数据。其中导入的csv格式是： 123file name, labelxxxx, xxx.... 导入方法：ImageDataBunch.from_csv(path, folder, label_delim, csv_labels, valid_pct, fn_col, label_col, suffix, delimiter, header,)。fastai根据path/csv_labels中的内容，将path/folder中的数据导入，并以valid_pct的百分比自动制作验证集。导入时会为csv文件中的文件名自动加上suffix后缀，并将label_delim作为分隔符，拆分csv文件中label的内容。其中默认fn_col=0, label_col=1，表示csv文件中，第一列保存文件名，第二列保存label。 - 表格类型，或者nlp任务的数据，使用TextList或TabularList导入。详细内容康这里 手动逐步导入：分四步走。 第一步：首先使用ItemList.from_folder(path, extensions, presort)（或者ItemList的子类，例如ImageList等）将path中扩展名在extensions中的文件导入为列表。其中，presort=True时，将对文件名进行排序，否则将以随机顺序返回。返回的列表是可迭代对象，也可以通过下标索引。 另外，也可以从csv或pandas的DataFrame中导入数据。导入以后，也可以手动过滤数据 第二步：分割训练和验证集。即使不使用验证集，也需要划分一个空验证集出来（使用split_none()）。划分时一般直接随机划分就好，使用split_by_rand_pct(valid_pct)。更多划分方法康这里 第三步： 为每个数据加上label。分类任务就一般使用label_from_folder()，直接根据文件夹名作为类别名。在更复杂的任务中，可以根据正则匹配、自定义函数等等来获得label。详情看这里 可选步骤：数据变换 对应pytorch中transform。作为参数传递给DataBunch或ItemList。详细写法在后面介绍。 添加测试集：使用add_test(item_list)或add_test_folder(folder_name)。注意，这里的测试集默认没有label 第四步：转化为DataBUnch，使用.databunch()即可 内置数据集：通过URLs.xxxx查看地址，使用untar_data(URLs.xxx)来下载并解压数据集 数据增强 定义DataBunch时传入的dl_tfms是一个二元组，分别包含对训练集和验证集使用的数据增强方法的列表。例如([*rand_pad(4, 32), flip_lr(p=0.5)], []) 要注意，在使用某些变换方法例如rotate(x, degrees)时，第一个参数x是要处理的图像。要想将其传入DataBunch或用apply_transforms应用于图像，则需要跳过传入x，直接手动指定degrees的值，否则会出错。即rotate(30)是错误的，正确写法是rotate(degrees=30) 使用get_transforms(args)可以获取包含常用处理方法的Transform对象，用默认值就很OK，已经包含了水平、垂直翻转、旋转等等。返回的值直接就是一个二元组，可以直接传给DataBunch。细节看这里。其中内置了各种各样好玩的变换。 特别注意：对于例如rand_pad(pad, size)的方法，会返回形如[[transform1, transform2]]形式的返回值，但我们想要的是[transform1, transform2]的形式，所以在使用时需要写成[rand_pad(pad, size)[0], 其它]或者使用星号将其逐元素取出：[*rand_pad(pad, size), 其它]。 构建与训练GAN 直接使用内置架构 GANLearner：GANLearner（data，generator, critic, gen_loss_func, crit_loss_func, switcher, gen_first, switch_eval, show_img, clip, **learn_kwargs） 其中gen_loss_func是一个函数，接受三个输入：(fake_pred, fake, real_label) crit_loss_func接受两个参数:(real_pred, fake_pred) gen_first=True表示一开始模型先训练G switcher是一个回调函数，描述如何在两个网络之间切换训练 clip是WGAN中梯度裁剪的值。 show_img=True，则会在每个epoch最后，展示一张生成图片。 wgan(data, G, D, switcher, clip) 分别构建G和D GANLearner.from_learners(learn_gen, learn_crit, switcher, weights_gen, **learn_kwargs) 分别单独构造判别和生成网络的learner，可以定制更细节的过程。例如两个网络分别执行哪些回调函数等。 一些好用的API open_image(path)，直接展示图片。格式为fastai内置的图片格式，可以方便地应用各种操作。 额外的网络层：Lambda(func)，可以创建一个由函数func指定的layer。 Flatten(full)，将Tensor展开到一个维度上(batch_size x n)。就是图像分类网络的倒数第二层。当full=True时，无把batchsize的维度也展开。 PoolFlatten()，先展开，后应用AdaptivePool。 Debugger()，加在模型中，运行时会自动在Debugger处启动pdb parallel(function, list)，相当于并行版的map，可以提高运行速度 在不需要label标记的gan数据集中，可以直接使用label_from_func(noop)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>fastai</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[考完就不看の数值分析笔记！]]></title>
    <url>%2F2019%2F11%2F27%2F%E8%80%83%E5%AE%8C%E5%B0%B1%E4%B8%8D%E7%9C%8B%E3%81%AE%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0%EF%BC%81%2F</url>
    <content type="text"><![CDATA[#1 误差 问题背景：真实值为\(x\)，近似值为\(x&#39;\) 误差分类： 模型误差：将实际问题抽象为数学问题（建模）过程中的误差。 观测误差：字面意思，测量误差 截断误差：出现和无穷项有关的地方（比如求级数、求极限），实际中不可能真的取极限 舍入误差：四舍五入。 误差：\(e = x&#39; - x\)（注意是近似值减去真实值，别搞混鸭） 绝对误差：\(\varepsilon(x) = |e|\) 绝对误差限：绝对误差的一个上界\(\eta\)，越小越好（越精确）。 相对误差：\(\varepsilon(x) = \frac{x&#39; - x}{x&#39;}\)。之所欲是除以\(x&#39;\)是因为精确值\(x\)往往不知道。 相对误差限：\(\varepsilon_r = \frac\eta{|x&#39;|}\)，即绝对误差限相对于\(x&#39;\)的值。 有效数字： 约定以下都把近似值转化为\(0.xxx\times 10^m\)的形式。 如果该近似值是四舍五入得来的，则从第一位非0数字到末尾数字都是有效数字。 若不是四舍五入得到，则看使得绝对误差满足式子\(|e|\leqslant 0.5 \times 10^{m-l} = \eta\)的l，那么该近似值的有效数字位数即为\(l\)。 有效数字\(\rightarrow\)相对误差限：若近似值有n位有效数字，则其相对误差限为\(\frac1{2a_1}\times 10^{-n+1}\) 相对误差限\(\rightarrow\)有效数字：已知相对误差限为\(\frac1{2(a_1+1)}\times 10^{-n+1}\)，则该近似值有n位有效数字。 各种算术操作的误差限： \(\eta(x&#39; \pm y&#39;) = \eta(x&#39;) + \eta(y&#39;)\) \(\eta(x&#39;y&#39;) = |y&#39;|\eta(x&#39;) + |x&#39;|\eta(y&#39;)\) \(\eta(x&#39; / y&#39;) = \frac{|y&#39;|\eta(x&#39;) + |x&#39;|\eta(y&#39;)}{y&#39;^2}\) #2 插值方法 问题背景： 给定函数\(f(x)\)上的一组点\((x_0, y_0) ...(x_n, y_n)\)（这些已知点称为插值节点），构造插值函数\(g(x)\)逼近\(f(x)\)。 拉格朗日插值 线性插值：使用直线拟合函数；抛物插值：使用抛物线拟合函数 拉格朗日插值公式及余项： \[ \left\{ \begin{aligned} p_n(x) = \sum_{i=0}^n [y_i I_i(x)] = \sum_{i=0}^n \left(y_i \prod_{j=0, j\neq i}^n \frac{x-x_j}{x_i-x_j} \right) \\ R(x) = f(x) - p_n(x) = \frac{f^{n+1}(\xi)}{(n+1)!}\prod_{i=0}^n(x-x_i), \xi \in (a, b) \end{aligned} \right. \] 其中，\(I_i(x)\)称为基函数，可以看到，拉格朗日插值公式就是将函数变为了基函数的线性组合，并且根据基函数的表达式可知，\(I_i(x_i)\)非零，但在其他所有插值节点上基函数的函数值都为0. 缺点非常明显，基函数和全部插值节点相关，因此每当增加一个插值节点，全部\(I(x)\)都要重算。 牛顿插值 试图构造插值函数\(N_n(x)\)，使得\(f(x) = \sum_i c_i \varphi_i\)，其中\(\varphi_i(x)=(x-x_0)(x-x_1)...(x-x_{i-1})\)为插值基函数。 即基函数为：用\(x\)依次与第\(i\)个以前的插值节点的\(x_j\)作差，并将这些差连乘得到的结果。由此可见\(\varphi_i\)具有继承性：\(\varphi_i = (x-x_{i-1})\varphi_{i-1},\varphi_0=1\) 这里先暂停一下，引入一个概念 差商，我们令记号\(f[x_0, x_1, ...x_k]\)叫做\(f(x)\)在\(x_0, ...x_k\)这\(k+1\)个点的\(k\)阶差商。其中： \[ \begin{cases} f[x0, x1] = \frac{f(x_0)-f(x_1)}{x_0-x_1} \\ f[x_0, x_1, ...x_k] = \frac{f[x_0,...x_{k-1}]-f[x_1, ...x_k]}{x_0-x_k} \end{cases} \] 差商有两个性质:\(f[x_0, x_1, ...x_k]\)是这n+1个点的函数值的线性组合： \[f[x_0 , x_1... x_n ] = ∑_{k=0} \frac{f(x_k)}{(x_k - x_0 )(x_k - x_1 )...(x_k - x_n )}\] 差商与节点顺序无关。即\(f[ x _0 , x _1 , x _2 ] = f[ x _1 , x _0 , x _2 ]\) 回到牛顿插值公式，我们要求的\(c_i\)，就是\(f[x_0, ...x_i]\) 分段线性插值 划分为多段小区间，每段区间上用直线拟合。 在区间\([x_i, x_{i+1}]\)上，\(f(x)\approx S_1(x)=y_i\frac{x-x_{i+1}}{x_i-x_{i+1}}+y_{i+1}\frac{x-x_{i}}{x_{i+1}-x_{i}}\) Hermite插值 优点：插值上不仅函数值相等，导数值也相等。 公式：已知\(2n+2\)个量，其中既有\(f(x_i)\)也有\(f&#39;(x_i)\)，则\(H_{2n+1} = \alpha_0f(x_0)+\alpha_1f(x_1)+...\alpha_nf(x_n) +\beta_0f(x_0)+\beta_1f(x_1)+...\beta_nf(x_n)\)，\(\alpha, \beta\)是基函数。 #3.1 数值积分 问题背景： 求未知函数的积分。 机械求积方法 思想：转化为函数值的加权平均，即： \[\int_a^bf(x) = \sum_{i=1}^n A_if(x_i)dx \tag{1}\] 梯形公式： \(\int_a^b f(x)dx= \frac{b-a}{2}(f(a)+f(b))\) 中矩形公式： \(\int_a^b f(x)dx=(b-a)f(\frac{a+b}{2})\) \(simpson\)公式： \(\int_a^b f(x)dx=\frac{b-a}6 [f(a) + 4f(\frac{a+b}2)+f(b)]\) 代数精度：我们称余项\(R=\int_a^b f(x) - \sum_{i=1}^n A_if(x_i)\)，若机械求积公式对\(m\)次（或更低次）多项式函数精确成立(\(R=0\))，但对\(m+1\)次多项式近似成立，则机械求积的代数精度为\(m\)。 例如梯形公式在当\(f(x)=1, f(x)=x\)时，都精确成立，但对\(f(x)=x^2\)就不成立，因此它的代数精度为\(1\)。 构造机械求积公式 对机械求积公式而言，重点在于怎么确式\((1)\)中各个\(A\)的值。这里使用两种方法： 解方程组。有几个未知量，就列几个方程，假设求积公式对前\(n\)次多项式都精确成立。但缺点是方程组过大时难以求解。 插值。使用插值法找到\(f(x)\)的近似函数\(p_n(x)\)（这个近似函数是一个多项式函数），对\(p_n(x)\)求积作为近似值。推导可得\(A_i = \int_a^bI_i(x)dx,I_i(x)\)是插值基函数。 注意到，由于插值得到的函数\(p_n(x)\)就是一个多项式，而代数精度也是通过令\(f(x)\)为多项式函数来进行计算，因此插值法得到的积分精确度至少为\(n\)。 因此我们规定，一个机械求积公式\(\sum_{i=1}^n A_if(x_i)dx\)是插值型公式\(\Leftrightarrow\)其代数精度为\(n\)（这里注意n是机械求积公式的项数） Newton-cotes求积 将区间\([a, b]\)等分为n份，在等分点做拉格朗日插值得到的公式就是Newton-cotes求积公式： \[ \left\{ \begin{aligned} \int_a^bf(x) = (b-a)\sum_{k=1}^nC_kf(x_k) \\ C_k = {(-1)^{n-k}\over n k!(n-k)!}\int_0^n \prod_{j=0,j\ne k}^n(t-j)dt \end{aligned} \right. \] 由于是插值型公式，所以Newton-cotes公式至少有\(n\)次代数精度。\(n\)为偶数时，代数精度为\(n+1\) \(n=1\)时即为梯形公式，\(n=2\)时为simpson公式。 缺点：即使n取很大，余项也不一定收敛到0. 稳定性分析 假设计算\(f(x_k)\)有一定舍入误差\(\varepsilon_k\)，且假设\(C_k\)的计算没有误差。那么\(\varepsilon_k\)会导致积分产生的最终误差为\(\varepsilon \leqslant max|\varepsilon_k|(b-a)\sum C_k\)，是无界的，因此高阶Newton-cotes公式是不稳定的。 复化求积 将区间\([a, b]\)分成\(n\)个小区间，在每个区间上构造低阶求积公式，并累加得到结果。其中令小区间宽度为\(h = \frac{a-b}{n}\) 复化梯形公式： \(T_n = \int_a^b f(x)dx= \frac h2[f(a) + f(b) + 2\sum_{k=1}^{n-1}f(x_k)]\) 复化\(simpson\)公式：要求\(n\)为偶数，令\(n=2m\)，则： \(S_n = \int_a^b f(x)dx= \frac h3[f(a) + f(b) + 4\sum_{k=1}^{m}f(x_{2k-1}) + 2\sum_{k=1}^{m-1}f(x_{2k})]\) 复化\(cotes\)公式：要求\(n=4m, m\in Z^+\)： \(C_n = \int_a^b f(x)dx= \frac {4h}{90}[7f(a) + 7f(b) + 32\sum_{k=1}^{m}f(x_{4k-3}) + 12\sum_{k=1}^{m}f(x_{4k-2}) + 32\sum_{k=1}^{m}f(x_{4k-1}) + 14\sum_{k=1}^{m-1}f(x_{4k})]\) 它们的余项（截断误差）分别为： \[ \begin{cases} R_T = -\frac{b-a}{12}h^2f&#39;&#39;(\eta), \eta \in [a, b] \\ R_S = -\frac{b-a}{180}(\frac h2)^4f^{(4)}(\eta), \eta \in [a, b] \\ R_C = -\frac{2(b-a)}{945}(\frac h4)^6f^{(6)}(\eta), \eta \in [a, b] \end{cases} \] 要注意，使用上述余项公式分别要满足\(f(x)\)在\([a, b]\)上分别有连续的\(2,3,6\)阶导函数。这三个复化公式中，\(cotes\)的效率最高。 Romberg求积 使用渐进的方式，先取\(n=1\)，使用复化求积计算积分，如果精度不达要求，再取\(n=2, 4, 8...\)，直到精度达到要求。 这就要求找到从\(n=n_1\)到\(n=2n_1\)的递推关系，以便可以利用上一步的计算结果进行计算。推导可得，梯形复化积分的递推公式分别为： \[ T_{2n} = \frac12T_n + \frac h2\sum_{k=0}^{n-1}f(x_{k+\frac 12}) \] 为了加速收敛，我们在此基础上，利用梯形公式的余项推导得出\(T_{2n}\)的误差，并用这个误差修正\(T_{2n}\)，得到修正后的公式： \[\bar{T} = \frac{4}{3} T_{2n} - \frac{1}{3} T_{n}\] 这里经过验证发现\((p75)\)，这个修正后的梯形公式，竟然和\(Simpson\)公式，也就是\(S_n\)的结果完全相等，即\(\bar{T}=S_n\)。类似地，\(S_{2n}\)用同样的方法得到的修正公式与\(C_{n}\)又恰好相等。而这个过程中，精度越来越高。 因此，我们把精度最高的\(C_n\)变步长（渐进）求积，并用误差修正后的公式作为\(romberg\)公式的最终值，也就是： \[R_n = \frac{64}{63}C_{2n}-\frac{1}{63}C_n\] 可以看到，\(romberg\)公式将之前所讲的方法都集于一身。在计算\(romberg\)公式时，一般就按照\(T\rightarrow S \rightarrow C\rightarrow R\)的过程一步一步求积，直到精度满足要求。 #3.1 数值微分 差商微分：\(f&#39;(x) \approx \frac{f(x+h)-f(x-h)}{2h}\)，截断误差约为\(O(h^2)\) 插值微分：拉格朗日插值拟合\(f(x)\)，再对插值后的多项式求导。 #4 解常微分方程数值解法 背景：给定常微分方程，求\(y(x)\)。数值解法中，不直接求\(y(x)\)，而是求区间\([a, b]\)内一系列点\(x_0, x_1, ...x_n\)上函数的近似值\(y(k)\approx y_k\)。 欧拉法：使用差商\(f(x_n, y_n) = \frac{y_{n+1}-y_n}{h}\)来代替\(f&#39;(x_n)\)，并使用该公式，结合\(y_0\)的初值，递推得到\([y_0, y_1...y_n]\)的近似解。即\(y_{k+1} = y_k + hf(x_k, y_k)\) 隐式欧拉法：使用前向的差商\(f(x_{n+1}, y_{n+1}) = \frac{y({n+1})-y(n)}{h}\)来代替\(y&#39;(n+1)\)。即: \[y_{k+1} = y_k + hf(x_{k+1}, y_{k+1}) \tag{1}\] 二步欧拉法：对公式\((1)\)右边的\(y_{k+1}\)，使用显式的欧拉法进行估计的估计值\(\bar{y}_{k+1}=y_k + hf(x_k, y_k)\)进行代替。推导可得，这样得到的公式相当于使用中心差商\(\frac{y(x_{n+1})-y(x_{n-1})}{2h}\)来代替导数。即：\(y_{n+1} = y_{n-1} + 2hf(x_n, y_n)\)。在实际计算时，需要同时知道\(y_0, y_1\)，其中\(y_1\)可以使用欧拉法求得。 精度：若在进行递推时，直到\(y_{n+1}\)的表达式，其表达式中的量都是精确的（注意，不是\(y_{n+1}\)本身没有误差，而是其表达式中的\(y_n,x_n\)之类的量没有误差），并且其局截断误差为\(O(h^{p+1})\)，则称该公式的精度为\(p\)阶。 可以证明，欧拉法、隐式欧拉法、二步欧拉法的精度分别为\(1, 1, 2\)阶。 改进欧拉法 梯形公式+欧拉法：不直接使用差商来进行递推，而使用定积分来递推。即为：\(y(x_{n+1}) = y(x_n) + \int_{x_n}^{x_{n+1}}y&#39; dx\)。这里的积分公式我们使用梯形公式。 即： \[y_{n+1} = y_n + \frac{h}{2}[f(x_n, y_n) + f(x_{n+1}, y_{n+1})] \tag{2}\] 可以证明，这样得到的公式精度为2阶，且恰好等于欧拉法和隐式欧拉法的算术平均。 改进的欧拉法：仍然使用预报+校正的思想：仍然使用\(\bar{y}_{k+1}=y_k + hf(x_k, y_k)\)代替公式\((2)\)中右侧的\(y_{n+1}\)得到改进的公式。精度为2阶。代入\(\bar y_{k+1}\)后，可以等价于： \[ \begin{cases} y_p = y_n + hf(x_n+y_n) \\ y_c = y_n + hf(x_{n+1}, y_p) \\ y_{n+1} = \frac{y_p+y_c}{2} \end{cases} \] 龙格-库塔法：更高阶精度的公式 公式为\(y(x+{n+1}) = y(x_n) + hy&#39;(\xi)\)，其中\(y&#39;(\xi)\)在实际计算中无法直接确定，而是使用多个点上\(y&#39;\)近似值的适当组合来得到。 二阶龙格-库塔法：在\([x_n, x_{n+1}]\)范围中取两个点，且设这两点上导数\(y&#39;(x)\)近似值为\(k_1, k_2\)，则二阶公式为： \[ \begin{cases} k_1 = f(x_n, y_n) \\ k_2 = f(x_n+ph, y_n+phk_1) \\ y_{n+1} = y_n + h[(1-\lambda k_1 + \lambda k_2)], where\; \lambda p = \frac{1}{2} \end{cases} \] 其中 \(\lambda, p\)可以在满足条件的情况下任取，这样可以得到无穷多的公式，他们的精度都是二阶。 改进欧拉法：当\(\lambda = \frac{1}{2}, p=1\)时，龙格-库塔法退化为之前的改进的欧拉法。 变型的欧拉法：当\(\lambda = 1, p=\frac12\)时，龙格-库塔法退化为变型的欧拉法。 \(n\)阶龙格-库塔法：使用\(n\)个点上导数近似值的线性组合来代替\(y&#39;(\xi)\) 当\(n=3\)，可以得到库塔公式： \[ \left\{\begin{array}{l}{y_{n+1}=y_{n}+\frac{1}{6}\left(k_{1}+2 k_{2}+2 k_{3}+k_{4}\right)} \\ {k_{1}=h f\left(x_{n}, y_{n}\right)} \\ {k_{2}=h f\left(x_{n}+\frac{h}{2}, y_{n}+\frac{k_{1}}{2}\right)} \\ {k_{3}=h f\left(x_{n}+\frac{h}{2}, y_{n}+\frac{k_{2}}{2}\right)} \\ {k_{4}=h f\left(x_{n}+h, y_{n}+k_{3}\right)}\end{array}\right. \] 最常用的是四阶龙格-库塔公式。其中，标准四阶公式为： \[ \left\{\begin{array}{l}{y_{n+1}=y_{n}+\frac{1}{6}\left(k_{1}+2 k_{2}+2 k_{3}+k_{4}\right)} \\ {k_{1}=h f\left(x_{n}, y_{n}\right)} \\ {k_{2}=h f\left(x_{n}+\frac{h}{2}, y_{n}+\frac{k_{1}}{2}\right)} \\ {k_{3}=h f\left(x_{n}+\frac{h}{2}, y_{n}+\frac{k_{2}}{2}\right)} \\ {k_{4}=h f\left(x_{n}+h, y_{n}+k_{3}\right)}\end{array}\right. \] 另一个四阶公式（也叫吉尔公式）为： \[ \left\{\begin{array}{l}{y_{n+1}=y_{n}+\frac{1}{6}\left[k_{1}+(2-\sqrt{2}) k_{2}+(2+\sqrt{2}) k_{3}+k_{4}\right]} \\ {k_{1}=h f\left(x_{n}, y_{n}\right)} \\ {k_{2}=h f\left(x_{n}+\frac{h}{2}, y_{n}+\frac{k_{1}}{2}\right)} \\ {k_{3}=h f\left(x_{n}+\frac{h}{2}, y_{n}+\frac{\sqrt{2}-1}{2} k_{1}+\frac{2-\sqrt{2}}{2} k_{2}\right)} \\ {k_{4}=h f\left(x_{n}+h, y_{n}-\frac{\sqrt{2}}{2} k_{2}+\frac{2+\sqrt{2}}{2} k_{3}\right)}\end{array}\right. \] 变步长龙格-库塔法：假设使用\(p\)阶公式，则从\(x_n\)开始，首先以步长\(h\)求出一个近似值\(y_{n+1}^{(h)}\)，然后步长折半，跨两步从\(x_n\rightarrow x_{n+1}\)，得到\(y_{n+1}^{(h/2)}\)…不断折半，直到精度满足要求。其中，步长折半前后两次，计算结果的偏差为：$= 1{2^P-1} |y_{n+1}^{(h/2)}- y_{n+1}^{(h)}| $ ，用于检验精度是否满足要求。 收敛性与稳定性 单步法：公式满足\(y_{n+1} = y_n + h\varphi(x_n, y_n, h)\)的方法（前面所讲的方法都是单步法）。 收敛性：当\(n \rightarrow \infty\)，\(y_n \rightarrow y(x_n)\)。对单步法，收敛等价于\(n \rightarrow \infty\)时，满足截断误差为0，也等价于函数\(\varphi\)满足李普希兹条件：$|(x, y, h) - (x, {y}, h)| L_|y - {y}| $ 稳定性：先空着叭复习再写 边值问题 问题背景：解二阶方程\(y’&#39; + p(x)y&#39; + q(x)y = r(x), a&lt;x&lt;b\)，且已知条件为边值条件（而非初值条件）。可能有以下三类： $y(a) = , y(b) = $ $y’(a) = , y’(b) = $ \(y&#39;(a) - \alpha_0y(a) = \alpha_1, y&#39;(b) - \beta_0y(b) = \beta_1\) ，其中 $ _0,_00, _0+_0&gt;0 $ 解法： 仍然划分\([a, b]\)为\(N\)等分，在每个划分节点\(x_i = x_0 + ih\)（\(h\)为步长\(\frac{b-a}{N}\)）上代入得到\(N+1\)个方程，并使用一阶、二阶差商代替一阶、二阶导数：$y’, y’’ $，得到一个方程组，然后再求解。 方程求根 问题背景：求方程\(f(x) = 0\)的根。 根的隔离：在\(f(x)\)的定义域上寻找\([a, b]\)使得在该区间内只有一个根（零点）。 找根：二分法（不能求偶数重根） 迭代法：先找出一个精度较低的根的近似值，然后逐渐迭代。以求\(x^3 - x - 1 = 0\)的根为例，具体方法是根据方程写出\(x\)的一个表达式： \[x=\sqrt[3]{x+1} \tag{1}\] 式子右边可以含\(x\)。假设要求\(x_0=1.5\)附近的根，那么将\(x_0\)带入式\((1)\)，求得\(x_1\)，再将\(x_1\)带入求得\(x_2\)…以此类推，不断迭代，精度不断提高。直到\(|x_{k+1} - x_k| &lt; \varepsilon\)时停止迭代。 不动点迭代：从另一个视角看，上述过程就是把\(f(x)=0 \rightarrow x =g(x)\)，然后求\(g(x)\)的不动点。 收敛性：啊我生病了这部分没上课（ 加速收敛（艾特肯方法）：这部分也没了复习再说吧 牛顿法：使用一阶泰勒展开来拟合\(f(x)\)。选定一个初值\(x=x_0\)，使用公式\(x_{k+1} = x_k - \frac{f(x_k)}{f&#39;(x_k)}\)进行迭代。 牛顿法の收敛性：收敛速度是二阶。 牛顿下山法由于牛顿法初值选取时如果距离根过远，很可能导致发散。因此在牛顿法的迭代过程中，修改迭代公式为\(x_{k+1} = x_k - \lambda\frac{f(x_k)}{f&#39;(x_k)}\)（加入了一个权重\(\lambda\)）。这个\(\lambda\)的作用是，控制使得迭代过程中始终满足\(|f(x_{k+1})| &lt; |f(x_k)|\)（称为下山条件）。每一步迭代时，\(\lambda\)从\(1\)开始取，如果不满足下山条件，则\(\lambda\)减半，直到满足为止。 近似牛顿法：不求到，改为一个固定常数\(c\)：\(x_{k+1} = x_k - \frac{f(x_k)}{c}\) 弦截法：使用\(x_0, x_k\)两点的一阶差商代替导数：\(x_{k+1} = x_k - \frac{f(x_k)}{f(x_k) - f(x_0)}(x-x_0)\) 快速弦截法：使用\(x_{k-1}, x_k\)两点的一阶差商代替导数：\(x_{k+1} = x_k - \frac{f(x_k)}{f(x_k) - f(x_{k-1})}(x-x_{k-1})\) 抛物线法：这个不考，好.jpg 解线性方程组 向量的范数：\(||x||\)满足正定，齐次，三角不等式(\(| ||x|| - ||y|| | \leqslant ||x-y||\))。 \(||x||_1 = \sum|x_i|\) \(||x||_2 = \sqrt{\sum{|x_i|^2}}\) \(||x||_p = (\sum|x_i|^p)^{\frac{1}{p}}\) 向量的收敛：若有向量序列\(X={x_k}\)，其中每个\(x_k\)，都存在\(x&#39;\)，有\(\lim_{k\rightarrow \infty}|| x_k - x&#39; ||=0\)，则称向量序列\(X\)收敛。 矩阵范数： f范数：对\(A\in R^{n\times n}, |A|_f =(\sum_i \sum_j |a_{ij}|^{2})^{\frac12}\) 诱导范数：对\(A\in R^{n\times n}, x\in R^n,||x||=1\)，有：\(||A|| = \sup ||Ax||\)。也即\(|| Ax || \leqslant ||A|| \cdot ||x||\) 列和范数：\(||A||_1 = \max_j \sum_i |a_{ij}|\)(各列和的最大值) 谱范数：\(||A||_2 = \sqrt{\lambda_1},\lambda_1\)是\(AA^T\)的最大特征值。 无穷范数（行和范数）：各行和的最大值。 迭代法：从初始近似解向量开始迭代，使其逼近真实解。 高斯消元法：]]></content>
  </entry>
  <entry>
    <title><![CDATA[考完就不看の组原笔记！]]></title>
    <url>%2F2019%2F09%2F16%2F%E8%80%83%E5%AE%8C%E5%B0%B1%E4%B8%8D%E7%9C%8B%E3%81%AE%E7%BB%84%E5%8E%9F%E7%AC%94%E8%AE%B0%EF%BC%81%2F</url>
    <content type="text"><![CDATA[Cp1.Introduction 1.冯诺依曼结构 冯诺依曼硬件结构：主机＋外设＋总线 思想：程序存储在存储器中；按照指令地址访存并取出指令。 硬件系统的五大部分： 运算器：做实际的计算 控制器：产生控制信号（硬布线，微程序） 存储器：读、写两种模式。容量与地址线的数量相对应。 输入设备、输出设备 冯诺依曼软硬件之间的关系：相互依存，逻辑等效（有的功能既可以硬件实现，也可软件实现），协同发展。 计算机不同层次：自顶向下分别为：应用程序，高级语言，汇编语言，操作系统，指令集架构（软硬的分界线），微代码，硬件。 2.计算机系统性能的评价指标 机器字长：机器一次能处理的二进制位数。 总线宽度：数据总线一次传递的最大的信息位数。 主存容量＆存储带宽：带宽指主存的读写速度 CPU 的主频、时钟周期 外频：CPU 和主板之间同步的时钟频率 倍频：主频除以外频 CPI：执行一条指令需要的平均时钟周期数目（注意，单位不是时间，是周期数目）\(CPI=程序中所有指令的时钟周期之和 \div 程序中的指令总数＝程序中各类指令的CPI加权求和\) MIPS：每秒钟CPU能执行指令的总条数，单位是百万条每秒 全性能公式：\(MIPS = \frac{f}{CPI\times 10^6}\) 程序执行时间＝CPU时间＋IO时间＋访存时间＋排队时延 \(CPU时间＝指令总条数\times CPI / f＝\frac{指令总条数}{MIPS\times 10^6}\) Cp2.Data Representation 机器数：二进制串 源码、反码、补码是三种不同的表示方法 反码加法要回卷（进位位要加到末尾） 移码表示：在补码的基础上，符号位取反。 定点数：定点小数可以表示\([-1, 1-2^{9n}]\)内的小数；定点整数可以表示\([-2^n, 2^n-1]\)内整数。 浮点数：分开表示数据的范围和精度。格式：\(2^{阶码}\times 尾数\)，阶码和尾数均使用补码表示，各自带有自己的符号位。 IEEE754格式：规定了阶码和尾数各自的位长。只有一个符号位，表示整个真值的符号（第一位） 单精度：32bit，1位符号，8位阶码，后23位尾数； 双精度：64bit，1位符号，11位阶码，后52位尾数 另外，阶码在保存时，加上了偏移量。因此在解读阶码时，结果应该减去127（单精度）或1023（双精度） 另外，尾数部分，默认是1.xxxxx 给定IEEE754二进制表示，还原真值：\((-1)^{符号位}\times 2^{8位阶码-127} \times 1.后23位尾数\) 给定真值，写出IEEE浮点数：先转换为二进制，然后变成\((-1)^s \times 2^e \times 1.M\)的形式，则\(e+127\)为阶码，M为尾数。 特殊数字： 阶码为0，尾数为0：机器0 阶码为255，尾数为0：无穷(X/0) 阶码为255，尾数不为0：NaN（0/0） 数据校验 在信息位后加入\(r\)位校验位。 码距：在一种编码下，相邻合法编码之间的差值。 码距\(\geqslant e+1\)时，具有检测\(e\)个错误的能力 码距\(\geqslant 2t+1\)时，具有纠正\(t\)个错误的能力 码距\(\geqslant e+t+1\)时，具有检测\(e\)个错误，并纠正\(t\)个错误的能力 奇偶校验：校验位只有1位，描述信息位中1的个数是否是偶数/奇数。偶校验是各位异或，奇校验是各位异或后再取非。 奇偶校验的码距为2。 改进：双向校验：把一串二进制串拆成多个小段，叠成二维，在两个方向上进行奇偶校验。 CRC校验 CRC校验：信息位数\(k\)和校验位数r要求\(k+r\leqslant2^r - 1\) 发送方：给定生成多项式\(G\)，求校验码\(r\)：用待传输数据和\(G\)做模二运算，得到的余数即为校验码。 接收方校验：用收到的整个二进制串和\(G\)做模二除法，看余数不是0则说明有错。 当有1位错时，不同位出错得到的余数是不同的。因此可以根据具体的余数值纠错：具体做法是： 针对一个特定的\(G\)，首先要看，当左边第一位出错时，对应的余数是什么。这里假设是\(a&#39;\)。 接收方得到二进制串，做除法得到非零余数\(a\)后，看是否\(a=a&#39;\)，是则说明第一位出错，直接纠错。不是则接下来想办法让出错位移到第一位。 具体办法是：在\(a\)后补一个0，继续除\(G\)，得到的新余数后继续补0，继续除\(G\)…每补一次0，我们就把收到的二进制串循环左移1位，直到得到的余数为\(a&#39;\)。此刻，位于当前二进制串最左边的一位就是出错位。 海明校验 有一点与CRC相同，海明校验信息位数\(k\)和校验位数r要求\(k+r\leqslant2^r - 1\) 与CRC不同，海明校验是校验位穿插在信息位之间。校验位的位置为\(1, 2, 4...2^i\)： \[{\color{red}1}, {\color{red}2}, 3, {\color{red}4}, 5, 6, 7, {\color{red}8}, 9, 10, 11\] 校验位的计算：某个校验位是由某些信息位的偶校验（异或得到）。对第\(i\)位，假如该位是信息位，将\(i\)表示为尽量少的校验位的和，例如\(6=4+2\)，则将第6位加入到第2位和第4位的校验分组中。对所有的信息位都进行这样的操作，就可以得到各个校验位分别负责的校验分组。 （啊啊啊啊啊这里文字说不清楚啊 自己意会好惹 纠错：为了能够纠错，再设置\(r\)个指错字，每个指错字是一位校验位和它负责的分组进行异或得到。 Cp3.Calculator 补码加减法要丢掉进位。 补码求和补码(x+y) = 补码(x) + 补码(y) 有符号数溢出检测方法: 正+正=负；负+负=正时，即对加数符号位\(f_0, f_1\)以及结果符号位\(f_s\)，有\(f_0f_1\bar f_s + \bar f_0 \bar f_1 f_s =1\) 看最高一位数据位的进位和符号位的进位是否相同，不相同即为溢出。 使用双符号位，计算后两个符号位若不同，说明溢出。（本质和方法2相同） 无符号数溢出检测：加法看进位，有进位即为溢出。减法相反，进位为0说明溢出。 原码一位乘法：符号单独运算，数据的绝对值进行运算。与手工运算相比，区别是：得到部分积后，将部分积右移到乘数寄存器中（而不是左移）。 补码一位乘法：\((X\cdot Y)_补 = X_补\cdot \sum_i(y_{i+1} - y_i)2^{-i}\)。 具体来说，首先求出\(X, -X, Y\)的补码，然后对\(Y\)，首先令\(Y_{i+1}=0\)，然后从最低位开始到最高位，重复进行： 如果\(y_i = y_{i+1}\)，部分积\(+0\)，且部分积算数右移一位 如果\(y_{i+1}y_i = 10\)，部分积\(+X_补\)，且部分积算数右移一位 如果\(y_{i+1}y_i = 01\)，部分积\(+(-X_补)\)，且部分积算数右移一位 （啊啊啊啊这里也好乱 请直接康mooc后半部分的实例 原码乘法设计：使用与门阵列 补码一位乘法器：算前求补和算后求补。 定点除法器：恢复余数法，不恢复余数法。啊淦这里我不复习啦jojo！ 浮点数加减法： 规格化浮点数：尾数用双符号位，尾数的最高数据位和符号位不同。即尾数为\(00.1....., 11.0......\)的浮点数。 可以使用左移的方法规格化一个浮点数。这里注意左移时，阶码也要同步改变。 浮点数加减：分为五步：提出大的阶码，修改小阶码浮点数的尾数（对阶），尾数加减，规格化，舍入，溢出处理。 如果规格化后，阶码双符号位不同，说明发生了溢出。 本章难点：乘除法的计算过程。 Cp4.Storage 存储体系：\(CPU\rightarrow cache\rightarrow 主存\rightarrow 辅存(disk)\) 存储字长：一个存储单元包含的二进制位数。例32位系统的含义是，一个字长度为32。 数据对齐：例如在32位系统，双字长数据地址二进制串的末三位一定是\(000\)（长度为8字节，因此地址为8的整数倍） 数据不对齐存储：节省了空间，但增加了访存次数。 大端方式：地址从大到小存放数据，首地址是数据的最高字节地址。小段反之。 SRAM（静态存储单元）：啊啊啊啊啊模电杀我 这里太难了 存储扩展：位扩展：\(M \times N \rightarrow M\times kN\)。使用\(k\)个存储器，每个存储器负责二进制串中不同的\(N\)位。连接时，数据线分线，地址线和控制线直接连接。 字扩展：\(M \times N \rightarrow kM\times N\)。使用\(k\)个存储器，数据线直接相连，地址线由于cpu比存储器多，高位的多余的地址线连到片选译码器，片选译码器产生控制线连到片选段。 片选信号的作用：在字扩展中，每个芯片负责整个地址空间中的一部分（类比段式存储），片选用来选择是哪一片寄存器。例如，假如一共有8个芯片，那么需要3位片选信号(\(2^3 = 8\)) 多体交叉存储器：和字扩展类似，只不过使用低位作为片选信号（而非高位），即相邻地址在不同的芯片中，可以并行访问。这时需要为每个芯片配置地址寄存器。 对这种存储器，要想以流水线方式存取，需要满足：\(存储周期=总线传送周期\times 芯片数\)（即最后一个芯片中取出数据后，刚好可以再回到第一个芯片取数据） cache cpu访问cache以字为单位，cache和主存交换数据以块为单位。 读：cpu访问cache，如果命中，直接取；若不命中，则去主存取数据，并且主存将该数据所在的整个块都搬到cache中。 写：写穿：cpu同时向cache和主存写数据；写回：cpu只写cache，整个写操作完成后，再把cache数据写到主存。 地址映射：主存地址划分为二维：块地址+偏移地址。其中块地址由标记（该块是否在cache中）和索引（该块在cache哪个位置）组成。 cache的存储空间划分为若干行，每一行的大小可以存储主存中的一块数据。此外，每一行还有tag（数据是否在cache）,valid（该数据是否有效）,dirty（主存中数据是否最新）三个标志信息。 地址映射：由此可见，cache中一行和主存中一块相对应。具体的映射方式有： 全相连：主存中一块可映射到任一行。 直接相连：根据cache的行数\(n\)给主存分区，使得每个区里有\(n\)个块。主存中一个块在映射时，它是所在分区的第几块，就只能映射到cache的第几行。也就是说，现在把块地址再分为两级：分区地址和cache行。到时候就根据cache行地址去cache的对应行查找。这里有个问题，对cache某一行而言，有很多块都有可能向自己搬迁数据。那么在访问该行时，如果该行有数据，不能直接读，而是需要同时看要访问数据的分区地址（标记）和该cache行内填入的标记是否一样，一样的话才能访问；如果不一样的话，仍然去主存访问，并使用本次访问的块替换调之前占据该行的数据。 \(k\)路组相连：cache也分组，\(k\)路的含义是，要求每组内的都有\(k\)行。主存地址仍然分3级，只不过原来的第二级地址（cache行地址）改为表示cache的组号。在映射时，一个主存块地址可以映射到它对应的cache组的任一行，兼顾了全相连和直接相连。如果当cache某一组已经放满，且这时又需要放入新的组，此时就需要从该组中选一行进行替换，下面再讲怎么替换。 替换算法： 先进先出FIFO：按顺序来，替换最早进cache的一行。设计时需要为每一行设一个计数器，记录每一行停留在cache中的轮数。每访问一次cache，组内所有行计数值都+1 近期最少使用LRU：设计数器，新加入的行的计数值为0，之后每次访问，所有没有命中的行计数+1。一旦被命中，计数值就清0. 最不经常使用LRU：设计数器，新加入的行的计数值为0，之后每命中一次计数+1。 随机替换：字面意思w 替换算法的抖动：不断地顶掉又调入某个块 做题：注意cache总容量包括标记和有效位。 虚拟存储器：类比swap分区。分为段式、页式、段页式。将主存和辅存中的部分空间共同视为一个虚拟存储器。 MMU：页表寄存器。记录逻辑页号到物理页号的映射。其中每一项也有有效位。 地址划分：虚页号&amp;页内偏移，页表放在主存中。 工作过程：CPU用虚拟地址访问MMU，MMU得到该地址的物理地址，去访问cache或主存。 后备缓冲器TLB：快表，缩短了访问主存的次数。 Cp5. Command System 指令字长：有单字长、双字长、半字长等。长度和机器字长对应。 根据操作数数目分类： 三地址指令：A1 OP A1 -&gt; A3 两地址指令：A1 OP A2 -&gt; A1 一地址指令：AC OP A1 -&gt; AC 指令格式：操作码字段+寻址方式+地址码字段，其中地址码根据操作数数目，再分为多个操作数的地址。 &gt; 这里注意，多操作数的情况下，需要为每个地址都分配该地址所使用的寻址方式字段。 指令的寻址方式： 顺序寻址：找到顺序存放的一系列指令的第一条指令的地址，然后pc不断+1得到后续指令。注意这里的+1指的是加一条指令所占的字长。 跳跃寻址：遇到jmp之类的转移指令时，直接将要跳转的指令地址放入pc 操作数寻址：汇编里那一套。 立即数寻址：操作数就是给出的立即数本身。 寄存器寻址：操作数在寄存器中。数据长度 直接寻址：操作数是所给出的立即数地址里的内容 间接寻址：地址码给出操作数地址的地址。例如mov ax, [200h]，首先访问200h单元，将其中的内容看做地址，再访问该地址，再得到的内容才是操作数。 寄存器间接寻址：寄存器中存操作数地址的地址。mov ax, bx 相对寻址：操作数的地址由pc的值+一个基地址d得到。这种方式中，由于pc的值会不断累加，因此实际操作数的地址是d+pc+指令字长 基址寻址：基址寄存器中的值+一个偏移值得到最终地址。 变址寻址：一个基地址+变址寄存器中的值。 指令格式设计： 注意，在研究执行一条指令访问多少次主存时，一定要注意不止取操作数、保存结果要访问主存，取这条指令本身也要访问主存。当前指令长度跨多少个机器字，取指令时就要访问几次主存。 MIPS mips指令：无内部互锁の流水线指令集。 三种指令格式： R型：三地址指令。6位操作码（始终为0）+3x5位地址码\(R_sR_tR_d\)+5位shamt码+6位funct码。寻址方式由unct字段隐式说明。这里操作数和结果的地址都是寄存器（所有只有5位，32个寄存器）。由于op全0，funct才真正起到操作码的功能。shamt指示在移位指令中要移动的位数。 I型：三地址指令。6位op+2x5位地址码\(R_sR_t\)+16位立即数。分为四类：运算型（Rs op imm -&gt; Rt），访存型（读主存lw: Rs+imm -&gt; Rt; 写主存sw: Rt -&gt; Rs+imm），移位，条件转移（bne, beq: pc+(imm&lt;&lt;2) -&gt; pc）。 J型：6位op+26位立即数。有两种： 无条件转移：高四位(pc++)和imm&lt;&lt;2拼接 -&gt; pc IJ型指令的寻址方式都由op隐式说明。支持立即数、寄存器、基址、相对寻址，以及伪直接寻址。 CP.6 CPU cpu组成： 运算器（状态寄存器、通用寄存器、ALU） 控制器（取指令、执行指令），程序控制、操作控制、时序控制、异常控制 特殊寄存器： PC IR：指令寄存器，存当前正执行的指令 AR：锁存访问内存的地址 DR：锁存主存取出的数据 AC：累加寄存器 PSW：程序状态字，类比x86里的flag。 取指令：Mem[pc++] -&gt; IR（指令寄存器） 操作控制器：控制取指令执行指令。 数据通路：考虑在两个寄存器之间传输数据的情形。数据从a通过组合逻辑电路传到b。时钟周期到来时，a的值经过一个寄存器延迟clk_to_Q后更新它的值。新值通过组合逻辑电路到达b的最长路径时间为关键路径实验l，b更新值需要建立时间setuptime，那么cpu的时钟周期应当大于这三个时延的和。 另外，ab时钟同时到来，而b在刚刚获得一个新值以后要求保持该值一段时间holdtime，如果a到b传输的最快时间（最短路径时延+clk_to_Q）比holdtime小，就会破坏b的保持。因此要求这两个值的和&gt;holdtime。 单总线结构：要防止总线争用，需要分时并锁存出入总线的信号。执行一条加法指令需要3个时钟周期 双总线：执行指令时可以并行地传送两个操作数。加法需要2个时钟周期 三总线结构：没有锁存器，加法只需要一个时钟周期。 单周期mips：要求一条指令在一个周期内完成。 cpu控制器输出的控制信号的产生：首先时序产生器周期地产生时序信号，不同的时序信号和指令译码得到的信号通过组合逻辑，就可以得到控制器的控制信号。 控制器设计： 传统三级时序：三级分别是：一个指令周期内有若干时钟周期，一个时钟周期内又有若干节拍。当前处于哪个时钟周期、哪个节拍这些时序信号由时序产生器来产生。我们将一个指令的不同时钟周期看做不同状态，做出状态图，就可以通过组合逻辑产生控制信号。 变长指令周期：把不同的指令在执行周期的所有节拍都认为是不同的状态。（而之前是，认为所有指令都有同样多的节拍，把不同的节拍看做不同的状态）。把这些不同状态输入一个fsm，生成次态。 微程序控制器：不同的指令中的不同时钟周期对应不同的状态，这里和上面都一样。但是注意到在每个状态下，要产生的控制信号是固定的，因此可以直接将其存储到一个存储器中，根据不同的状态直接取用即可。这里，一条指令对应的操作我们称为微程序；不同状态下，对应的固定的控制信号，将其拼接成一个二进制串，称为微指令；微指令在存储器中的地址称为微地址。那么我们只要在的状态下，用微地址访问存储器，得到微指令，也就得到了要输出的控制信号。 微指令内除了各个控制信号的拼接部分（操作控制字段），末尾还有顺序控制字段：包括判别字段和下址字段。下址字段给出下一跳微指令在控存中的地址。判别字段中的\(p_1\)为0时，下一微指令直接就是下址字段的微指令；为1说明下址字段无效，要根据地址转移逻辑来获得下一个微地址（这种情况往往是取指令微程序的最后一条微指令中会出现，因为对它而言，下一个微地址需要根据具体的指令类型来跳转到相应的微程序） 不同的机器指令，在取指令阶段的状态是相同的，只是在执行阶段根据指令的不同分支为了不同的若干状态，因此不同机器指令的取指令部分，可以共用微程序。 因此现在，我们从IR中取出指令后，不需要使用复杂逻辑生成控制信号，而只需要一个地址转移逻辑，生成对应微程序的微地址即可。 微指令的设计：微指令中的控制字段往往很长，希望可以压缩: 对微命令（控制信号）分类：将互斥型的微命令（不会同时为1）进行编码（而不是onehot方式）。例如，有七个微命令互斥，那么可以使用二进制编码压缩为3位。 &gt; 这里注意，3位编码最多只能表示7位而不是8位。因为要预留一个000来表示没有任何信号。 压缩下址字段：约定位微指令都顺序存放，那么就不需要下址字段，直接顺序执行即可。 垂直微指令：和机器指令有点类似。原来的一个时钟周期对应一条微指令，可能执行多条机器指令，现在让其中的每一个机器指令对应一条微指令（就是把原来一条长的微指令拆成几个短的）。不过有一说一，这个好像实际中性能太差已经被淘汰了。 要看的：性能评价指标计算（cpi，mips），原码补码一位乘法，双符号位浮点数加减 ，恢复余数法，不恢复余数法，定点数 浮点数数据表示范围，字位扩展]]></content>
  </entry>
  <entry>
    <title><![CDATA[whyの学习笔记:julia]]></title>
    <url>%2F2019%2F07%2F28%2Fwhy%E3%81%AE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-julia%2F</url>
    <content type="text"><![CDATA[数学计算&amp;运算符 Julia可以进行任意精度的大数运算，可以使用例如BigInt(1e233), BigFloat(2.0^233)来创建一个大数，也可以使用parse(BigInt, '1234567890')从字符串获取。 在数字中使用下划线是允许的，可以用来分隔数字。例如10_000表示10000。 浮点数的0有两个，+0和-0（这个所有语言倒是都一样…） 特殊浮点数有三种：Inf, -Inf, NaN 后缀im表示虚数 乘法例如2*x可以直接用2x的形式表示。例如(x-1)x, 2(x-1)也是被允许的。 但是(x+1)(x-1), x(x-1)的形式不允许。（由于括号同时有函数调用的功能，会产生歧义） 浮点数中，使用f代替e进行科学计数法，表示该数字是Float32类型 函数zero(x), one(x)可以返回与x同类型的0或1，一般用于数值比较 算数右移&gt;&gt;， 逻辑右移&gt;&gt;&gt;，异或xor(a, b) 所有的运算符前加.，并作用于向量时表示逐元素计算。例如[1, 2, 3] .^ 2 = [1, 4, 9] 形如a &lt; b &lt; c的表达式是允许的。 使用div(a, b)计算整除 bitstring(x)查看二进制数 字符串之间可以比大小，依据是字典序 数据结构 数组 数组a = [1, 2, 3]，下标从1开始！！。各元素类型可以不同。 a[end]表示最后一位元素 二维数组使用;分割维度：mat = [1 2; 3 4] 使用push!(a, x)或者append!(a, x)向a末尾添加元素，使用!pop(a)弹出末尾元素 同理，使用popfirst!和pushfirst!向开头增删元素 这些以感叹号后缀的函数表示可以修改参数的值（原地操作） append!也可以直接链接两个数组 支持切片，a[1:3]包括a[1], a[2], a[3] 初始化时，a = 1:5等价于a = [1, 2, 3, 4, 5] 检查a是否在b中：in(a, b) length(a)获得长度,size(a)获得shape 列表生成式：和python一致 元组 只读结构，但各元素类型可以不同。a = (1, 2, 'hello') 可以为元组的各个元素命名，成为命名元组a = (b=1, c=2)，通过a.b索引元素 大多数数组操作也支持元组 字典 创建：d = Dict(&quot;key1&quot;=&gt;1, &quot;key2&quot;=&gt;2) 获得所有的键keys(d)；所有的值values(d) 使用in判断一个键值对是否在d中：in((&quot;key1=&gt;1&quot;), d) 使用haskey(d, &quot;key1&quot;)检查是否有键 集合 s = Set([1, 2, 3]) 使用push!新增元素 使用intersect, union, setdiff函数计算交集、并集、差集 字符串 格式化字符串使用&quot;$(任意表达式)&quot; 控制流 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354551.输出函数：println(str)或者使用`printf`宏：using Printf@printf "%d is less than %f\n" 4.5 5.32.条件语句：注意最后的end。也可以用三元运算符`? :`if ... # 这里必须是不布尔类型的条件表达式，即`if 1`这种写法时不允许的 ...elseif ... ...else ...end3.for循环：下列两种等价：for x = [1, 2, 3] ...endfor x in [1, 2, 3] ...end4.异常处理:try ...catch e ...end5.定义函数：funciton func(a, b) ...end更简洁的定义：func(a, b) = 返回值这种形式等价于匿名函数：func = (a, b) -&gt; a+b默认参数：和python相同变长参数：function func(args...)传参时，解包列表（类似python里的*）func(array...)内置函数：map, filter，和python用法相同6. 断言使用 @assert 表达式 作用于单个元素的函数可以通过加.直接广播到数组：例如sin.(arr)，会对arr中的所有元素执行sin 类型 使用typeof(x)获得类型。类型（例如Int）的类型是DataType 使用subtypes获得类型的子类， supertype获得父类 构建自己的类型（结构）： 1234struct Tiger &lt;: Father #表示继承Father taillength::Float64 #使用::限制类型 coatcolor #不带类型标注相当于 `::Any`end 抽象类型： 模块 123456789101112131415161718192021222324252627282930313233341. 定义模块：在代码文件开头定义。在该部分使用export指定要导出的内容：module MyModuleexport x, yx() = "x"y() = "y"p() = "p"function __init__() ...endend其中，在模块中定义的__init__函数会在该模块加载完毕后自动执行一次2. 使用：导入代码 导入了什么？using MyModule export的东西using MyModule: x, p 指定啥导入啥（不管有没有export）import MyModule 模块里的所有东西（包括没有export的）import MyModule.x, MyModule.p 指定啥导入啥（不管有没有export）import MyModule: x, p 指定啥导入啥（不管有没有export）* 也就是说，模块定义的export只影响 using MyModule 形式的导入。3. 使用自定义模块include("./module.jl")using .MyModule # 注意前面要加点（表示相对路径）]]></content>
      <tags>
        <tag>julia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习中的生成模型]]></title>
    <url>%2F2019%2F05%2F04%2F%E6%B7%B1%E5%BA%A6xio%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[生成任务是一种典型的不适定任务。对于分类、回归等判别类任务而言，对数据集中的每一个样本\((x, y)\)，模型需要学习参数\(\theta\)，来拟合后验分布\(P(y|x)\)；而生成任务则恰恰相反，需要拟合的是分布\(P(x|y)\)，模型的输入是标签\(y\)（或者其他什么东西，一般使用向量\(z\)表示），输出的是\(x\)。在深度学习中，以gan为代表的生成模型就是完成这一功能的。 Pixel RNN / CNN pixelRNN的初始输入是图片的第一个像素（初始像素）（如果是RGB三通道，那么每个像素包含三个数字），输出下一个像素；然后输入初始像素和生成的像素，得到下一个像素…每一次的输入都包含之前所有的像素，不断循环得到整张图片。因为是逐像素所以很慢。也可以用在语音合成和文字和影像生成上。 pixelCNN同理，只是改用卷积来逐个区域进行生成。 自动解码器（AE） \[\text{原图片} \stackrel {encode}{\longrightarrow} code \stackrel {decode}{\longrightarrow} \text{生成图片}\] 通过训练使得生成的图片更接近于原图片，训练完成后输入一个随机的\(code\)向量，通过\(decode\)就可以得到生成的图片。依然很菜。 改进：变分自动解码器（VAE） 不直接生成\(code\)向量，而是生成三个向量\(m,\sigma\)，再从一特定分布中取一向量\(e\)，通过\(code=exp(\sigma)\times e+m\)得到\(code\)。其他和\(AE\)相同。 训练完毕后，如果在生成过程中人为指定\(code\)向量的某些维度，得到的图片中会有某些共同性。可以认为，\(code\)中的维度具有控制图片性质的功能，具有某些特定的含义。 原理：假设所有的真实样本都符合高斯混合分布，是从某一个由高斯混合模型中生成的，\(VAE\)通过极大似然来估计获得该模型的参数，即通过高斯混合模型来拟合真实的数据（记为\(x\)）分布。高斯混合模型（记为\(G\)）是由多个不同的高斯模型\(g_1,g_2,...g_n\)组合得到，初始的噪声\(code\)（记为\(z\)），其实包含的就是 选择哪些\(g_i\)，以及从这个高斯模型中的那个地方采样，采样结果组合起来就得到了生成的样本。 具体如何得到这个高斯混合模型\(G\)，也即如何从\(z\)得到各个\(g_i\)的参数\(\sigma, \mu\)，方法就是通过神经网络\(decoder\)来拟合不断拟合\(G\)的分布。也即，训练\(decoder\)的过程就是求\(decoder\)中的参数对\(P(x)\)的最大似然。而此时: \[P(x)=\int_{z}P(z)P(x|z)\] 即真实数据的概率分布等于，在向量\(z\)中选择一个初始值\(z\)（这个值用来控制选择哪个高斯模型的哪个位置采样），概率为\(P(z)\)，并且在给定一个初始值\(z\)的情况下，\(G\)生成真实样本\(x\)的概率，即\(P(x|z)\)。对于每个这样的初始值\(z\)，模型进行一次采样，将所有采样的结果叠加，概率为\(\int P(x)\) 通过数学推导，\(L = logP(x) = L_b + KL(q(z|x)||p(z|x))\)，\(L\)表示似然，也即\(L\)由\(q(z|x),p(z|x)\)的\(KL\)散度（一种衡量概率分布之间的关联性的量）以及一个\(L_b\)组成，由于\(KL&gt;0\)，因此\(L_b\)就是似然的下界。 在实际的操作中，往往通过最大化该下界，来使得\(L_b\)尽可能接近\(L\)，即尽可能最小化\(KL\)散度，使得\(KL=0\)。通过同时最小化\(KL\)以及最大化\(E_{q(z|x)}[logP(x|z)]\)，就得到了\(VAE\)的损失函数。 缺点：只是单纯地使得输出的结果和数据集中的某些图片越接近越好，有时只是数据集中不同样本的简单组合。 GAN 使用两个网络\(D, G\)对抗学习的思想，来获得生成模型。其中\(D\)为二分类模型，判断输入是否为真是数据；\(G\)是生成模型，试图将噪声输入\(z\)映射到真实数据分布。网络结构任意。在原始的gan中，损失函数为： \[\begin{cases} Loss_G = \min_G \ \mathbb{E}[log(1-D(G(z)))] \\ Loss_D = \max_D \ \mathbb{E}[log(D(x)) + log(1-D(G(z)))] \end{cases}\] 实际上\(G\)最小化损失，其实就是在最小化两个分布的\(JS\)散度，使得两个分布更加接近。\(JS\)散度其实就是\(KL\)散度的对称版： \[JS(P|Q) = {1\over2} KL(P||{P+Q\over2}) +{1\over2} KL(Q||{P+Q\over2}) \] 在该损失函数上进行优化，对两个网络交替更新。由于初代gan训练不稳定，在交替训练过程中，每一步训练一次\(G\)，但要训练\(k\)次\(D\)。这是因为生成器D实际是在衡量真假数据之间的距离（JS散度），如果\(G\)训练多步时，假数据的分布就改变了，这时再使用\(D\)，显然无法正确衡量出二者的差距。因此我们要让G训练尽可能慢，来使得生成数据的分布变化不是很明显，使得D仍然可以发挥作用。 再实际操作中，G的损失函数中\(log(1-D(G(z)))\)一项，由于在\(D(G(z))=0\)附近斜率很小（即梯度下降比较慢），因此常常用\(-log(D(G(z)))\)来代替。 数学原理 其实是极大似然法。生成器\(G\)试图拟合真实数据分布，其实就是求一组\(G\)的参数，在极大似然的约束下求解。易证极大似然逼近等价于最小化两个分布之间的\(KL\)散度。现在假设只有一个生成器，只要能够找到计算\(KL\)散度的办法，就可以达到目的。但两个分布都过于复杂，无法数学上计算，因此判别器\(D\)实际上就是用来近似这个KL散度的。判别器对真假图片进行评分，并使用评分来衡量分布之间的距离。当一个判别器训练好之后，它就具有了拟合分布之间距离的能力。当生成分布和真实分布完全一样时，理想的判别器对二者的评分将都是0.5。 在这里，判别器使用分类对分布进行拟合的过程，其实并不十分科学。原始的gan中使用的是交叉熵作为loss，但事实上任何可以描述距离的函数都可以作为loss使用。后面你会看到，各种gan在此做了很多后续工作。（详见后文fGAN） 同时，由于判别器只有在训练好之后，才具有拟合能力，才可以指导\(G\)更新参数，而在训练过程中，随着G被更新，它生成的图片分布也在不断变化，从而导致判别器需要重新训练，由此带来导致训练不平衡的带问题，这也是后续多数论文想要解决的问题。 后续发展 DCGAN: 简单有效。使用了\(CNN\)，去掉全连接层，加入\(BN\)，改进激活函数，通过大量实验获得了好的效果。 CGAN： 发现\(z\)的各个维度其实隐含了生成图片的某些特征。于是使用有label的数据集，将噪声（原来的\(z\)）和\(y\)拼接起来作为\(z\)。这样的方法可以控制生成数据的特征。同时，\(D\)的输入也需要加入\(y\)。 infoGAN 使用无label的数据集，实现和CGAN一样控制特征的效果。将CGAN中的\(y\)改为随机的向量\(c\)，和噪声拼接作为\(z\)。通过改进损失函数，最大化\(c\)和生成假数据之间的互信息，强制让网络把\(c\)中各个维度的值当做特征来进行生成。 互信息：可以认为是\(KL(p(x, y)||p(x)p(y))\)，即两个分布之间互相包含部分多少（或不独立的部分的多少）。可以衡量分布的相似性。 这样只要在原始GAN的\(loss\)中加上一项：\(\lambda I(c, G(z, c))\)即可。\(\lambda\)是该项\(loss\)的权重，\(I\)是互信息。那么怎么计算这个复杂的互信息呢？答案是直接使用一个神经网络\(Q\)进行拟合。在实际操作中，计算互信息需要使用到无法计算的后验概率\(P(c|x)\)，因此展开互信息的表达式，经过一番数学推导（太长辽这里就不放了，可以去康康原paper），可以得到互信息的一个下界，并使用\(Q\)来拟合该下界即可。最终得到\(I(c, G(z, c)) \approx \mathbb{E}_{x\sim G(z, c)}[logQ(c|x)]\) 在实际的训练中，\(Q, D\)共享卷积层，只是在最后的全连接层分支成不同的输出。 损失函数的改进 fGAN: 原始gan使用的损失函数是\(JS\)散度。但实际中真实分布和生成分布并不一定有很大重合。这就需要寻找新的损失函数。fGAN指出，衡量概率分布差异的函数应当具有形式：\(D_f(P||q)=\sum q(x)f({p(x)\over q(x)})\)，但其中\(f\)只要满足是凸函数，且\(f(1)=0\)即可，这样就可以制造出各种各样的损失函数。当\(f(x) = xlogx\)，这时\(D_f\)就是常用的\(KL\)散度。 并且实际上，数据往往维度很高，真实分布和生成分布常常不会有交集。但是反映到\(JS\)分布上，二者只要没有交集时，则无论远近，\(JS\)值都相同(都是\(log2\))，导致梯度消失。这显然与常识不符。 LSGAN：实际的训练中，用来衡量两个分布误差的是\(D\)。上述问题会导致，当二者完全没有交集时，尽管G仍存在好坏之分，但D却始终可以轻松鉴别出二者的差异，导致无法指导G进行训练。因此，可以想办法让D没那么轻松地鉴别分布差异。因此LSGAN使用最小二乘损失函数，并将最后的sigmoid层改为线性层，来惩罚生成距离真实分布非常远的假数据。 WGAN：使用EM距离作为loss。此外做的改动有： D最后一层去掉sigmoid G和D的loss不取log（sigmoid_cross_entropy_with_logits） 每次更新D的参数之后将其绝对值截断]]></content>
  </entry>
  <entry>
    <title><![CDATA[whyの线性代数笔记！]]></title>
    <url>%2F2019%2F05%2F02%2Fwhy%E3%81%AE%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%AC%94%E8%AE%B0%EF%BC%81%2F</url>
    <content type="text"><![CDATA[我数学太菜了AI酱不陪我玩只好再把线代学一遍呜呜呜… 1.线性方程组 \[ G: \begin{cases} a_{11}x_11+a_{12}x_12...+a_{1n}x_1n = b_1 \\ a_{21}x_21+a_{22}x_22...+a_{2n}x_2n = b_2 \\ ...... \\ a_{m1}x_m1+a_{m2}x_m2...+a_{mn}x_mn = b_m \end{cases} \] 行图像的视角：考虑\(m\)个线性方程构成的方程组\(G\)，第\(i\)个方程的形式为\(\sum_{j=1}^n a_{ij}x_{ij}=b_i\). 单独考虑每一个方程（有\(n\)个变量\(x_1...x_n\)），可以认为该方程是\(n\)维空间中的一个超平面（子空间），\(m\)个方程同时满足，即为\(m\)个超平面的交点。交点坐标即为方程的解。 列图像的视角：将方程组中的总计\(m\times n\)个系数\(a_{11}, a_{12}, ...a_{1n}, a_{21}, a_{22}...a_{2n}.....a_{mn}\)按列分成\(n\)组，第\(j\)组为\(\mathbf{a_j} = [a_{1j}, a_{2j}, ...a_{mj}]^T\)，恰表示\(n\)维空间中的一个向量。则方程组可以看做\(\sum_{j=1}^n \mathbf{a_j}\mathbf{x_j} = \mathbf{b}\)，此时向量\(\mathbf{x_j}=[x_{1j}, x_{2j}, ...x_{mj}]^T\)即为各个\(\mathbf{a_j}\)的线性组合系数。 由此，我们将各个列向量\(\mathbf{[a_1, a_2, ...a_n]}\)排列起来，形成矩阵\({A}\)，则方程组简写为\(A\mathbf{x=b}\)。求解\(\mathbf{x}\)即为求解对特定向量的线性组合。 \[ A\mathbf{x=} \left( \begin{matrix} a &amp; b \\ c &amp; d \end{matrix} \right) \left( \begin{matrix} e \\ f \end{matrix} \right)=e \left( \begin{matrix} a \\ c \end{matrix} \right)+f \left( \begin{matrix} b \\ d \end{matrix} \right) \tag{1} \] 由此也可以看出矩阵乘法的本质，例如上述矩阵与向量相乘，可以表示为矩阵\({A}\)关于向量\(\mathbf{x}\)的线性组合。 解方程：对系数矩阵使用行初等变换（交换两行，数乘，两行相加减）进行消元（高斯消元法）。消至三角矩阵为佳。具体怎么做…想想你初中怎么解方程的？在进行每一次初等变换时，相当于矩阵\({A}\)左乘了一个对应的初等矩阵（即单位矩阵\({I}\)进行一次同样的初等变换所得到的矩阵） 矩阵乘法\({A_\mathcal{m\times n}B_\mathcal{n \times p}=C_\mathcal{m\times p}}\)的不同视角： 转化为向量点积：\({C}\)中的任意元素\(c_{ij}\)看做由\({A}\)中第\(i\)行和\({B}\)中第\(j\)列点积的结果。 转化为矩阵乘列向量：\({C}\)中任意一列\({C_j}\)看做由\({A}\)乘\({B}\)中一个列向量\({B_j}\)得来，然后将各个列排列在一起。矩阵乘列向量参考式\((1)\)。 转化为行向量乘矩阵：\({C}\)中任意一行\({C_i}\)看做由\({A}\)中一个行向量\({A_i}\)乘\({B}\)得来，然后将各个行排列在一起。行向量乘矩阵参考式\((1)\)。 转化成向量乘法：考虑\({A}\)中任一列\({A_j}\)乘\({B}\)中任一行\({B_i}\)，得到一个\(m\times p\)的矩阵，将所有这样的矩阵相加即可得到\({C}\)。 2.逆矩阵 矩阵的逆：满足\({AA^{-1}=I}\)（或\({A^{-1}A=I}\)）。方阵的左逆和右逆相等，而非方阵的左逆和右逆不相等。不可逆矩阵又叫奇异矩阵。 \({A}可逆\Leftrightarrow |{A}|=0\Leftrightarrow \exists 非零向量\mathbf{x},使A\mathbf{x=0}\) 求逆矩阵：将原矩阵\({A}\)与单位阵\({I}\)构成增广矩阵\({[A|I]}\)，对该矩阵进行行初等变换，使得原矩阵变为单位阵，则变换之后单位阵的位置即为逆矩阵。本质是把\({A^{-1}}\)看做多个未知列向量的组合\([\mathbf{\alpha_1, \alpha_2...\alpha_n}]\)，同时解\(n\)个方程组，再将解出的列向量\([\mathbf{\alpha_1, \alpha_2...\alpha_n}]\)拼起来。因此表现形式上和解方程组相同（都是进行高斯消元法）。 性质： \({(AB)^{-1} = B^{-1}A^{-1}}\)（顺序相反。因为线性变换\(AB\)相当于先进行\(A\)，再进行\(B\)，其逆动作应当是先恢复\(B\)再恢复\(A\)）。 \((A^T)^{-1} = (A^{-1})^T\)]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浮生录。]]></title>
    <url>%2F2019%2F04%2F30%2F%E6%B5%AE%E7%94%9F%E5%BD%95%E3%80%82%2F</url>
    <content type="text"><![CDATA[这是一篇充满了秘密的神奇文章w 2019.11.30 初雪 我终于开始正式记日记了。不…其实也不算日记，恐怕我应该不会有时间每天写吧。但是有些事情总得写下来，否则指不定哪天就忘了。 今天的武汉下着阴冷的小雨，明明快要腊月了。妈妈发消息告诉我，孝义下雪了。这时候我竟然恍然之间有点想落泪。我想起了之前看《百年孤独》时，一句平淡的句子：“马孔多在下雨”。很多人似乎对这句话充满了感慨，当时的我却毫无感觉。但是此刻——我内心突然感到了一点触动。 当自己真正离开故乡，才真真切切感受到了无穷尽的想念。家乡的雪现在是什么样的？想必和往常一样吧。往常我听到下雪的消息，都是某天早上从懒觉中睁开惺忪的眼睛，听见爸爸高兴地说，外面下雪啦。然后我走到阳台，就能看到满眼的白色。家里的暖气总是开得很足，即使下雪也没有丝毫的冷意。 人生真是奇妙啊。以前读古诗，总觉得这些个诗人要么思乡要么郁郁不得志，也没点新鲜花样。等这些感情出现在自己身上的时候，才发现自己的感受是如此鲜明浓烈。 啊……快点放假吧。我本来还打算假期留校的。这样看来，也不知道到时候我能不能忍得住不回家…明天就是12月了。腊月一过就是年。新年快到了啊。 2019.11.30 遗忘的梦境 反正是第一天写，干脆多写一点叭。昨天晚上又梦到有趣的梦了。但是最坑的是，我明明在快醒来的时候想起了要把梦记下来，结果没想到，我做记录的过程也是在做梦。我梦到自己拿起一张纸，然后飞快地写下梦境的剧情——唉当我拿起纸的时候就应该意识到不对劲的。 结果现在我只记得一些零碎的场景了。我似乎坐着一艘奇怪的乌篷船，在仿佛威尼斯一般的水上城市里穿行，直到到达了入海口，看到眼前的大海延伸到尽头，海上漂浮着浓浓的雾气，雾气中隐约浮现出一些长在海水中的树木。场景极其雄壮阔大，就是这个画面让我马上产生了要把它记录下来的念头。但是很奇怪，这个画面在我眼中是以俯视的视角呈现的，雾气把海面覆盖得一片白色，从白色中透出星星点点的绿色树冠，小得仿佛米粒一般。嘛，反正是梦，管它为什么呢。 只希望下次遇到好玩的梦能记得记录就好了。当然，是在现实中记录。 2019.12.18 病中 自从上大学以来一直没有生过病的我，昨天终于被一场冷雨淋感冒了。明明是冬天，天气却湿冷湿冷的。唉太难受了。人带盖只有在生病的时候，才会疯狂地感觉到孤独和无助吧。 2020.1.1 结束与开始的序章 2019年12月31日在爱の告白中结束了。历经那么那么漫长的寻觅，我觉得我终于找到了的人生的意义。这一天，两个孤单的灵魂相遇在一起，他们决定今后不再分离。 而且……他们的故事即将开始。 祝所有人新年快乐！(●’◡’●)ﾉ♥ 2020.1.2 随意の记录#1 脱单的第二天，感觉全身都沉浸在巨大的幸福感中。喜欢的人也喜欢自己实在是太幸福了啊啊啊！！！（捂脸）感觉每时每刻都特别想大声说我真的超喜欢你啊！！但是实在是太令人害羞了又不太好意思开口（再度捂脸） 2020.1.8 晓看天色 从很久以前开始，我就是那种特别怀旧的人。 这样的心情差不多从初中时期一直持续到了现在。小学的时候，我在班级里还有几个朋友，虽然大家都说我很内向，但是并不影响一群小孩子关系融洽地学习生活。只不过，当其他小朋友在教室外面追逐打闹的时候，我觉得我还是没办法融入他们，这对我来说还是太难辽。 至于到了初中，就完全变成了我的社交灾难现场。一来我本来就不太擅长和人打交道，二来当时我们班成绩是全校最差的，认真学习的同学在这里反而会遭到排挤和报复，而我又不愿意向那些奇奇怪怪的威胁屈服（我就是不愿意给你们考试抄答案哼！略略略），结果经常被班里那些成绩特别差还喜欢欺负人的学生冷言嘲讽，恶语相向（淦我好惨），再加上我在班里几乎没有朋友，那时候也没有智能手机，没办法上网看沙雕视频消遣，结果我就变成了完全的独行者，每天开启自闭模式，一个人上学一个人做题一个人看书，三年的时间就被磨耗在了空洞的题目中。 这种处境下的我生活得很不快乐（不好.jpg），整个人也变得越来越敏感和冲动。有一说一，如果这种情况再持续个几年，我估计我绝对会抑郁。这段时间里，我真切地感觉到我的童年结束了，有什么抓不住的东西消失了，再也回不来了；也就是在这段时间里，我开始不断地想念童年的时光，开始慢慢变得怀旧了。以前每年暑假在乡下奶奶家生活的景象一直萦绕在脑海里。清晨干燥冷冽的空气，远处传来的缥缈的鸡鸣，小院黝黑的院墙和屋檐，院子里午后温暖的阳光，门口那条曲曲折折的巷子，过年时寒冷的窗外和暖和的屋内，一片片望不到头的田埂，这些都离我而去了。也正是在这段时间，爸妈为了我上学方便搬了家，我离开了生活了十多年的地方，也离开了我童年时仅有的，两个能称得上好朋友的朋友，迎接我的是无数陌生且残酷的面孔。（淦越写越伤心） 所以从那时候起，我大概和同龄人的成长轨迹出现了割裂。每天都是一个人，导致我的心性可能也停滞在了小学毕业的时候，直到后来上了高中，时针才再次开始慢慢走动。不过有一说一，这样也许有一点好处，那就是内心一些我不想丢掉的东西，被我以这种方式保留了下来。没有和别人的社交活动，所以我才会在无聊的时间胡思乱想，想象一些奇奇怪怪的故事。会有很多中二の脑洞，晚上还会做光怪陆离的梦，内心还能小心翼翼地保存着这份感情，直到和你相遇。（全世界只有我们两个人能看这篇文章，说的就是你啦！） 再后来我上了高中，终于遇到了一群愿意接纳我，又有些有趣的同学们，但是令人悲伤的是——我已经习惯孤独了，几乎成为了一种自我保护的条件反射，有时候还会因此被误会，甚至辜负别人的好意。同样平淡的高中三年就这样结束了。虽然我慢慢也变得没有那么自闭，也可以正常和别人相处，但是我的怀旧的情绪一点也没有减少。这时候我突然真切感觉到时间的流速仿佛加快了，我竟然已经要上大学了。在大学的前两年里，我又渐渐陷入了另一种恐慌，再次开始感觉到格格不入：身边的人似乎都很现充，过着大人世界的生活，有时候又感觉很冷漠，对，说白了就是感觉他们似乎很难被感动，但是我却经常会听到某段旋律，看到某段剧情，感情都会有巨大的触动。大家仿佛都变得功利了起来，这时我内心就很迷茫，我该变得像他们一样吗？融入他们五光十色的生活？我最终还是决定遵从内心，靠着对代码的热爱走到了今天。我仍然是一个人去图书馆，一个人吃饭，一个人跑步，一个人复习，一个人写代码，一个人偷偷想着中二的脑洞，一个人在午后醒来的时刻发呆，一个人在夜深人静的时候思绪慢慢飘荡。 后来，大三的学期开始了，2019年的秋天匆匆过去了，之后的故事你就知道了。2019年12月31日落下帷幕，辛苦但仍然令人留恋的考试周过去了，后来，我们来到了现在，今日，此时此刻。现在，每次我回忆起跨年的晚上，都感觉浑身都浸入在巨大的幸福感之中，伴随着的也有强烈的不舍，如果那天晚上永远不要结束该多好；之后的一周虽然复习极其无聊，但是和你在路上是最最快乐的时光，现在这一周也过去了，突然从忙碌中解放的我竟然也有一点不舍，只要和你在一起哪怕每天都是无聊的考试周我也很开心。 之前说过了，从很久以前开始，我就是那种特别怀旧的人。我现在觉得，我会怀旧是源于对美好的东西本能的回忆，以及对“故事”的渴望。我一直是喜欢听故事的人，而那些美好的记忆，就是我自己仅有的故事了。与此同时，这些记忆也已经再也回不来了，只能一遍又一遍地回忆并感叹着。但是和你在一起的回忆不一样——我们的故事还在继续。跨年夜的回忆美好是因为你，过去的一周令人留恋也是因为你，只要你在身边，这些记忆就永远不会褪色，它们每时每刻都作为我们的一部分。这些记忆并没有离开。 这也就是为什么离别如此令人难过。而今天仅仅是你离开的第一天，我就已经开始遏制不住地开始回忆过去的一个学期我们一起经历的时光，这仅仅是短暂分别的第一天而已。所以此刻我无比确信，我也绝对不会离开你。这辈子我绝对会陪你一起走到尽头，至死不渝。最后，那么为什么这篇文章的名字叫《晓看天色》呢（笑），这其实出自唐寅的诗。那句诗的下半句就是我现在想对你说的话了，也正是我此刻的真实写照。 那句诗是这样的——晓看天色暮看云，行也思君，坐也思君。 2020.1.9 无题（李商隐 淦其实我就想假期写信，那我今天就直接写了。 开始回忆过去.jpg 在我小时候，我们大院的孩子们经常一起玩，我那时候还特别开朗乐观向上（笑），极其单纯，在大院里捉迷藏，老鹰捉小鸡，虽然我现在不能回忆当年的一切，但是我记得那种单纯的快乐，很美好。 那时我始终都觉得人际关系是简单的（甚至没想过人际关系这个东西，因为我觉得大家都认识，都很好），所以我很快乐的活着。当然这一直持续到我上学前班（不上的话我大概就是18级了哈哈），我依然清晰地记得有天下午，我和我的两个我觉得关系很好的朋友玩老鼠偷油，大概就是划定一块地方由当猫的小朋友镇守，防止其他当老鼠的小朋友跑进来，被抓住就算老鼠输了，嗯差不多就是这个规则。当时的情况是，我当猫他们就会一起当老鼠，我要是当老鼠他们就会一起当猫，我不知道怎么回事，我就是感觉我被排斥了，很难过，于是我就去问他们为什么要这样，他们说”啊我们本来就想两个人自己玩，不想和你玩诶“之类的。 现在想想可能只是他们不想带着我，这个事情挺正常，但是我还小，我印象中我从来没感受到来自别人的恶意，所以特别绝望，还迷茫。我不知道发生了什么，我做错了什么，他们为什么不和我说？为什么突然不和我玩？还是之前他们就不想跟我玩？还是怎么样？我到底（此处停顿一首三生树让我哭一会）。 反正从那时候开始我就对与人交往这件事很敏感，我对主动寻求别人的帮助开始抵触，主动和别人说话开始抵触，并且开始观察别人。那天我听到那个回答以后就默默走了，去哪个角落哭了很久。我花了很久回忆我做过了什么，他们是不是讨厌我，我哪里做的不好会被讨厌，我的其他朋友是不是我的朋友，我到底有没有朋友。其实应该也没有什么很黑暗的东西，但是我当时是真的觉得，一切都和我的认知产生了冲突，世界观崩塌。我后来把他们不跟我玩的原因归咎于我成绩不够他们好，然后开始超努力学习。现在想想我应该感谢他们，不然我可能不会在华中科技大学与你相遇哈哈哈哈哈哈。 其实后来就还好，我只是没有以前那么活泼，对别人都保持了一点距离，面对陌生人无法先开口，就像我有些话说不出来梗在喉咙里那种感觉。大概算是开始内心自闭了。 后来的小学时期（大概三年级以后？）我父母因为工作所以不常在南宁，我有时候会在我一个同班同小区同学家里寄住，因为她和我关系其实并不非常好（我猜可能是因为我成绩比她好她妈妈经常说她的原因吧），所以我更加敏感，而且小学时间多，我因为不怎么和她聊天，父母也不在，我就喜欢看书，看了很多小说，并且有很多时间幻想。其实我觉得小说里的对世界的描述还是太简单了，现实生活中的人际关系没有那么简单，但是我因为小说看多了就习惯简化人际关系，不愿意主动细想。这种简化造成的我对有些行为的难以理解导致我其实真的不喜欢过多的人际交往，基本的能在社会上生存下来的人际交往就足够了。我选择不往深了想，被动成为啥也不知道，啥也看不出来的傻逼，简单快乐，耶。（我选择计算机的原因之一就是电脑不会骗我，没有弯弯绕绕，输入什么就会得到什么结果，我不用猜） 后来我考上了广西最好的初中，住校，军事化管理，一切为了全A+，一切为了成绩。其实在里面还好，虽然有不认真学习的人，但是我跟谁都是，在被迫认识以后能笑着说两句客套话，正常的表面人际交往我能维持，我只是不喜欢谈到我自己，而且有了手机更加沉迷小说和动漫的幻想世界。当然我初中也有喜欢幻想看小说看动漫的朋友，过得还算可以。我大概是内心自闭，表面维持正常生活。好在有题目可以刷，大家没那么多精力社交，表面维持得也还好。只是现在都基本不再联系了而已。 顺利上了高中，进了重点班，同学们的心智成熟了，维持表面社交更加轻松了，刷题，有人问问题就回答，有人找我就去，不会问让我难受的个人问题，别人需要发泄对象我就去听故事，再顺着她的想法说两句附和，不表达自己的实际观点。靠着这样，我在刷题和逃离现世的空余时间认识了不少人，虽然他们不了解真实的我在想什么，或者说没法理解为什么我会是现在这样，但是总算是朋友多了起来，就算是表面朋友也懂得维持关系。跟我不是一个世界的现充知道点头之交就好，和我相似的人也知道要保持距离。于是我平稳的度过了高中，虽然吃饭时常一个人，回家时常一个人，但是这种孤独带给了我平静安心，虽然和朋友们无法交流心里在想什么，但是可以聊日常的事情，也就还不错。也许高三我妹妹的突然出现起了一点波澜，但总体上平淡无奇，没有什么可以追忆的事情。 然后就是上大学了，我一开始想过要不要像他们一样开始往现充方向发展，交很多很多朋友，每个周末都满满当当，成群结队呼朋引伴，因为假期三个月自己在家看小说，虽然快乐，但是真的太孤独了。不过我发现我做不到那种社交方式，同时和这么多人做朋友，如果能同时维持这么多关系，那这个朋友的分量太轻了，这种有点轻浮的做法我内心无法接受（可能是我太理想主义了淦。所以我就还是保持高中交友方式，后面你也知道啦。（此处到横山克的To Victory！耶心情突然不沉重了） 我从来没想过我能遇到一个和我想法这么相似的人，淦真的是缘分啊草。我才认识了你三个月不到吧，但好像我们认识了很久。我没有妹妹的时候，我想我大概就是以后被逼婚然后为了给父母一个交代去相亲，我有了妹妹以后我的想法变成了那我大概要孤身一人到老了，现在哎，请意会。 看小说和动漫那种美好的东西多了以后，我成了理想主义者。谁都不知道我其实很敏感，不知道我其实看了小说会哭很久（比如说魔道祖师，我大概哭的时间比看的时间长），不知道我其实一个人去看电影会捂着嘴很难受的哭然后回家之前先去遛弯等消停了再回去，也不知道我喜欢幻想那些有趣的东西，不知道我一直都很中二，不知道我有时候会在下午或者半夜醒来发现自己在世界上无人倾诉开始无声落泪…… 我讨厌有目的性的去寻找爱情，想不期而遇，想一切随缘，但是我也知道这几乎不可能。不找怎么会遇到相似的人？不说谁会知道我其实是这样？但是我就是倔强，并且还真的，遇到了你。 老李的脱单说说是我以前打趣她说的的，我说找到烧仙草你用光了一生的运气。但现在我觉得我才是这样，我大概永远也抽不到想要的卡了，不过无所谓，反正有你啦，耶！ 其实我们的轨迹相似却还是有很多不同，性格相似但也有不同，不过这不是坏事，相似又互补（对立统一草哈哈哈哈），我会一直陪你走下去的，我也相信我们会一直在一起的，遇到你真的是太好了。“斯人所彩虹，遇上方知有。”说的很在理。 在平时聊天的时候说这些我们大概都没有勇气，而就算是用即时聊天软件，我打的字也会因为害羞删掉，只有写信这种一鼓作气，无法回头的方法才能描述我的心情。“两情若是久长时，又岂在朝朝暮暮”，来日方长，也不差这几天啦。 2020.1.29 记忆的烟花 现在是2020年1月29日凌晨。如今肺炎仍在蔓延，开学的日子遥遥无期。但是我要写的不是这些，而是一些别的东西，关于爱和回忆。 之前我不知道看过了多少次《月刊少女野崎君》，但是唯独这次我有了不同以往的感受。尤其在看到结局的时候，我的思绪又不由自主地回到了那天晚上，2019年12月31日。 现在想来，烟花真是神奇的东西，也承载了我太多的回忆。那天你问跨年有哪里可去，我四处搜索的时候第一眼就看到了烟花。当时我想到的就是月刊少女野崎君的剧情。如果我们也能在烟花下表白的话，那一定是最完美的剧情了。当然啦这时候我也只是想想而已，心里更多还是担心（怂），万一是我一厢情愿想多了，那我们岂不直接没了？所以出发前，我一直在犹豫要不要表白，最终也没作出决定。 那天的经历给我的感觉也很奇妙，我们一起去博物馆，路上一起吐槽看到的东西，气氛的融洽就仿佛我们已经在一起了一样。直到晚上看烟花前，我们坐在长椅上聊天，我还是没有决定到底要不要表白，最终还是想着干脆走一步看一步叭。之后烟花开始了，你兴奋地跑出去，我一直站在你后面，一边看烟花也一边在悄悄看着你，看着你脸上满溢的快乐，我想如果时间能永远停在此刻就好了。 那个时刻，我心里挣扎许久，但在嘈杂的人群和巨大的声响中，我还是没能鼓起勇气说出口。之后烟花表演结束，我们就要回学校了。本来我几乎已经放弃了，觉得还是再等一等叭，我还没办法确定你是什么想法。不过我也不想就此放弃，提议不如去唱歌跨年。我说出这话的时候其实心里超级紧张，害怕提议得有点突然，会被你看出什么。我原本预想你的回答可能会犹豫，毕竟当时已经有了病毒的消息不宜到处乱跑，而且当时我觉得你不是那种特别喜欢唱歌的人，但是出乎我意料的是，你竟然没有一丝犹豫地答应了，这让我感觉到那天的故事也许还没有结束，心里又燃起了一丝希望。 之后就到了2019年的最后几分钟。我表面看着也许面无表情波澜不惊但是内心其实超级超级紧张。如果跨年结束的话肯定要回学校了，就没办法两人独处了。如果现在不作出行动，那之后就是忙碌的考试周，更不可能有单独出来玩的机会了。但是——我真的可以现在说吗？如果一切只是我的一厢情愿该怎么办？秒针一点一点走向抉择的时刻，我的大脑开始变得一片空白，这个时刻仿佛我在经历什么生死攸关的情节，不禁开始回忆起我们相处的一切（走马灯（雾））。我们每天晚上熬夜聊天到很晚但也不觉得疲惫或厌倦，一起走在路上的时候我总是忍不住想看着你，一起去看电影的时候差点因为迷路没有赶上；一起坐地铁的时候两个人都安静地沉默；一起看烟花的时候你脸上快乐的笑容；还有在路边等车的时候我们开心地聊天；以及圣诞节晚上的匆匆一瞥。虽然我们认识的时间没有很长但是却充满了默契；有着相似又不同的，互相吸引的内心，以及无比契合的灵魂。毫无疑问，这种感情就是爱。而我们如此相似，如果我感受到的感情是爱的话，那么对你来说——也许也是一样。 我的大脑又恢复了知觉，音乐声停下了，我缓缓开口。我说起我的诗，正在犹豫着如何表达，你却把它从手机壳里面抽了出来。这一秒钟我明白了一切，你作为一个女孩子会把它无比珍视地带在身上意味着什么，原来一直以来，你的心情也是一样的——我感觉脑海里一片轰鸣，大脑再次变得一片空白，仿佛遭受了重重的一击，彻底忘记了我脑中准备的发言。但是这些都不重要了，这个结局已经足够圆满。现在回想起来，我仍然坚信，这就是世界上最甜的爱情故事。 所以昨天晚上我们看到动画结局的时候，我实在感到唏嘘不已。之前每次看到这里，我都是一样的感觉：心里觉得空落落的，美好的故事结束了。有时候也会羡慕地想，如果现实中也有这样的爱情那该多好，然后陷入孤独又惆怅的情绪。一开始看到这里我也会感到伤心想哭，故事实在是太美好了，而且还是这种有一点缺憾的故事，让人感觉故事本身情节有点伤心的同时，又想亲身经历这些故事，但理性又告诉我这是不可能的，然后产生巨大的落差感，造成了更深的伤感的情绪。当初看哈利波特的时候也是这样，看小说的时候感觉无比真实就像自己在亲身经历一样，但是故事突然结束了，自己不得不被拉回到现实中，刚刚仿佛还是亲身经历的故事，瞬间就变成了回忆，有一种再也回不来了的感觉。 但是这一次，我心里的惆怅似乎变淡了一些，在看到结局的时候，更多的是我们自己的回忆，因为有你在身边。这一次我知道了现实里也是有甜甜的恋爱的，也是有理想的爱情的，情节精彩到小说都不敢这么写。之前每次看完结局都会给我带来失落感，但是我没有想到，真的有一天我们也可以成为完美故事的主角。 也许将来会有人问起我们在一起的经历，一定会有人问是谁先追的谁呀，谁先表白的呀，诸如此类的问题。他们绝对想不到我们两个人都怂得不行，一直都是互相暗恋，就连最终表白的时候，几乎都可以算是同时表白的。一直以来，我们都是抱着一样的心情，抱着一样的的默契。这么看来——其实从一开始，我们就已经注定在一起了。 4.diaying12 生气的时候看看这个 我们还有很多美好回忆呀。就算现在觉得对方多么烦多么不可理喻，我们也会一直走下去呀。别生气啦。 写fgo脚本时同样的感受 玩我的游戏同样的感动 一起谈论看过的动画 校十大比赛你送我的票 每天晚上都聊到很晚却依依不舍 一起吃瓜谈论别人的故事，一起当调查员 一起感叹价值观的相似。我们真的真的很像呀，只不过在一起之后可能习惯了占大多数的相似之处，发现一点点小小的不同，自然也会引起注意 一不小心这篇博客没关预览，被你看到了 你在图书馆六楼发说说，巧合的是我正好也在六楼 第一次一起去看电影的回忆。路上一直欢快的氛围，一不小心差点迷路没赶上电影。出发之前你害羞的话语 给你送圣诞礼物时在图书馆的走廊里等你的心情 圣诞节晚上的相遇 一起聊明日方舟的新剧情 去听音乐会路上的交谈和中二脑洞交换 圣诞礼物和诗。共看烟火人间。 去博物馆抽卡，路上一起吃东西，看着你的侧脸 去花博会，一起坐在长椅上吃东西 烟花大会，烟花的时候欲言又止的感觉 ktv里唱歌的氛围 最后也是最初的时刻，稍稍有些害羞的表白（其实还是没说出口）。我们在一起啦。 刚刚脱单之后，一起走在路上的巨大兴奋感 回到学校去吃鸡排，一起在空旷无人的马路走着 你走进宿舍楼蹦蹦跳跳地高兴地上楼 考试周，一起去东一食堂吃早饭 自习的时候偶尔抬头看到你 去自习和回寝室的路上一边聊一边散步 被室友吃瓜后去操场散步 吃完晚饭回到紫松，看着天边苍茫的风景 一边看风景一边聊天，看着天色暗下去，远处五颜六色的灯光，一边开心地拍照 为了躲其他人，去最安全不会有人来的百惠园吃饭 下雨的时候一起打伞去食堂 我们其中一个人出发去考试，另一个人小声地说考试加油呀 路上小心翼翼地躲着别人，送你到宿舍楼下 还发现了通往东九舍的杂草丛生的新路径 你送给我的抱枕和零食，之后在学校的时间里，我一直抱着它写代码 感觉可以闻到淡淡的你的味道 最后地铁站前离别的拥抱。当时谁能想到，这一别就是半年。 晚上一起谈论各种话题，一起玩mc，打平安京，看各种视频 一起看b站拜年祭 看着窗外的烟花一起说新年快乐 一起看月刊少女野崎君，看灵笼 偶尔你会失眠，我半夜有时候也睡不着，聊天到很晚很晚 一起模拟吵架 一起出去散步打电话 一起联机学习 一起玩解谜游戏 试图量手指的长度比谁的手指更长 一起看石头门，猫和老鼠，看大侦探皮卡丘和哈利波特，看歌手 一起聊如果被绑架了要对的暗号 一起想着以后去哪哪哪旅游 做各种奇奇怪怪的情侣测试 我很低落失意的时候你也会安慰我 我自卑的时候你告诉我，我也是值得被喜欢的 现在才发现，也许和你有关的所有事情，无论当时看着多么平淡，在见不到你的时候回忆起来，都是内心珍贵的记忆 我童年的很多记忆已经暗淡了。每次回想都感觉只有些隐约的印象，但有什么东西已经抓不住了。但是和你的记忆我想全部清晰地保留下来。 我….真的好想你呀。]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WhyのＣ艹学习笔记！]]></title>
    <url>%2F2019%2F03%2F16%2FWhy%E3%81%AEC%E8%89%B9%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%81%2F</url>
    <content type="text"><![CDATA[每当你看到我更新这篇博客，就说明我又快考CSP惹( o _ o) std::vector 初始化:std::vector &lt;int&gt; v {1, 2, 3, 4}; 定义vector的迭代器：vector&lt;int&gt;::iterator iv; 迭代器有些类似指针，使用*iv来访问当前指向的元素。 v.begin(), v.end()返回v首/尾元素的迭代器 v.front(), v.back()返回v首/尾元素的引用 v.empty():如果v为空，则返回true,否则返回false。 v.size():返回v中元素的个数。 v.push_back(t):在v的末尾增加一个值为t的元素。 v.reserve(100)：设置vector要预留的元素容纳数量 v.assign(10, 2)：将10个值为2的元素赋到vector中 v.capacity()：返回vector所能容纳的元素数量(在不重新分配内存的情况下） 插入: 12v.insert(v.begin() + 3, 99); v.insert(v.end() - 3, 99); 删除: 123v.erase(v.begin() + 3); //删除第三个元素v.erase(v.begin(), v.begin() + 3); //删除前三个元素v.pop_back(); //删除最后元素 清空：v.clear() std::pair pair &lt;int, int&gt; p 许多函数的返回值是pair类型。 std::map 单独使用要inclde &lt;map&gt; 初始化: std::map &lt;int, string&gt; m; 初始化迭代器:map&lt;int, string&gt;::iterator im; 成员是pair类型。 多种插入方法: 1234pair&lt;map&lt;int, string&gt;::iterator, bool&gt; res;res = m.insert(pair &lt;int, string&gt; (1, "Alice")); // 若插入失败，res的第二个元素是falsem.insert(map&lt;int, string&gt;::value_type (1, "Bob"));m[9] = "Candy"; 要注意map虽然是根据key索引，但其中元素是有序的。 附：获得pair中的元素:p.first, p.second 查找key：m.find(2) 删除: 123m.erase(2); //根据key删除m.erase(m.begin()+4);m.erase(m.begin(), m.end()-5); 清空：m.clear() std::set 单独使用#include &lt;set&gt; 创建：set&lt;int&gt; s 方法： 12345678begin() //返回set容器的第一个元素end() //返回set容器的最后一个元素clear() //删除set容器中的所有的元素empty() //判断set容器是否为空max_size() //返回set容器可能包含的元素最大个数size() //返回当前set容器中的元素个数rbegin //返回的值和end()相同rend() //返回的值和rbegin()相同 queue 1234567queue &lt;int&gt; q;q.push(1);q.pop()q.size();q.front; q.back();q.empty() 上述方法含义同vector。 string 初始化string s1 = &quot;hello&quot;； 可以进行直接赋值和相加操作。 求长度：s1.size()或s1.length() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647- 多种初始化： string s1(); // s1 = "" string s2("Hello"); // s2 = "Hello" string s3(4, 'K'); // s3 = "KKKK" string s4("12345", 1, 3); //s4="12345"的从下标1开始 长度3的子串- 求子串： s2 = s1.substr(); //获取整个s1 s2 = s1.substr(3); //获取从s1[3]到末尾 s2 = s1.substr(2, 4); //获取从s1[2]开始，长度为4的子串- 查找： where = s1.find(str2); where = s1.find(str2,pos1); //从s1[pos1]开始到末尾的范围内搜索。 where = s1.rfind(str2); //从后往前搜，返回最后出现的结果 - 插入字符串: str1.insert(pos1,str2); str1.insert(pos1,str2,pos2,len2); str1.insert(pos1,numchar,char); //numchar是插入次数，char是要插入的字符. str1.append("233") - 替换字符串: str1.replace(pos1,str2); str1.replace(pos1,str2,pos2,len2); - 删除字符串: str.erase(pos,len) str.clear(); - 比较：compare 成员函数有以下返回值： 小于 0 表示当前的字符串小； 等于 0 表示两个字符串相等；m 大于 0 表示另一个字符串小。string s1("hello"), s2("hello, world");int n = s1.compare(s2);n = s1.compare(1, 2, s2, 0, 3); //比较s1的子串 (1,2) 和s2的子串 (0,3)n = s1.compare(0, 2, s2); // 比较s1的子串 (0,2) 和 s2n = s1.compare("Hello");n = s1.compare(1, 2, "Hello"); //比较 s1 的子串(1,2)和"Hello”n = s1.compare(1,2,"Hello",1,2);//比较s1的子串(1,2)和"Hello"子串(1,2)- 交换：string s1("West”), s2("East");s1.swap(s2); // s1 = "East"，s2 = "West" list 双向链表。除了顺序容器都有的成员函数外，list 容器还独有下列成员函数： 成员函数或成员函数模板 作 用 void push_front(const T &amp; val) 将 val 插入链表最前面 void pop_front() 删除链表最前面的元素 void sort() 将链表从小到大排序 void remove (const T &amp; val) 删除和 val 相等的元素 remove_if 删除符合某种条件的元素 void unique() 删除所有和前一个元素相等的元素 void merge(list &lt;T&gt; &amp; x) 将链表 x 合并进来并清空 x。要求链表自身和 x 都是有序的 void splice(iterator i, list &lt;T&gt; &amp; x, iterator first, iterator last) 在位置 i 前面插入链表 x 中的区间 [first, last)，并在链表 x 中删除该区间。链表自身和链表 x 可以是同一个链表，只要 i 不在 [first, last) 中即可 #include&lt;algorithm&gt; sort(ia, ib, func), ia, ib是表示排序起始位置的迭代器(也可是指针)，func是定义排序方式的函数名，接受两个参数，返回bool。例如升序排序： 123bool func (int a, int b) &#123; return a &gt; b; &#125; stable_sort：稳定排序，参数同上。 部分排序：只排最小的n个：partial_sort(begin, begin+n, end) 找到第n小的元素：nth_element(begin, begin+n, end) max(a, b); min(a, b); is_sorted(ia, ib, func)：返回该范围内是否已经有序。参数含义同上。 reverse(it,it2) 可以将数组指针在[it,it2)之间的元素 或容器的迭代器在[it,it2)范围内的元素进行反转。 找元素:find(begin, end, value)。如果找到，返回迭代器；找不到则返回end的迭代器。 使用二分查找，查找大于等于value的数第一次出现的位置：lower_bound(begin, end, value) 使用二分查找，查找大于value的数第一次出现的位置：lower_bound(begin, end, value) 注意：上述两个函数要求数列必须有序，因为使用的时二分查找。 过滤：all_of(), any_of(), none_of()。参数均为begin, end, function。检测序列元素是否使得function返回true unique(begin, end, function)可以在序列中原地移除重复元素，在移除重复元素后，它会返回一个正向迭代器作为新序列的结束迭代器。可以提供一个函数对象作为可选的第三个参数，这个参数会定义一个用来代替 == 比较元素的方法。 对每个元素作用某函数: for_each(begin, end, func). replace(begin, end, old_value, new_value)或者replace(begin, end, fucntion, new_value)，把满足function的元素做替换 带_copy后缀的方法，都不是原地操作。 #include &lt;cstring&gt; c语言中的字符串操作. strcpy(s1, s2)：复制字符串 s2 到字符串 s1。 strcat(s1, s2)：连接字符串 s2 到字符串 s1 的末尾。 strlen(s1)：返回字符串 s1 的长度。 strcmp(s1, s2)：如果 s1 和 s2 是相同的，则返回 0；如果 s1&lt;s2 则返回值小于 0；如果 s1&gt;s2 则返回值大于 0。 strchr(s1, ch)：返回一个指针，指向字符串 s1 中字符 ch 的第一次出现的位置。 strstr(s1, s2)：返回一个指针，指向字符串 s1 中字符串 s2 的第一次出现的位置。 #include&lt;stack&gt; stack &lt;int&gt; s s.push(x), s.pop(), s.top()返回栈顶元素的引用 s.size(), s.empty() 一些常用的东西 读取一行输入到string：getline(cin, string)，结合string.find可以进行复杂的字符串解析。 文件重定向： 12345678#include &lt;fstream&gt;// #define DEBUG#ifdef DEBUG freopen("in.txt", "r", stdin); ifstream in("in.txt"); cin.rdbuf(in.purdbuf());#endif]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习超分辨率算法]]></title>
    <url>%2F2019%2F03%2F05%2F%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6xio%E4%B9%A0%E8%BF%9B%E8%A1%8C%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[任务背景 超分辨率，指的是对降采样图片进行高分辨率重建的过程。可以分为单图像超分辨率(SISR)以及多图像超分辨率(MISR)。在使用深度学习方法处理超分辨率任务时，常常面临各种问题: 使用的损失函数往往是像素级（pixel wise），很难描述图片的整体质量。 解决：使用与训练网络提取后的feature map计算loss来表示全局特征 目前的主流算法大致可以分为：基于插值、基于重建以及基于学习的算法。 评价指标 给定原图\(G\)(ground truth)以及重建后的图像\(O\)(output)： MSE：均方误差，像素级求误差，逐像素求和，最后求平均。 \[MSE={1\over n} \sum_{i\in all\ pix}||G(i)-O(i)||^2\] RMSE：\(\sqrt{MSE}\) PSNR：峰值信噪比，一定程度上可以反应主观感受。 \[PSNR=10log_{10}\left(max^2(G)\over{MSE}\right)\] SSIM：结构相似度，可以较好地反映人眼主观感受。一般取值范围在0-1，值越大质量越好。计算时使用一滑动窗口，窗口逐像素移动，计算每一步下的相似度，最后求平均。 相关工作 SR-CNN 最早使用CNN进行深度学习的版本。使用LR的双三次插值作为网络输入（由于没有使用反卷积，插值后的图像甚至比HR尺寸更大），使用CNN拟合超分辨率映射。使用bicubic插值作为网络输入，浪费了很多时间，并且插值后的图像过于模糊，由于原图像的退化方式未知，相当于是对原图像的分布进行了错误的先验估计，不利于重建细节。 改进： FSRCNN，使用了反卷积。其中的反卷积本质是先将LR进行近邻采样，后进行普通卷积。 ESPCN 改进了上采样操作。使用子像素卷积进行上采样。 （话说上述这三个上采样我感觉差不多啊 为啥效果差别这么大…） VDSR 使用了深度网络（VGG-20），来获得更大的感受野。为了训练深层网络，使用了梯度裁剪（防止梯度爆炸）以及更大的学习率（加速训练），但是也使用了双三次插值后的图像作为输入（这又是为啥啊）。 考虑到超分辨率任务学习的是LR到HR的映射，二者只有一些细节差别，不需要像图像分类一样提取高维度feature map，因此只要学习LR与HR之间的差别（也就是残差）即可。因此使用了残差学习。同时残差块也可以加速收敛，防止梯度消失。 DRCN 使用了RNN共享参数的思想来平衡更深网络带来的参数量增加 以及过浅的网络带来的容量不足。但是使用RNN会导致梯度爆炸/消失的情况，因此使用了全局的残差连接。(即输入的残差连接直接跳接到最后的重建层)。在由卷积层组成的RNN中，每一个时间步的输出都跳接到最后的重建层。因此若RNN中进行了n次递归，最终将获得n个重建结果，这些结果与输入LR一起，进入最后的重建层进行重建，获得n个输出，这些输出加权平均得到最终结果。 损失函数使用多目标优化的思想，是每一个RNN时间步的输出误差以及最终的输出误差的加权和。 DRRN 在DRCN的基础上，去掉了RNN中每一时间步到最终输出的跳跃连接，并在RNN的每一个递归单元中加入了局部残差连接。同时使用了局部和全局的残差连接。提升并不很大… SRDenseNet 使用了DenseNet中的稠密连接，作者在稠密连接的基础上提出了两种方案：仅适用全局残差连接以及使用全局和局部残差连接，事实证明使用更多残差连接的效果更好。 EDSR 在Resnet的基础上，去除了BN（提高效率，在超分辨率任务中，不需要对数据分布和尺度进行缩放，只需要进行对输入的细节调整，例如亮度、、对比度等，因此BN往往会限制超分辨率性能）以及残差连接后的ReLU。 在训练过程中，使用循序渐进的训练方法。例如要训练x4的网络，先训练x2网络，待收敛后再以该网络作为预训练模型训练x4，可以提高收敛速度。 作者也提出了一种多尺度网络MDSR：在输入之前将LR通过预训练的x2, x3, x4网络，在输出之后，通过不同尺度的上采样层，以获得不同尺度的超分辨率结果。 WDSR 在EDSR的基础上，去除了冗余的卷积层（经过实验发现性能没有下降），同时将大卷积核拆分成小卷积核，并增加filter的数目，在保证参数量不变的情况下，使得激活函数的输入获得更大的感受野。同时去除BN，改用WN，对参数进行归一化。 SRGAN 使用预训练的VGG分别对\(SR\)图像和\(HR\)(原图)提取出的特征图之间的欧氏距离表示内容损失Content loss。 使用\(D\)接受\(SR\)图像后的输出结果（伪图片为真的概率）的负对数（信息熵）来作为Adversarial loss（对抗损失），也就是原版GAN中的损失函数。 使用GAN架构： 生成器\(G\)使用resnet中的残差连接，并使用PReLU(负饱和区变为可学习的线性函数)作为激活函数 生成器前端使用sub-pixel conv来进行上采样。具体做法以放大二倍为例，使用普通卷积提取4张feature map，然后将其逐像素合并（四张特征图的相同位置的像素组合成新图的一个像素，即放大了两倍）。 判别器\(D\)使用VGG，LeakyRelu作为损失函数，并去掉最大池化。 判别器中使用空洞卷积。 一些灵感 使用StyleGan的方法 使用autoML 使用VAE]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[why家今天的饭！]]></title>
    <url>%2F2019%2F01%2F18%2Fwhy%E5%AE%B6%E4%BB%8A%E5%A4%A9%E7%9A%84%E9%A5%AD%EF%BC%81%2F</url>
    <content type="text"><![CDATA[从今天开始窝就是美食博主！我不写代码啦jojo！（雾） 事先声明 这篇博客仅用作个人记录 虽然超级简单但毕竟是第一道菜姑且还是记录一下の咖喱饭 需要食材 量 大米 一碗 咖喱酱 一小块 差不多超市一盒的\(1\over6\) 鸡胸肉（冻） 一块 土豆 大的半颗小的一颗 小葱 1根 鸡蛋 1个 各种调料 盐 胡椒粉 料酒 酱油 （嗯…以上食材用量是按我的饭量来的） 123456789101112131415161718192021222324252627282930313233# 作为一名程序员 窝决定用这种方式来写步骤#（说好的美食博主呢喂！）# 0.蒸饭冻鸡肉.unfreeze() # 让鸡肉线程先自己解冻去淘米()电饭煲.add(淘过的米).add(清水) # 这里保证水：米=2: 1电饭煲.run() # 1.炒鸡蛋蛋液 = 打鸡蛋()蛋液.add(一小勺盐, 一小勺料酒, 一小勺葱花)锅.add(少量油) # 能润一下锅就可锅.add(鸡蛋).炒()# 2.炒鸡肉&amp;土豆土豆块 = 土豆.cut(片).cut(条).cut(丁)if (鸡肉已经解冻): 鸡肉.cut(条).cut(柳).cut(丁)锅.add(少量油).add(鸡肉).炒()until(鸡肉断生) # 肉色发白即可锅.add(花椒, 料酒)锅.add(土豆块).炒()锅.add(酱油, 炒蛋, 适量水, 咖喱酱).盖上锅盖焖()until(米饭熟)# 3.merge！出锅装盘() 奇形怪状の面食二重奏 众所周知山西面食博大精深（且奇形怪状 雾）这里要做的分别是拌汤和炒揪片。 食材：面粉（量取决于你多能吃），以及任意可以煮的菜，基本调味料（盐 醋 酱油啥的），任意可以炒的辅料（鸡蛋啊各种肉啊啥的） 1234567891011121314151617181920# 0. 和面盆.add(面粉).add(水) # 水必须缓慢加 直到面到合适的硬度# 如果是拌汤 絮状的面和菜一起直接下锅就做好了（是真滴简单） 和面() # 直到面团表面光滑盆.add(盖) # 醒面time.sleep(10min)# 1. 煮面面团.apply(擀面) 加工() # 切/揪/捻/捏成合适的形状 取决于你吃啥煮锅.add(水).heat()煮锅.add(面).add(其他菜) # 煮熟为止# 3.炒面炒锅.add(油).add(蛋液).炒() # 哦因为我喜欢蛋炒面 这里放肉也可 炒肉的话记得下料酒炒锅.add(面).add(面汤).add(你喜欢的随便什么调味料) # 好随意啊淦出锅()]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[考完就不看の操作系统笔记！]]></title>
    <url>%2F2018%2F12%2F24%2F%E8%80%83%E5%AE%8C%E5%B0%B1%E4%B8%8D%E7%9C%8B%E3%81%AE%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0%EF%BC%81%2F</url>
    <content type="text"><![CDATA[Review 中断：特定事件发生后，先跳去执行中断处理程序，执行结束后返回断点 中断分类：机器断电是机器故障中断；敲键盘是IO中断；printf是访管中断 linux中断分上下部 unix：层次结构； 用户接口分类：系统功能调用，操作命令接口 Windows 启动方式：滚雪球式 （第四章开始每章必考）第四章至少两道，一道简单一道难（？） 进程程序的最大区别：有无生命周期 独占资源（互斥）一定有一个mutex信号灯 死锁产生原因：系统资源不足，进程推进顺序非法 写pv题：最后手动拿一个样例跑一炮 移臂调度和电梯算法：同一个方向先做完再管另一个方向 死锁4个必要条件：互斥、不剥夺、占有并等待、有环路 死锁的避免：有序资源分配、银行家算法 考试会挖的坑：\(p155,figure\; 5.9\)弯道问题，\(5.10, 5.14\)，银行家算法中加各种奇怪条件（比如加入资源到达时间的限制） 上下界寄存器：针对的是物理地址；基址限长寄存器：针对逻辑地址 缓冲调度重点看第一次有没有命中 页面大小和块大小是一一对应的（一页里，装且只装一个块） 设备无关性：用户使用的设备与物理设备无关（逻辑设备） 缓冲：在两种不同速度的设之间平滑传输信息的技术 linux和unix对设备的管理：当作文件 三种文件：普通文件、目录文件、特殊文件（设备文件） 第九章两道或以上答题 文件后缀：标明文件属性 文件的组织结构：逻辑结构、物理结构 文件物理结构：连续（查找快，增加慢）、串联（查找慢，增加快）、索引（二者兼得） systemV索引结构：前十个是一级索引，一个二级索引，一个三级索引 考试10道大题，两道pv，一道自己写一道填空。存储有缓冲区队列题（一定要画图）。文件系统有。考定义的简答题只有一道很简单。 快表。 Cp1.概论 单道程序设计技术：完全串行，无法分别处理不同任务。一个程序阻塞，其他程序也被阻塞。 多道程序设计技术：多个程序之间（宏观上）并行，微观上串行。不同的程序使用不同的“道”，多道之间穿插地运行，一个程序阻塞，可以马上运行其他程序。 多道运行特点：多道、微观并行、宏观串行 分时技术：cpu时间划分为时间片，大家轮流用。 实时处理：对外来信息能在其允许的deadline内做出反应。（反应快速） 操作系统：一个大型的系统，负责软硬件资源分配，控制并发活动，提供用户接口。 操作系统特征：并发、共享、不确定性（可以处理不确定的事件序列） 总览：操作系统资源管理： 管cpu（先给谁用、谁用多久）：进程调度，处理机分派 存储器管理（谁的东西该放在哪、占多少）：存储分配、存储无关性、存储保护、存储扩充（虚拟存储） 设备管理：设备无关性、设备分配、设备传输控制 文件系统：存取方法、文件共享、安全性、完整性、分配磁盘空间 操作系统分类： 批量操作系统：将程序组织为作业，批量处理。 分时操作系统：使用时间片轮转法。特点：并行性、独占性、交互性 实时系统：可以进行实时处理。特点：可靠性、安全性、及时响应 多处理机系统：并行系统，单机-&gt;集群（例如多核cpu） 分布式系统：多个资源部件是分布的，经过通信网络相互作用。系统对资源进行全局的管理。 Cp2.结构 操作系统虚拟机：裸机+操作系统程序 指令系统： 裸机的指令系统只有机器指令； 操作系统虚拟机的指令系统有操作命令（作业控制语言、键盘命令、图形化界面）和系统功能调用 操作系统结构： 单体结构：一整块 模块化结构 可扩展结构 层次结构 unix结构：一层一层壳 linux：一块块不同的模块 windows：不同组件 处理机の特权级： 管态：操作系统管理程序的特权级。可以使用全部指令、全部资源 用户态：用户程序执行时的状态，不能使用特权指令，只允许访问自己的存储区域。 中断：某个事件发生，系统暂停工作处理该事件，处理完毕后又继续回到断点执行。 IO中断 外中断：例如命令行Ctrl-C 故障中断：例如电源故障 程序性中断：例如程序溢出、非法操作（0/1） 访管中断：对操作系统提出某种需求发出的中断。 按来源分类：中断（cpu外部中断）&amp;俘获（cpu内部中断） 程序状态字（psw）：反映程序执行时机器所处的现行状态的代码。包括指令地址（pc），指令执行情况，cpu状态等。例如x86汇编中，psw包括CS, IP, flag. 响应中断需要的硬件支持：指令计数器（pc），cpu状态寄存器，堆栈，中断矢量表 中断响应的实质：pc，ps入栈，保存现场。 Cp3.用户接口 操作系统的初启动：系统引导：将必要部分装入主存。 引导方式分为：现场独立引导方式（滚雪球，例如Linux），下载方式（从别的机器下载主要文件） 滚雪球步骤：加电/复位，bios启动，启动引导程序，内核初始化。 应用程序的处理：编辑，编译，链接，运行 链接类型： 静态（如果多个不同程序调用同一个库文件，会连接多次）：在编译时就把需要的额外代码直接一起放入可执行程序中，在发布时就不再需要依赖库。 动态（只会链接一次）：编译时只向可执行文件放入库地址，在运行时才链接 操作系统提供的用户界面：命令接口，程序接口 不同的操作命令： 作业控制语言（JCL）：提供系统的作业控制 键盘命令：命令行 图形界面：ui 系统功能调用：通过访管指令和访管中断实现。 Cp4. 进程管理 顺序程序：程序顺序执行。在单道系统中，无法实现并行。结果可再现。具有封闭性（即一旦开始运行，就不受外界影响） 并发程序：可以实现宏观上的并行。一个程序尚未结束，也可以开始执行另一个程序。失去了封闭性和可再现性。 进程和程序的区别：一静一动；进程是独立运行的活动单位；进程是竞争系统资源的基本单位；一个程序可以对应多进程，一个进程至少包含一个程序。 进程的状态：运行；等待（被阻塞）；就绪（只待cpu） 大量进程的组织：处于不同的状态的进程们分别属于不同的队列（例如运行队列，等待队列，就绪队列） 进程的组成：程序＋数据＋PCB 进程控制块（PCB）：包含pid，进程当前的状态，next指针（指向当前队列的下一个进程），进程优先级，cpu现场保护区，和别的进程的通信信息，家族关系，占有资源清单。 资源争用： 互斥（临界）资源：同时仅允许一个进程使用。 临界区：指的是某个进程中的一段代码，在该段代码中，该进程对临界变量进行了审查、修改操作。 进程同步：不同进程之间传递消息。 锁：0表示未🔒（可用），1表示已上🔒。 信号灯：一个信号灯由二元组&lt;s, q&gt;组成。s是该灯当前的值，q值为试图获取该灯被阻塞的队列（没p到该信号灯的进程）。在pv操作中，若信号灯值&gt;0，表示可用的资源个数，&lt;0则其绝对值表示等待使用该资源的进程个数。 所谓的[原语]，其实指的是原子操作，即该操作不可被分割，不可被中断。 生产者消费者问题：生产者生产资源（执行v操作，资源增加），p空位；消费者p资源，消费资源，取走资源后，v空位。 因此需要两个信号灯，分别表示缓冲区中的空位数目和满位数目。 同时需要设置锁，防止同读同写。 对于多个生产者消费者的情况，上述模板仍然使用（只要没有啥奇奇怪怪的条件XD） 进程通信（IPC）： 消息缓冲通信：使用约定的格式收发消息 信箱：嗯这个大概不考8 我不复习了jojo！ 线程：共享分给父进程的内存，有自己私有的堆栈和cpu环境。 用户线程＆内核线程：在用户空间中调用的线程。无需内核干预。内核线程：完全由os管理 fork()：为新进程分配pcb，pid，增加与该进程关联的文件表和索引节点（也就是说，父进程已经打开的文件子进程也可继续使用） fork类型程序的打印顺序：考虑父子进程分别先后执行、以及穿插执行的情况。 exec()：传入可执行文件名，执行该文件。实际上，是把当前进程的执行内容替换为了一个新进程。因此，exec()语句之后的语句不会被执行，执行完exec后直接返回。 等待进程/线程终止： wait()，挂起当前进程，直到任意一个子进程返回。 waitpid(pid, status)，挂起直到特定的子进程返回。 进程调度＆分派：在众多就绪态的进程中，选择一个运行。 调度是把一个新进程插入就绪队列的合适位置；分派是将程序从就绪队列移出并运行。 调度策略： 先来先服务 优先数调度：为每个进程分配优先数，根据该数字大小来决定优先级。 循环轮转调度 Cp5. 资源管理 资源分配策略：资源选择请求者；请求者选择资源。 常用策略：先到先请求；优先调度；移臂调度（电梯）；旋转调度（直接找最近） 死锁原因：资源不足，进程推进顺序非法。 产生的必要条件：资源互斥，进程之间不剥夺，部分分配（不会一次申请完），环路条件（存在依赖链） 资源请求矩阵：每一行表示某一个进程需要的各个编号的资源的个数。 资源分配矩阵：每一行表示某一个进程当前占有的各个编号的资源的个数。 预防死锁： 静态分配； 有序分配：为每个资源都编号，分配资源必须升序 银行家算法：进程申报自己对各个资源的最大可能的需求量，系统看当前资源够不够，不够就拒绝。 Cp.6 cpu调度 两级调度：宏观决定那些程序调入os，微观决定当前哪个进程占用cpu。 批处理系统：有两层，以作业为调度单位，在决定了作业的基础上再调度进程 分时操作系统：以进程为调度单位 作业的状态：与进程类似，有提交状态（用户刚刚提交），后备状态（作业录入到后备存储设备），执行状态（调入内存），完成状态。 作业调度使用的数据结构：作业控制块JCB，记录已经进入os的各个作业的状态。 作业周转时间：作业的完成时间－提交时间。包含了排队时间。 平均周转时间：各个作业的周转时间求平均。 带权周转时间：周转时间/实际的运行时间。该值大于等于1。 平均带权周转时间：各个带权周转时间求平均 作业调度算法：先来先服务，短作业优先（总选当前耗时最短的来执行）；响应比高者优先（响应比为1+等待时间/运行时间）；优先数调度算法 进程调度算法： 循环轮转（RR）：时间片q足够大时，等效为先到先服务调度。 linux进程调度：动态优先数，且为可抢占式。 Cp.7 主存管理 主存管理的功能：地址映射（逻辑地址-&gt;物理地址）；为不同用户分配主存；存储保护；扩充虚拟主存。 逻辑结构： 一维地址 二维地址：段地址+偏移 主存空间：物理地址的集合 程序地址空间：逻辑地址的集合 静态地址映射：程序装入主存时，即进行映射 动态地址映射：运行时才映射（例如，通过给逻辑地址加上一个偏移量（段地址），该偏移量存储在重定位寄存器中） 主存分配使用的数据结构：主存资源信息块(M_RIB) 主存分配中涉及的策略： 分配（主存块选请求者）； 放置（请求者选主存块）； 调入策略（决定信息什么时候装入主存）； 淘汰（主存满时，淘汰哪些信息） 虚拟存储器：swap分区。需要提供地址变换机构（用来映射地址） 存储保护：保证各个用户程序只能在给定的区域内活动。 上下界保护：分别用上下界寄存器存储当前程序的上下界，越界会发生中断。 基地址限长保护：使用基址寄存器＋限长寄存器保护 存储分区：使用主存资源信息块＋分区描述器＋空闲区队列来管理，负责分配当前程序使用哪一块主存。 空闲区队列结构：一个链表，首指针是第一块空闲区首地址，之后每一个节点表示一块空闲区，有三个成员：flag（始终为0，表示空闲），分区大小，next指针 放置策略： 首次匹配：尽量选低地址的 最佳匹配：尽量选最接近大小的 最坏匹配：尽量选最大的（与程序大小差距最大的） 碎片拼接：把多个零散的小空闲区拼起来。 页面（虚页）：程序地址空间-&gt;一个个大小相等的页面/虚页 主存块（实页）：主存等分得到。 主存中的实页和程序空间中的虚页具有映射关系。该映射关系通过页面存储。每个程序有自己的页表。 相联存储器（联想存储器）：cache； 快表：在cache中存放正在运行的进程使用到的页表，因为很快所以叫做快表。起名鬼才.jpg 使用关联存储器时的映射过程： 获得逻辑地址后，首先提取出页号、偏移 使用页号去联想存储器中查询，如果命中则直接获得物理空间中该页对应的主存块； 未命中时，再去主存中的页表查询。 请求型页式机制：装入一个程序的一部分页面即可运行。 扩充页表：再增设一个中断位（0表示该页在主存），一个辅存地址（表示该页在辅存中的位置） 缺页处理：即查询页表时，发现要查的页中断位＝1（不在主存），此时发生缺页中断。此时说明要访问的页不在主存，在辅存中，需要将其调入主存。如果主存有空闲位置，则直接调入，否则需要淘汰当前程序在主存中存在的其他页。 再次扩充页表：增设引用位（表示该页最近是否有被访问过），和改变位（该页是否被修改） 淘汰（置换）算法： 最佳算法：淘汰在最长的时间之后才会用到的页（或者最好永远不会再用到） 先进先出：选择最早进入主存的页（需要记录各个页进入主存的顺序） LRU（最久未使用）：就组原里那个，完 全 一 致。软件实现可以使用一个栈，每访问一页，则调整栈中各个页号的次序。 需要注意，页式存储地址仍然是一维的。 段式存储：一个分段是一片连续区域（段大小未必都相等）。 段式地址映射机构：每一项需要记录段号、段长和基址 段页存储：地址是二维的，一个段中有多个页。 段表：每一项存储段号、页表的长度和页表基址。 Cp.8 设备管理 分类：存储设备，io设备，通信设备 设备管理目标：提高利用率，方便用户使用。 统一性：为各种不同设备提供一致的界面 独立性：逻辑设备与物理设备无关 设备管理的功能：状态跟踪，设备分配与回收，设备控制 设备独立性实现：使用高级语言中的软通道实现；批处理系统中联接语句实现；指派命令实现 总之，实现方法就是把一个变量/逻辑结构和实际的设备关联 设备独立性优点：方便用户，提高利用率，提高可扩展性。 管理设备使用的数据结构：设备控制块DCB。 缓冲：用来暂时存放io数据的区域。 引入缓冲的原因：生产、消费者速度有差异；传输数据速度不一致；可以用来实现应用程序的拷贝 进程请求读取键盘输入的过程： 进程从os获得一个空buffer 键盘输入的内容-&gt;buffer 用户请求时，os将buffer内容提出，发送到用户进程存储区 若buffer空，进程还在请求，这该进程挂起，等待输入。 进程请求写内容到输出设备： 进程获得空buffer 进程向buffer写入 缓冲区满时，os才将其内容写入输出设备 若缓冲区已满，且进程仍想继续输出，则进程挂起。 常用缓冲： 双缓冲：生产者先向buf1填满内容，然后消费者取buf1的时候，生产者填buf2，如此循环。 设备分配： 独享分配：一个作业执行时独占该设备 共享分配：多个作业、进程共同使用 虚拟分配：在一类物理设备上模拟另一类物理设备，可以把独占设备转换为共享设备。 spoooling系统：一种把独占物理设备变成多个虚拟设备的手段，提供外围设备同时联机操作。 例如用户请求使用正在被占用的打印机，spooling系统仍会同意打印请求，然后再输出井中为其创建buffer，并将待打印内容填入其中，排队等待打印（缓输出） -对于输入设备，spooling系统则将其输入内容放入输入井，使其可被多个程序共享。 io控制方式： 循环测试io方式 io中断方式 通道方式 dma方式 io子系统：用来实施对设备的控制和操作，通过设备处理程序直接控制设备 控制io核心模块的方式： 为每个设备设置设备处理进程，io请求到来时该进程启动，无请求则睡眠。 把设备作为文件 执行io过程的步骤： 逻辑设备名-&gt;物理设备名 检查io合法性 形成io请求块，并将其加入对应的设备请求队列 Cp.9 文件：逻辑上具有完整意义的信息的集合。由信息项和记录组成。 文件系统：负责管理、存取文件的软件机构 功能：实现了按名存取。 特点：使用简单，安全，既共享又保密 文件的结构： 逻辑记录：按照逻辑含义划分的信息单位 物理记录：在存储介质上的存放。 逻辑结构（文件组织）： 流式文件：无结构的，按照特殊字符为界或者按照信息个数存取。unix中所有文件都是流式文件。 记录式：结构化的。一个文件有若干记录组成。 定长记录：每个记录长度相等 变长记录：不定长。 存取方式：顺序方式，随机存取 物理结构： 连续文件：连续存储。目录项需要：文件名、文件大小、首地址。连续存取快，不易增删。 串联文件：存在若干分散的块中，每块最末是下一块指针。目录项需要：文件名、首地址。易于增删。 索引文件：仍然分散存储，但是这次建立索引表来指示每一块的地址（类比页表）。目录项只需要文件名和索引表地址。易于增删，可以读写任意记录。索引表中的表项存放逻辑块号和磁盘块号。 多级索引：上一级索引表中不存实际数据的磁盘块地址，而是下一级索引表地址。 文件目录：存放文件名、文件逻辑结构、文件物理结构（上面提到的内容）、权限、文件类型等。 文件目录分类： 一级文件目录：将所有文件都放到一张表。不允许文件同名。 树形文件目录：目录内容可以是另一个目录，或者一个文件。 文件链接🔗：一个文件目录中的文件指针直接指向源文件的目录表项（注意是指向它对应的目录表现，不是指向文件对应的磁盘块） 硬链接：一个文件的多个硬链接共享一个inode号（相当于都是一个文件的不同别名）。删除一个硬链接不影响对应磁盘块中的内容，因为还有别的硬链接指向它。 软链接：是一个另外的文件（inode号与指向的文件不同），只不过是在对其进行操作时，os自动将其路径修改为源文件的路径。 unix文件分类：普通文件、目录文件、特殊文件（设备） 第七版unix索引结构：一共8个指针： 小文件全是直接索引 大文件全是一级索引 超大文件最后一个指针是二级索引，其他还都是一级索引。 systemv文件索引结构：设磁盘块大小为\(a\)，则0~9指针直接指向磁盘块（\(10a\)），10号指针是一级索引(\(256\times a\))，11号是二级索引（\(256^2\times a\)），12号是三级索引（\(256^3\times a\)） 要看的0w0 进程状态图（等待、就绪啥的），或者问其中的因果变迁；pv题；银行家算法题目；作业调度算法；]]></content>
  </entry>
  <entry>
    <title><![CDATA[考完就不看の离散数学笔记！]]></title>
    <url>%2F2018%2F12%2F08%2F%E8%80%83%E5%AE%8C%E5%B0%B1%E4%B8%8D%E7%9C%8B%E3%81%AE%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0%EF%BC%81%2F</url>
    <content type="text"><![CDATA[计数原理 问题背景：N个物体放到k个盒子中 排列组合公式： \[C_n^m = \frac{n!}{m!(n-m)!}\] \[A_n^m = \frac{n!}{(n-m)!}=\prod_{i=n-m+1}^ni\] \[A_n^n = n!\] 广义鸽巢原理: 如果\(N\)个物体放置到\(k\)个盒子中,那么至少有一个盒子装有不少于\(⌈{N\over k}⌉\)个物体。 朋友敌人问题:假定有一组6个人,任两人非友即敌。证明存在三个人 彼此是朋友或者存在三人彼此是敌人。 解: 任选其中一人A出来,剩下的5个人中,分成两个子集,一 个是A的朋友的集合,另一个是A的敌人的集合。这两个集合至 少有一个多于两个人。至少有3个人要么是A朋友,要么是A的敌 人。如果三个人是A的朋友,那么如果这三个人之间有一对是朋 友,则已;否则这三个人本身就是彼此为敌人。 ★组合恒等式： \[C_n^k = \frac{k}{n} C_{n-1}^{k-1}\] \[Pascal: C_n^k = C^{k}_{n-1} + C_{n-1}^{k-1}\ \text{(杨辉三角中相邻两元素之和)}\] 有重复排列： \(n^k\)种 有重复组合：把\(N\)个相同的物体放到\(k\)个不同盒子中，有\(C_{k+n-1}^n\)中方法。相当于解不定方程： \[\sum_{i=0}^n x_i = k, x_i\in [0, r]\] ★分组问题：把\(N\)个不同的物体放到\(k\)个相同盒子中： 把 \(2n\) 个人分成 \(n\) 组,每组\(2\)人,有多少分法? 解: 相当于\(2n\) 不同的球放到 \(n\) 个相同的盒子,每个盒子\(2\)个。可以先考虑\(n\)个盒子是有标记有顺序的,分步处理:那么先选第一组\(2\)个,再选第\(2\)组\(2\)个,如此类推,一共有\(C(2n,2)C(2n-2,2)...C(2,2)\). 这个数字包括了所有可能的组的排列方法。但实际上\(n\)个盒子作为\(n\)组本身是无序的,所以就应该将有序的分组数再除以\(n\)个组的有序排列数\(n!\)，因此结果为： \[\frac{1}{n!}C(2n,2)C(2n-2,2)...C(2,2)\] 不相邻问题：从\(S={1, 2, ... , n}\)中选择 k 个不相邻的数,有多少种方法? \(k\)个数不相邻，相当于他们是剩下的\(n-k\)个数的隔板，因此有\(c_{N-K+1}^K\)种办法。 不可辨别对象排列：单词计数问题。把单词 SUCCESS 重新排列能得到多少不同的串? 单词长度固定为7，想象有7个位置，首先放入三个S，有\(C_7^3\)种方法；再放入两个C，有\(C_4^2\)种，再放入U和E，有\(A_2^2\)种。乘起来即可。 设\(n\)个对象中，有类型\(i\)的相同对象\(n_i\)个，共有\(\frac{n!}{\prod n_i!}\)种办法。 四个基本问题： 可辨别对象放入可辨别盒子：分步组合即可。 不可辨别对象放入可辨别盒子：分组问题。 可辨别对象放入不可辨别盒子：不作要求。 不可辨别对象放入不可辨别盒子：不作要求。 高级计数原理 ★常系数线性递推方程 齐次: 已知\(a_0, a_1, ...a_k \text{(初值)}, a_n = \sum_{i=1}^k c_ia_{n-i}\)，\(c\)均为常数 解特征方程：\(r^k-c_1r^{k-1}-c_2r^{k-2}-...c_{k-1}r-c_k=0\)，即\(\sum_{i=0}^{k}-c_ir^{k-i}=0,c_0=-1 \ \ (1)\)，则\(a_n= r^n\) 例:\(a_n = a_{n-1}+ a_{n-2}\rightarrow r^2=r+1\) ★如果\(a(n)\)和\(b(n)\)都是某常系数线性齐次解递推关系的解,那么\(a(n)\)和\(b(n)\)的任一线性组合\(t_1 a(n)+t_2b(n)\)也是该递推关系的解。 无重根的情况： 给定初值后，解应当是\(a_n = \sum_{i=1}^m \alpha_i r_i^n\)，\(r_i\)是式(1)的第\(i\)个解，并且没有重根。\(\alpha\)是待定系数，根据初值确定。 二阶方程，有重根\(r_0\)：\(a_n = α_1 r_0^n + α_2 nr_0^n\) 一般情况，有重根: 每个\(r_i\)的系数\(\alpha\)变成：\(\alpha_1+\alpha_2n+...\alpha_{m-1}n^{m-1}\)，根据初值求出各个\(\alpha\)。对应的\(r_i\)是几重根，其系数中就有几个\(\alpha\) 非齐次: 已知\(a_0, a_1, ...a_k \text{(初值)}, a_n = \sum_{i=1}^k c_ia_{n-i}+F(n)\)，\(c\)均为常数 首先解出相伴齐次方程的通解\(a_h(n)\) 再找到非齐次方程一个特解\(a_p(n)\) 最终结果为\(a_n = a_p+a_h\) ★如果\(f(n)\)为\(n\)次多项式,则特解一般也是n次多项式 ★\(f(n) = \beta^n\)，且\(\beta\)是\(e\)重特征根(不是根时e=0)，则特解为\(a_p = pn^e\beta^n\)，\(p\)为待定系数。 分治（不做要求） 时间复杂度为：\(f(n) = af(n/b) + g(n)\)，\(f(.)\)是该类问题需要的复杂度，\(g(n)\)是把子问题合并为原问题需要的复杂度。 生成函数 \(\{a_n\} \Rightarrow \sum_i a_ix^i \Rightarrow f(x)\) \(C(m, n) \Rightarrow (1+x)^m\) 物体配置问题： 不同面值\(\{n_1, n_2...n_k\}\)组合得到一个特定数字\(k\)，求方法数\(p\)。（求解带限制的不定方程） 同一面值不可辨别：解不定方程\(\sum_i m_i = q\)，\(m_i\)为面值\(n_i\)贡献的数值，即\(m_i=\text{选取ni的数量} \times n_i\) 构造生成函数:\(f(x)=\prod_i^{i=k} \sum x^{(m_i\text{可取的值)}}\)，该函数中\(x^q\)的系数即为答案。 若不定方程中的\(m\)没有限制，则转化为之前的组合问题。（\(C_{n+k-1}^n\)） 若同样面值的纸币/砝码之间有区别，则给相应的项加上乘方。可以看出，每一项表示的是每一个*每最小粒度可辨别元素**贡献的面值。 例子：分饼干，砝码称重量，钱面值组合 容斥原理 \(|A\cup B| = |A| + |B|-|A \cap B |\) \(|A \cup B \cup C|= |A|+|B|+|B|-|A\cap B|-|A \cap C|- |B \cap C |+| A\cap B \cap C|\) 一般形式: \(\bigcup_i A_i = \sum_{j=1}^{n} \left( (-1)^{i+1}\sum_{i=1}^{j}\bigcap_i|A_i| \right)\) m个元素\((x)\rightarrow\) n个元素\((y)\)，求满射数量: 映射总数为\(N=n^m\)，先求非满射数量\(p\)。设集合\(P_i\)表示第\(i\)个\(y\)值没有被映射到，则\(p=|\bigcup_i P_i|\)，即可利用容斥原理求出\(p\)，答案即为\(N-p\)。 满射数目为\(\sum_{i=0}^{n-1} (-1)^i(n-i)^mC_n^i\) ★\(n\)元素错位排列\(n!\sum_{i=0}^n (-1)^i {1\over i!}\)： 数论 整除：\(a|b,a|c \Rightarrow a| (mb+nc)\) 取余数(\(mod\)) 模同余\(a \equiv b (mod\ m)\)：\(a-b=km, a\ mod\ m = b\ mod\ m\) 约定\(a+_mb=(a + b) (mod\ m), a\times_mb=(a \times b) (mod\ m)\) 模同余の性质： \[\bigstar a ≡ b (mod\ m), c ≡ d (mod\ m)\Rightarrow a + c ≡ b + d (mod\ m), ac ≡ bd (mod\ m)\] \[a ≡ b (mod\ m)\Rightarrow ac ≡ bc (mod\ m)\text{(不能反推，反推要求c,m互素)}\] \[(a + b) (mod\ m) = ((a\ mod\ m) + (b\ mod\ m)) mod\ m\] \[ab\ mod\ m = ((a\ mod\ m) (b\ mod\ m)) mod\ m\ (2)\] 模指数运算：快速计算\(b^n\ mod\ m\) 将\(n\)分解为二进制串\((n_1n_2n_3n_4...)_{(2)}\)，分别从低阶到高计算\(b^{n_i2^{i}} mod\ m\)，利用式子\((2)\)计算高阶模运算。 算数基本定理：任何一个大于1的正整数都可以唯一地分解为若干个素数的乘积 最大公约数\(gcd(a, b)\),最小公倍数\(lcm(a, b)\) \(lcm(2^3 3^ 5 7^ 2 , 2^ 4 3^ 3 ) = 2 ^{max(3,4)} 3^{ max(5,3)} 7^ {max(2,0)} = 2 ^4 3^ 5 7^ 2\) 最大公约数上式中\(max\)改为\(min\)即可 \(ab = gcd(a, b)\times lcm(a, b)\) ★辗转相除:\(a&gt;b\Rightarrow gcd(a, b) = gcd(a\ mod\ b, b)\) \(\exists s, t\in Z, gcd(a, b)=sa+tb\) ★\(a\)关于\(m\)的模逆\(\bar{a}:a\bar{a}\equiv 1(mod\ m)\) \(m&gt;1, a\ m\)互素，则\(\bar{a}\)存在且唯一。 ★解同余方程\(ax \equiv b (mod\ m)\): 两边同乘模逆得到\(\bar{a}ax \equiv \bar{a}b (mod\ m)\)，简单表示为\(\bar{a}ax\equiv\bar{a}b\ (3)\) 利用传递性求解：\(\bar{a}a\equiv1\)两边同乘x得\(\bar{a}ax\equiv x\)，结合式子(3)得\(x\equiv \bar{a}b\)，即可解出x。 欧拉函数:\(\Phi(n)=[1,n]\)中与\(n\)互素的数字的数目。 ★\(n\)为素数，则\(\Phi(n) =n-1\) ★\(p,q\)为素数，则\(Φ(pq) = pq − (p + q − 1) = (p − 1)(q − 1)\) ★欧拉定理：\(a ^{Φ(n)} ≡ 1 (mod\ n),a\)与\(n\)互素。 ★费马小定理（欧拉定理在\(n\)取一素数\(p\)时的情况）：\(a ^p ≡a(mod\ p),p\)是素数，\(a\)是任意整数。这是\(p\)为素数的必要条件。 ★或者: \(a^{p-1}\equiv1(mod\ p)\) 例：计算\(7^{222} mod\ 11\)。\(222=11\times 10+2,\)利用费马小定理得 \[7^{10}\equiv1(mod\ 11)\Rightarrow (7^{10})^{11}\equiv1^{11}(mod\ 11) \] 再计算\(7^2mod\ 11\)即可推出答案。 RSA:随机选取2个(足够大的)大素数\(p\)和\(q(p&lt;q)\), 记\(n=pq\), \(\Phi(n)=(p-1)(q-1)\). 选择正整数（加密公钥）\(e\),要求\(e\)与 \(\Phi(n)\)互素, 解密私钥\(d\)是\(e\)关于于模\(\Phi\)(n)的模逆，即\(de\equiv1(mod\ \Phi(n))\) ★加密时对每一段明文\(m&lt;n: c =m ^e mod\ n\)，这里要求\(m,n\)必须互素 ★解密时对密文\(c:m = c^d mod\ n\) 其中加密公钥e和n是公开的, 而\(p,q, \Phi(n),d\)(秘钥)是保密的. 格论 运算\(*,@\)的吸收律：\(x*(x@y)=x,x@(x*y)=x\) 一个非空集合S和定义在该集合上的一个或多个运算\(o_1,o_2...o_n\)组成的系统称为代数系统\(&lt;S;o_1, o_2, ...o_n&gt;\) 偏序：反对称(\(x≤y , y≤x \Rightarrow x=y\)），传递，自反。 最小上界\(a\vee b\)，最大下界\(a\wedge b\) 全序：链式偏序，良序：任何子集都有最小元。 格：任意两个元素都有最小上界和最大下界の偏序（注意，上下界必须唯一） 完全格：任意子集都有最小上界和最大下界の偏序 代数格：\(&lt;L; \vee, \wedge&gt;\) 幂等：\(x∧x=x; x∨x=x\) 交换：\(x∧y=y∧x; x∨y=y∨x\) 结合：\((x∧y)∧z = x∧(y∧z); (x∨y)∨z = x∨(y∨z)\) ★吸收：\(x ∧(x∨y ) = x = x∨(x∧y )\) ★分配不等式：\(x∨(y∧z)≤(x∨y)∧(x∨z),(x∧y) ∨(x∧z)≤x∧(y∨z)\)，二者对偶。 ★当证明相等时,一般从偏序的定义出发,或者证明两者相互有≤的关系 对偶命题：\(\leqslant,\geqslant,∧,∨\)分别替换成\(\leqslant,\geqslant,∨,∧\) 求子格：要注意哈塞图删掉了原本存在的一些连线 分配格：满足分配律。钻石格、五角格不是. 我记不住的比较难的部分 辗转相除求模逆，解同余方程，解递推方程（齐次、非齐次），证明格代数不等式，全错位排列]]></content>
  </entry>
  <entry>
    <title><![CDATA[各种乱七八糟的记录]]></title>
    <url>%2F2018%2F12%2F05%2F%E5%90%84%E7%A7%8D%E4%B9%B1%E4%B8%83%E5%85%AB%E7%B3%9F%E7%9A%84%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[Linux相关 Ubuntu换源 路径：/etc/apt/sources.list： 123456789101112131415deb http://mirrors.aliyun.com/ubuntu/ xenial maindeb-src http://mirrors.aliyun.com/ubuntu/ xenial maindeb http://mirrors.aliyun.com/ubuntu/ xenial-updates maindeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates maindeb http://mirrors.aliyun.com/ubuntu/ xenial universedeb-src http://mirrors.aliyun.com/ubuntu/ xenial universedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb http://mirrors.aliyun.com/ubuntu/ xenial-security maindeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security maindeb http://mirrors.aliyun.com/ubuntu/ xenial-security universedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security universe 远程GUI-App在本地显示 不要求远程主机有显示设备。 方法是让远程主机使用本地的Xserver和DISPLAY： - 首先确保远程安装了xauth，然后更改远程配置，启用X11-forward： - /etc/ssh/sshd_config中修改相应部分为： 1234567X11Forwarding yesX11UseLocalhost no # 默认可能没有这一行，最好手动加一下X11DisplayOffset 10PrintMotd noPrintLastLog yesTCPKeepAlive yes /etc/ssh/ssh_config中修改相应部分为： 1234Host * # ForwardAgent no ForwardX11 yes ForwardX11Trusted yes 本地使用ssh -CY &lt;server&gt;登录远程主机，登陆后直接启动GUI程序即可。 添加开机启动脚本 把.sh放在/etc/init.d中，然后执行： 1sudo update-rc.d /etc/init.d/脚本名.sh defaults 为脚本添加LSB tag，在脚本第一行后，代码行前添加格式例如下面代码的一段内容： 123456789### BEGIN INIT INFO# Provides: scriptname# Required-Start: $remote_fs $syslog# Required-Stop: $remote_fs $syslog# Default-Start: 2 3 4 5# Default-Stop: 0 1 6# Short-Description: Start daemon at boot time# Description: Enable service provided by daemon.### END INIT INFO 具体的说明可以看这里：https://wiki.debian.org/LSBInitScripts 如果不知道该写啥，也可以从/etc/init.d里边别的脚本里抄（虽然很不推荐啦 最后记得为脚本添加执行权限！ 列出当前目录文件数量： 12ls -l |grep "^-"| wc -l # 不包含子目录文件ls -lR|grep "^-"|wc -l # 包含子目录文件 jupyter没插件? 12345# 在jupyter的home显示NbExtension选项卡:jupyter nbextensions_configurator enable --user# 安装各种自带插件:jupyter contrib nbextension install --user]]></content>
      <tags>
        <tag>编程</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[考完就不看の计网笔记！]]></title>
    <url>%2F2018%2F11%2F30%2F%E8%80%83%E5%AE%8C%E5%B0%B1%E4%B8%8D%E7%9C%8B%E3%81%AE%E8%AE%A1%E7%BD%91%E7%AC%94%E8%AE%B0%EF%BC%81%2F</url>
    <content type="text"><![CDATA[令人混乱的专有名词大集合 cp.1 Introduction DSL:数字用户线，一种接入网的方式，特点是带宽独享 HFC:光纤同轴电缆混合网络，特点是带宽共享 电路交换网络： 第一代网络，交换数据前需要建立起一条从发端到收端的物理通路 在数据交换的全部时间内用户始终占用端到端的固定传输信道 交换双方可实时进行数据交换而不会存在任何延迟 带宽利用率低下 分组交换网络： 第二代网络，以分组的方式收发报文，每个分组有首部。 传输数据时不必预先确定分组的传输路径 存储转发能力：整个分组都到达路由器才会开始转发 数据报网络：分组交换网络的一种，分组的目的地址决定下一跳。 虚电路网络：分组交换网络的一种，分组携带一个虚电路标识，该标识决定下一跳。 对等体(peer)：不同机器上的对应层的实体 服务访问点：同一系统中相邻两层的实体进行交互的地方 cp.2 Application Layer SSL: 安全套接字层。位于应用层和TCP之间的协议，提供加密的TCP连接。 SMTP发送邮件过程： \[a本地\xrightarrow{邮件内容} a的server \xrightarrow[TCP连接]{邮件内容}b的server\] b的server收到邮件后存储在服务器上，等待b查看时，再发往b本地。本地获得服务器上的邮件，可以使用pop3, imap, http等协议。 DNS还负责负载均衡的功能，当一个域名对应多个ip时，在这些ip中轮转。 DNS记录的格式：\((name, value, type, ttl)\)，根据type的不同，将name解析为value \(type=A\):解析主机名\(\rightarrow\)ip地址 \(type=cname\):解析别名\(\rightarrow\)真名 \(type=ns\):解析域名\(\rightarrow\)该域权威域名服务器的主机名 \(type=mx\):解析name\(\rightarrow\)与 name相关的邮件服务器域名 \(p2p^*\)：还需要再看看 cp3.Transport Layer 多路分解：接受方将报文段交付给对应的网络进程的socket。分解的含义是：一个发来的报文段\(\rightarrow\)多个socket 多路复用：发送方从多个网络进程的socket收集数据块，并装配成报文段。 多路复用要求socket有唯一标示，且带有源端口号和目的端口号 周知端口号：\([0,1023]\)范围的端口号 UDP的socket由目的ip和目的端口唯一标识。 TCP的socket由源ip、源端口、目的ip、目的端口四个标志唯一标识。也就是说，来自不同发送发的TCP报文，即使目的地一样，也会多路分解给不同的socket。（server为每个客户端连接都建立新的socket） UDP提供的全部功能：多路复用/分解，差错检查（求校验和：以16bit为单位，将各个16bit相加，如果有进位，将进位加到结果末尾（回卷），然后对最后结果求反码得到校验和。） ARQ：自动重传请求协议 rdt：可靠数据传输协议 1.0: 假设信道可靠，简单的发送和接收 2.0: 考虑bit差错，引入检验手段（例如校验和）。接收方接收数据后，发现检验成功/失败，则发出ack/nak；发送方发送数据后，等待ack/nak，并继续传下一个数据/重传受损数据 2.1:考虑连ack/nak也可能受损。则： 发送方：以\(0,1,0,1...\)交替的方式为发送的分组编号。发送0号分组后，等待ack/nak。如果收到ack，则继续发1号分组；若收到nak/受损的回应（就发送方也不知道收到的是ack/nak了），则重发0号分组。 接收方：一开始等待接受0号分组，成功接受则交付数据并发送ack，并等待1号分组；检验失败则发nak，等待重传0号；若收到了1号分组，则说明这个收到的1号分组是一个重传（即自己之前发出去的ack受损了），则再发一个ack（emmm就是 这个接收方其实是作为ack/nak的发送方。这种情况不是ack受损了吗，所以接收方此时作为发送方，就重发ack）。 2.2：在2.1基础上，ack也有了0,1的序号之分。接收方不发nak，当等待0号分组时，收到数据的检验出错时，发一个1号ack。这个时候发送方本应该等待0号ack，但是却收到了1号ack（冗余ack），发送方就知道刚刚发出去的0号分组出问题了，需要重传。 3.0：考虑数据包和ack都可能丢失。则发送方在2.2基础上，发包后启动一个计时器，在等待ack时，如果等待时间超过阈值，则直接重传。（如果是丢失ack，接收方不需要管，发送方收不到ack时间超过阈值，直接重发就行了） 继续改进：不用发一个包，等到ack再发下一个，可以一口气都发完，然后等那个出问题重传哪个。这就要求要使用序号来唯一标识每一个分组。 窗口：发送方用来标识已发送但未确认的分组、可用但未发送的分组的一段范围。例如，下图中，发送方可以使用的分组是0到8，其中\(0, 1, 2\)号分组已经收到了对应的ack，那么窗口范围包括剩余的3~6分组。 \[ \underbrace{0, 1, 2,}_{got\; ack} \overset{window}{\boxed{3, 4, 5, 6}}, \underbrace{7, 8, 9, 10...}_{not\; available} \] GBN：一种重传协议，一个分组出问题，重传它以及它后面的所有分组。实现： 接收方如果收到有问题的分组（例如第4个分组丢失了，那么接收方收到分组3后，直接收到了分组5），则从此刻开始，丢弃后续的分组，并不断发送冗余ack（最后一个成功接受的ack，也就是ack3）。发送方收到这些ack3后，什么都不做，直到发送方发现ack3接受超时，再重新发3,4,5…分组。 SR：选择重传协议。与GBN的区别在于：发送方在某个分组ack超时后，只发该分组本身；接收方接收到失序分组后不丢弃。而是将其缓存，知道所有分组都被收到。此时，接收方也有一个滑动窗口，窗口内是期望接受（但还没接受）的分组序号。（或者可以理解为，将要发送但还没有发送的ack序号，这两者是等价的） 此外，接收方如果收到了序号在窗口左边（即之前已经发出的ack对应的序号）的分组，说明之前发出去的ack损坏/丢失了，此时虽然已经收到过一次，接收方仍要再发一次该分组对应的ack。 MSS: 最大报文长度。(不包括首部) TCP首部： 序号：该报文段的第一个字节处在整个数据流中的第几号。注意，是对字节编号，而不是分组。例如假设bit流中报文的长度都是1000，那么各个报文的序号分别为0, 1000, 2000… 等等 不对劲啊 这里给字节编序号到底考不考虑首部啊 确认号：每个机器在发送数据的同时，也要接受数据。确认号是我希望从对方那里获得的下一个字节的序号。 估计往返时间：取各个抽样时间的加权平均，其中常用\(\alpha=0.125\)： \[\text{EstimatedRTT} = (1- \alpha)\text{EstimatedRTT} + \alpha\text{SampleRTT}\] 估计RTT误差:使用同样的方法。这里常用\(\beta=0.25\)： \[\text{DevRTT} = (1- \beta)\text{DevRTT} + \beta|\text{SampleRTT} - \text{EstimatedRTT}|\] 根据上述两个指标可以得到TCP的超时间隔为：\(\text{TimeoutInterval} = \text{EstimatedRTT} + 4\times \text{DevRTT}\;(1)\)。并且每当出现超时时，该超时间隔都会加倍 为什么要加倍？是为了防止即将被确认的后继报文过早超时。也就是说，假如这个超时报文其实再等一下下就可以收到了，加倍超时时间就可以防止之后的报文纷纷超时。其实也相当于一种对当前\(RTT\)的估计叭） 这里再注意嗷，出现超时后间隔加倍并不影响在没有超时的情况下，式\((1)\)的计算。也就是说，虽然超时后间隔临时加倍了，但是之后再次采样\(\text{SampleRTT}\)时，超时间隔仍然按照式\((1)\)来算。 快速重传：TCP对于乱序分组的处理：如果接受到乱序分组，则发送冗余ack（参考GBN）。由于收到乱序分组时，有可能意味着出现丢包了。所以，发送方连续收到3个冗余ack后，就重新发送该包。 之所以是三个，是因为冗余ack达到3个及以上的时候，是丢包的可能性比较大（而不是单纯的乱序）。还有一点，是3个冗余ack，也就是连上最初一个正确的ack，总计有4个同一序号的ack。 流量控制：防止发报太快，接收方的buffer溢出。方法是接收方把当前剩余buffer大小填到窗口字段（假设值为rwind），通知发送方，限制发送方的窗口大小不能超过这个值。 由于这个窗口信息是附加在首部里的，只能随着数据捎带发送。那么当接收方buffer已满，即rwind=0，将这个信息发回发送方后，发送方暂停发送数据。那么，在没有数据交换的情况下，即使之后buffer有了空闲，发送方也无从得知。 解决办法：规定发送方一旦受到rwind=0的信息，就在后续持续发送只含一个字节的试探报文，目的就是为了在接收方有空闲时，可以及时知晓。 三次握手：教材\(p169\)。SYN比特只有在三次握手的前两次中为1，其他情况都是0 关闭连接：如果我想关闭连接，则我发送FIN=1的报文，表示“我没东西要发了”，然后对方给出ack；（此时连接处于半关闭。我不发东西，但我仍可以接收）之后对方也向我发送Fin报文，我给出ack。连接正式关闭。 Reno算法：TCP使用的拥塞控制算法。设置一个拥塞窗口cwind，限制当前可发送的数据量。以及一个阈值ssthresh，限制cwind的大小。实际可发送分组范围应取接收端窗口rwind和拥塞窗口的交集。那么，发送速率为： \[rate = \frac{cwind}{RTT}bytes/s\] - 拥塞控制的具体方法： - 慢启动阶段：连接初期，发射速率很小，cwind=MSS, ssthresh=64kb。 - 无丢包时，则每收到一个ack，将cwind+=MSS。（等价于每过一个RTT,cwind翻倍）由于这一点，发射速率在慢启动阶段以指数速度增加 这里注意，发射速率本身是很慢的，所以叫慢启动。只不过增长速度快。 拥塞避免阶段：任意时刻出现cwind == ssthresh时，cwind改为线性增长。（每过一个RTT，cwind+=MSS） 若出现三个冗余ack：快速重传，然后ssthresh = cwind/2, cwind = ssthresh + 3MSS，并且此后cwind线性增加。 若出现超时，ssthresh = cwind/2, cwind = MSS，重新进入慢启动阶段。 总结：思想是“加性增，乘性减”。即增加cwind大小是线性（加性）的，减小大小是翻倍的（乘性）。 Tahoe算法：与reno的区别是，收到三个冗余ack也将cwind=1 MSS（而不是减半） TCP的吞吐量（忽略慢启动）：平均为\(0.75 \frac{cwind}{RTT}\)；若已知丢包率\(L\)，则吞吐量为：\(\frac{1.22MSS}{RTT\sqrt{L}}\) cp4.NetworkLayer 链路层交换机和路由器的区别：分别根据链路层首部信息和网络层首部信息进行转发。 线头阻塞：路由器输入端口的处理（查表）速度超过路由器交换结构的速度，导致产生排队。 三种交换方式：经内存、经主机、经内联网络交换。前两种分别受限于内存带宽、总线带宽。 路由器的输入和输出端口都分别由三部分组成，输出输出端口分别如下： 排队部分需要设置缓冲区，如果输出缓冲区溢出，则会导致输入端口排队，甚至分组丢失。所以输出端口缓冲区大小要合理，一般为\(B = RTT \times R\);对于tcp连接，则为：\(B = \frac{RTT\times R}{\sqrt{N}}\) RED：随机早期检测。一种队列管理算法（决定有排队时，先转发队列中哪个分组）。对各个输出端口上的队列计算加权平均值avg，并设置两个阈值min, max： 若当前avg&lt;min，则允许接受分组； 若avg&gt;max则丢弃或标记分组 其他情况下，按照一定概率丢弃或标记分组 MTU：最大传输单位。类比运输层的MSS。传输单位称为数据片。 IP数据报首部一般长度为20字节。整个数据报最大长度65535 路由器的每一个接口都有一个不同网络号的ip地址 子网：通过从主机号中借用一些bit位，来标记子网。 子网掩码：网络号和子网号的bit位全1，主机号全0 作用：路由器是根据目的网络地址来决定转发表的（而不是具体的目的ip）。先检查目的ip的网络号+子网号（子网掩码和目的ip做与操作），如果是路由器所在网络，则交付给对应的子网；若不是，则转发给下一个路由器。 CIDR：一种地址表示方式，写法是ip地址/子网掩码长度 最长前缀匹配：使用CIDR时，路由器转发表存网络前缀，根据网络地址确定下一跳的地址，在匹配目的ip时，选择匹配到的网络前缀中最长的。 地址块：指具有同一前缀的一批ip地址 WAN：广域网 LAN：局域网 DHCP：动态主机配置协议:从服务器上动态获取IP地址。一个新机器加入子网后，进行如下四步： 新机器广播dhcp发现报文：源ip为0.0.0.0，目的ip为255.255.255.255:67 任意一个dhcp服务器收到后，广播dhcp提供报文，包含新机器可以使用的ip地址 新机器收到后，选定一个dhcp服务器，发送dhcp请求报文 dhcp服务器返回DHCP ack NAT：网络地址转换。NAT路由器从dhcp服务器处获得一个单一的ip地址ip1，并负责将子网中设备的端口和这个ip地址的端口进行映射（反向代理？）在外界看来，整个子网都只有这一个ip地址。 UPnP：通用即插即用。允许在NAT后的机器参与TCP、UDP连接。 ICMP：因特网控制报文协议。网络层协议，位于ip协议上层，常用来差错控制。 IPv6：32bit$$128bit。改变：无检验和，中间节点不再进行分片，首部长度固定，有跳数限制（超过该上限则直接丢弃） 选路算法分类：全局/分散，静态/动态，负载敏感/负载迟钝 Dijkstra’s算法：没错，又是它。这个算法是全局的。 LS算法：链路状态算法。是全局选路算法。 OSPF协议：实现各个路由器之间链路状态同步的协议。使用洪泛法广播链路状态。 DV算法：距离向量算法。是分散选路算法。原理： \[d(x, y) = \min_v[c(x, v) + d(v, y)], v \in neighbor(x)\] 其中\(c(x, v)\)表示\(x\)和他的邻居\(v\)之间的费用。对于一个节点\(x\)，距离向量\(\mathbf{D}_x=[d(x, v_1), d(x, v_2)...d(x, v_n)]\)，即为\(x\)到其他所有节点的费用的估计值。所有节点在自己的距离向量变化时，向自己的邻居发送自己新的距离向量；当节点收到邻居的距离向量时，使用上述公式更新自己的距离向量。 毒性逆转：为了解决坏路问题\((p252)\)。解决方法：如果路由\(x\rightarrow y\)的路径上经由邻居\(z\)，则\(x\)向\(z\)发送距离向量时，令\(d(x,y)=\infty\)。 为什么要这么做？这里的\(d(x,y)=\infty\)含义是，\(x\rightarrow y\)要经过\(z\)，也就是说\(x\)的距离向量中，\(d(x, y)\)这个值就是从\(z\)获得来的。\(x\)告诉\(z\)，我离\(y\)的距离是\(\infty\)，就可以避免\(z\)反过来从\(x\)到达\(y\)。 AS:自治系统 IGP：内部网关协议。负责AS内部的选路的一类协议。例如RIP 和 OSPF 协议。 EGP：外部网关协议，负责AS之间的选路。例如BGP协议。 网关：AS边缘的路由器，具有连接到其他AS的链路。 RIP协议：互联网中实现距离向量选路算法的协议。使用路由表（包含目的地、下一跳路由器、到目的地的跳数）来表示距离向量。每隔30s向邻居发送RIP通告来交换距离向量。 OSPF 协议：实现Dijkstra算法的协议。每个路由器都拥有当前AS的拓扑图，并在状态改变时向AS内全部路由器广播。 BGP协议：边界网关协议。每个AS指定一个路由器作为BGP发言人，发言人之间互相交换各自的AS可以抵达哪些前缀的地址。交换的内容包括： 前缀 AS-PATH：这个通告已经经过的AS NEXT-HOP：到下一跳AS的具体的路由器 BGP路由选择：当一个AS知道到一条前缀的多条路由，路由器必须在可能的路由中选择一条： 本地偏好值: 策略决定。具有最高本地偏好值的路由将被选择。 最短AS-PATH ：在余下的路由中，具有最短AS-PATH的路由将被选择。 热土豆路由：从余下的路由中，选择具有最靠近NEXT-HOP路由器的路由。 cp5.LinkLayer 链路层数据单位：帧。 TDMA、FDMA、CDMA：信道划分协议。分别根据时间、频率、编码方式来划分信道。 碰撞：多个节点在同一时刻在共享信道上发送帧，导致互相干扰。 随机访问协议：解决碰撞检测和回复问题。 ALOHA：如果传输的帧发生碰撞，该节点在全部传输完碰撞帧后以概率p重传该帧，或以概率1-p等待一帧的传输时间。等待时间结束后以概率p重传该帧，或以概率1-p继续等待。 时隙ALOHA：分成一个个时隙，只在时隙开始传帧。 CSMA：载波监听协议。不断监听信道，寻找合适的发帧时机。如果信道空闲则直接发，如果正忙，则： 非坚持型：不再监听，休眠随机时间后再重新监听 “1”坚持：继续监听，一有空闲马上发 “P”坚持：继续监听，一有空闲，以概率p发帧；以概率（1－P）延迟时间τ(网络中最远的端到端的传播时延)，然后重新监听。 CSMA/CD：载波侦听/碰撞检测。边传输边检测碰撞，一旦发现当前帧碰撞，就停止传输。在第\(k\)次冲突后，睡眠时间\(r\times 2\mu\)，其中，\(r\)从\([0, 1, 2, ... 2^{k}-1]\)里等概率地选择。 CSMA/CD效率为（无碰撞传输时间所占的比例）： \[\frac{1}{1+5d_{prop}/d_{trans}}\] 强化碰撞：碰撞后再发送48bit的人为干扰信号(jamming signal)，以便让所有用户都知道现在已经发生了碰撞。 轮流协议：大家轮流用信道。 轮询协议：主节点邀请从节点轮流传输 令牌传递协议：控制令牌依次通过各个结点，谁拿令牌谁使用 MAC地址：6段48位。标识网络中的适配器。广播地址为ff-ff-ff-ff-ff-ff。 CRC校验：给定生成多项式\(G\)，要传输的二进制序列\(D\)，求\(R\)： \(D\)后补\(r\)个\(0\)得到\(D&#39;\)，\(r\)是\(G\)中\(x\)的最高次的指数 \(D&#39;\div G\)得到的余数\(R\)即为校验码。注意，这里做的是模二除法。 ARP:mac地址解析协议，根据目标的IP地址获取其MAC地址。通过主机内的ARP高速缓存实现，存储着局域网节点的IP/MAC地址映射：\(\left&lt; IP; MAC;TTL\right&gt;\)，其中\(TTL\)是存储生命周期。 ARP过程：a想知道b的mac地址，则主机a发送ARP广播报文（目的ip是b的ip；目的mac是广播地址ff-ff-ff-ff-ff-ff），b回复arp响应报文，告知b的mac。 跨子网时：通过路由器中继。 交换式以太网：星型结构。争用期为\(51.2\mu s\)，最短有效字长为64字节。 前同步码：以太网帧首部中的前8字节，用来“唤醒”对方的接受适配器，同步双方时钟。 以太网不可靠、无连接：没有握手，没有ack 随机回退：以太网传输中出现碰撞时，在重新传输之前，适配器要等待一段随机时间。 10BaseT，100BaseT：10、100代表速率，T 代表双绞线（Twisted Pair） 以太网限制节点和交换机之间的最大距离：因为使用CSMA/CD广播，为了保证碰撞检测效率。 集线器（hub）：物理层的中继器。 交换机：链路层的存储-转发设备。不转发同一网段的帧。一个接口对应一个网段。可以自学习转发表：收到帧时，自动将MAC地址：接口映射加入转发表。转发帧时，如果表中没有要找的目的MAC地址，则转发向所有输出端口。 二者区别： 集线器只是将网络的覆盖距离简单的延长，而且距离有限，具体实现在物理层；交换机不仅具有将LAN的覆盖距离延长的作用，而且理论上可做到无限延长，具体实现在MAC层。 集线器仅具有简单的信号整形和放大的功能；交换机则属于一种智能互连设备，它主要提供信号的存储/转发、数据过滤、路由选择等能力。 集线器仅是一种硬设备，而交换机既包括硬件又包括软件。 点对点传输：交换机的每个端口都只连接一个MAC地址。此时不会发生碰撞，可以实现全双工。 虚拟局域网：把一个物理交换机的端口分组，每个分组构成一个虚拟局域网。这些端口连接的主机只能在本VLAN内通信。 跨物理局域网通信：如果两个主机属于一个vlan，但是不在同一个物理局域网内怎么办？将交换机上一个特殊端口设置为所有vlan共享（称为干线端口），并将其和目的局域网对应的交换机的干线端口相连。vlan1发来的数据通过干线端口转发到同一vlan的另一主机。 为此，使用vlan时，需要在以太网帧上再加一个vlan-id cp7.Wireless and Mobile Networks ad-hoc：无基础设施的无线网络，终端之间自组织。 无线网络特点：比特差错更加频繁，信号强度衰减。 CDMA：之前提过的，多用户共享信道，但是各自持有码片\(m\)。在传输数据时，用\(m\)来代表1，\(m\)的反码代表0。用\(+1, -1\)表示码片中的\(1, 0\)，可以得到一个向量\(S\)。要求各个用户的向量\(S\)之间相互正交。 判断数据哪些哪个用户：将收到的向量与该用户的\(S\)求内积，得到正数说明该用户发送了\(1\)，得到\(-1\)说明该用户发送了\(0\)，得到0说明该用户没发送。 BSS：802.11的基本服务集，指的是一个基站和若干个移动站。（类比子网）所有的站在本 BSS 以内都可以直接通信 BBA：一个BSS覆盖的地理范围。 802.11协议簇： 802.11b：最高速率11 Mbps 802.11a：最高速率54 Mbps 802.11g：最高速率54 Mbps AP：BSS中的基站 ESS：扩展的服务集。基本服务集通过接入点 AP连接到一个主干分配系统 DS (Distribution System)，然后再接入到另一个基本服务集构成 DIFS：Distributed Inter-Frame Space，分布式帧间间隔 SIFS：Short Inter-Frame Space，短帧帧间间隔 CSMA/CA：802.11使用的碰撞避免算法（不是检测，是直接避免）：用户向ap发送一个很短的预约帧，用来预约信道，AP确认后，向BSS广播，赋予该用户信道使用权 802.11帧格式：包含四个地址，源、目的、与AP相连的路由器接口的MAC、仅在ad hoc模式使用的地址。 令人混乱的数学指标大合集 带宽：单位\(bps\)（比特每秒），表示传输速率。\(1mbps=10^3kbps=10^6bps\) 时延：数据包到达路由后一次进行： 节点处理（差错、计算输出链路） 排队（等待传输） 传输（数据包从路由器发送到传播媒介上，类比包裹装车） 传播（路上的时间）。 四个过程花费的时间就对应四种时延：\(d_{total}=d_{proc}+d_{queue}+q_{trans}+q_{prop}\)。 流量强度：针对一个节点（路由器）上时延的评估指标。假设输出链路带宽为R，分组长度为L，平均分组到达速率为a，则流量强度为\(L\times \frac{a}R\) 理解：a表征数据进入的速度，R表征数据离开的速度 流量强度趋近0时，即\(a\ll R\)，排队时间几乎为0 流量强度小于1时，即\(a&lt; R\)，排队时间会不断变小 流量强度趋近1时，即\(a= R\)，排队时间几乎不变 流量强度大于1时，即\(a&gt; R\)，排队时间会不断变大 吞吐量：\(\frac{发送方发送的比特速率}{接收方接受的比特速率}\) RTT：从发送包发第一个bit开始，到发送方收到ack为止的时间。即一个来回的时间。 在TCP的超时间隔估计中，采样到新的数据后，估计顺序是：\(\text{EstimatedRTT} \rightarrow \text{DevRTT} \rightarrow \text{TimeoutInterval}\)]]></content>
  </entry>
  <entry>
    <title><![CDATA[你需要知道的有关AI的一切]]></title>
    <url>%2F2018%2F11%2F21%2F%E4%BD%A0%E9%9C%80%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84%E6%9C%89%E5%85%B3AI%E7%9A%84%E4%B8%80%E5%88%87%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习中的Normalization]]></title>
    <url>%2F2018%2F10%2F08%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84Normalization%2F</url>
    <content type="text"><![CDATA[什么是归一化？ 标准的定义是这样的： Normalization is performed on data to remove amplitude variation and only focus on the underlying distribution shape. 也就是说…很多时候我们并不在意给定数据的具体数值，而更在意它们的分布情况（或者说“形状”），例如服从怎样的概率分布，最大值和最小值分别在哪里取到…诸如此类。而现实中的数据往往杂乱无章，数值有大有小，因此我们在处理数据前先对它们进行归一化，压缩到某个区间之内（一般是\([0, 1]\)），这样更容易从数据中学习到一些一般规律。 同时，由于数据的归一化具有权重/数据伸缩不变性，即原始权重\(W\)和伸缩后的权重\(\lambda W\)，在进行归一化之后得到的值相等，同理，数据经过伸缩之后的归一化值也不发生改变。这样使得权重的伸缩变化对反向传播的梯度值没有影响，避免了由于权重过大而造成的梯度爆炸。 深度学习中的归一化？ 而在深度学习中，也有着花样繁多千奇百怪的归一化，这些归一化的目的大致都是相同的：让深度神经网络更容易训练。 数据归一化 Batch-Normalization 实际的机器学习方法多是基于iid假设（独立同分布假设），但是实际情况中常常会有Covariate Shift问题，即数据采样自不同的分布时（即输入的分布差异很大时），由于网络权重的梯度受输入的影响很大，导致难以优化。 BN是一种广泛用于CNN和RNN的一种层间归一化方法，它使得在神经网络内部特定某一层的输入的一个batch的所有维度都符合归一化之后数据分布。为了防止破坏网络已经学习到的数据分布，BN层在初始化时先使用两个因子\(\gamma,\beta\)来抵消归一化的作用，之后再由网络学习\(\gamma,\beta\)来逐渐进行调整。 在进行推断时，均值和方差使用全部数据的均值和方差。在CNN中，将某一层中的一个特征图作为一个维度。 关于BN的细节可以看这里w 缺点： - 使用每个batch的均值方差，相当于引入了噪声（随机性），因此不能适用于对噪声敏感的学习任务。 - 对mini-batch依赖，不能用于RNN - 在batchsize较小时表现较差。 Mean-only Batch Normalization 只减去均值，不除以方差，减小了计算量,也减小了训练时的噪声。 Recurrent Batch Normalization 在RNN中使用的BN。在每个时间步重新收集一次均值和方差信息 Layer-Normalization https://arxiv.org/pdf/1607.06450.pdf 由于BN依赖于每个batch进行归一化，而每个batch中的均值和方差都不相同，在RNN中使用时，由于每个时间步的batch数据都不同，导致必须存储每个时间步的信息，使得开销增大。因此RNN最好使用不依赖于batch的归一化方法。 LN基于每一个batch中每个样本的各个维度进行归一化，由于不依赖batch，在RNN上表现较好。但由于同一个样本的不同维度有可能不具备相似特征，因此会略微降低模型的表达能力. Instance Normalization https://arxiv.org/pdf/1607.08022.pdf 适应于图像风格迁移中。由于风格迁移的风依赖于一个特定图像，不能使用依赖batch的BN，因此IN在一个图像实例上进行归一化, 其他与BN相同。 Group Normalization https://arxiv.org/pdf/1803.08494.pdf 在LN的基础上，在同一个训练样本的不同维度中，分成\(G\)（超参数，默认为32）个组，对组内数据进行归一化。比起LN和IN更加灵活，因为当\(G=Channel\)时就是\(LN\)， \(G=1\)时就是IN。 GN divides the channels into groups and computes within each group the mean and variance for normalization. GN’s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. 进过实验，在许多主流的数据集上都取得了很好效果，小批量时表现很好，但是正常批量时表现不如BN。BN还是强啊。 Instance Normalization &lt;-()- Group Normalization -&gt; Layer-Normalization Switchable Normalization https://arxiv.org/pdf/1806.10779.pdf 香港中文大学，2018 深度网络中常常多达上百个归一化层，按照习惯都使用BN并不能使得效率最大化。SN可以自适应地学习使用哪种归一化,可能是 BN，IN，LN，GN 或者它们的组合。 这种归一化方式对每一个像素点（所有数据）都执行归一化操作。首先分别计算 BN，IN，LN对应需要的均值和方差，并将各个均值&amp;方差加权得到最终的均值&amp;方差。权重为： \[w=\frac{e^{\lambda_k}}{\sum_{z \in \{in, ln, bn\}}e^{\lambda_z}}, k\in bn, in, ln\] 该权重使用\(softmax\)进行归一化，其中各个\(\lambda\)均初始化为\(1\)，在反向传播时学习。要注意的是，计算均值和方差对应的权重时使用的不是同一组\(\lambda\)。 这种加权计算使得SN具有对噪声的鲁棒性、以及很强的泛用性 非数据归一化 Weight Normalization https://arxiv.org/pdf/1602.07868.pdf 不对输入进行标准化，而是对权重标准化。\(W=g\frac{v}{||v||}\)将权重分解为一个标量\(g\)和方向向量\(\frac{v}{||v||}\)的乘积。看似不是归一化，但是只要令\(\sigma=||v||, \mu=0\)，恰好就是z-score归一化的形式。其中要学习的参数是\(g\)（一般初始化为\(||W||\)）和\(v\)。 本质是使用权重的欧式范数进行归一化，对权重的范数和方向进行了分解。解决了BN的batch依赖的缺点，可以用于RNN，在用于CNN时， 由于参数数量少，开销也变得更小。引入更少的噪声，增强了对噪声的鲁棒性， 在生成模型和强化学习模型中的表现效果较好。 可以认为是一个低成本的BN。 train： BN&gt;WN&gt;WN+mean-only BN&gt;normal param&gt;mean-only BN test： WN+mean-only BN&gt;BN&gt;normal param&gt;WN&gt;mean-only BN 缺点： - 只适应于SGD，对Adam和动量法的适应性不好。 - 不具备规范化每层输出数据到特定尺度的能力，因此在进行参数初始化的时候要慎重。建议从平均值\(0\)和标准差为\(0.05\)的高斯分布中采样。 - 表现力不如BN，当网络参数的均值是\(0\)时，两者效果相近，WN性能较差。 Cosine Normalization 对神经元的计算\(f(W \cdot x)\)中的乘法进行改进（由于乘法运算无上下界，导致运算结果很大）。改为计算\(W, x\)之间的余弦值，即\(f(\frac{W \cdot x}{||W||\cdot||X||})\)。与WN形式上相似。 当然，原始的内积运算，既包含了夹角信息也包含了模的信息，现在去掉了模信息，会使效果稍微变差。 Self-Normalization 一种适用于深度MLP的归一化方法。在MLP的训练中，由于权重\(W\)的分布震荡十分广泛，使得\(|W| \to \infty\)，在使用\(Sigmoid\)函数时，常常会导致梯度消失。以往，我们使用\(ReLU\)解决这个问题，但是事实上，\(ReLU\)的效果在MLP上的表现并不好。 为了改善FNN的效果，提出了新的损失函数selu，可以使得FNN具有高级抽象特征的提取能力。论文\(100\)页….无比惊恐的眼神.jpg \[2018.10.12.\ \ by \ why\]]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[whyのpython进阶教程！]]></title>
    <url>%2F2018%2F09%2F17%2Fwhy%E7%9A%84python%E8%BF%9B%E9%98%B6%E6%95%99%E7%A8%8B%EF%BC%81%2F</url>
    <content type="text"><![CDATA[算法&amp;数据结构 列表整体赋值 将列表中的元素分别赋值给多个变量：a, b, c = list，变量个数必须等于列表长度。 将部分列表赋值：a, b, *x = list，c会得到列表除去前两个元素剩下的部分。 保存循环中最近N轮的记录 使用q = collextions.deque(maxlen = n)，创建一个长度固定为n的列表，在使用.append()添加新元素时，旧元素会被挤出队列。 可以使用q.pop()手动弹出最右侧的元素，使用p.popleft()弹出最左侧的元素。 不指定大小时，q = collextions.deque()会创建一个可以无限增加新元素的队列。 查找最大或最小的N个元素 heapq模块有两个函数: nlargest() 和 nsmallest() 可以完美解决这个问题: headq.nlargest(n, list, key)返回list中最大的n个元素，nsmallest同理。 key参数应当是一个函数，该函数接受list，并返回要排序的部分，通过灵活使用key可以对各种复杂数据结构进行排序。 内置的sorted(list, key)方法也可用来排序复杂结构，参数key含义同上。 命名切片 可以提高代码可读性。list[m:n:p]可以写为： 12a = slice(m, n, p)list[a] 查找序列中出现次数最多的元素 使用c = collections.Counter(list)将list包装为一个Counter，Counter包含了list中每个元素出现的次数。 调用c.most_common(n)可以返回list中出现次数最多的n个元素及其出现次数。 不止list，可以适用于任何可迭代对象。 字典推导 可以快速创建字典，类似列表生成式：{a:b for a, b in .... if ....} 常用来快速创建一个字典的子集 将字符串转化为字典 待转化的字符串必须是字典格式 使用ast.literal_eval(str)进行转化 使用pytest进行接口测试 在当前目录下直接运行pytest，pytest按照以下方法进行测试： 查找文件名前缀为test_或后缀为_test的python文件 若想只测试一个文件，也可以手动指定：pytest test_xxx.py。 pytest进入其中，查找前缀为test的函数/方法，或者前缀为Test，且没有初始化方法的类，加入测试队列 如果该函数有参数，例如test_func(a)，该怎么指定测试哪些a呢？方法是单独写一个由@pytest.fixture修饰的函数a，该函数的返回值即为待测试的值。 如果想测试同一个参数的多组值，则给装饰器@pytest.fixture传参。具体写法是： 123456import pytest @pytest.fixture(params = [1, 2, 3])def a(request): # 这里的request.param，即为params中每次测试的值 return request.param 接下来，pytest就会对该函数进行3次测试，每次测试传入的参数分别为1, 2, 3 whyの神奇函数 123456789101112131415161718192021222324252627282930313233341. 彩色日志import term # 需要安装py-termdef info(target): term.writeLine(f'✔ &#123;time.strftime("%b%d-%H:%M", time.localtime())&#125; ➜ &#123;target&#125;', term.green)def warn(target): term.writeLine(f'❗ &#123;time.strftime("%b%d-%H:%M", time.localtime()) &#125; ➜ &#123;target&#125;', term.yellow)def error(target): term.writeLine(f'✘ &#123;time.strftime("%b%d-%H:%M", time.localtime()) &#125; ➜ &#123;target&#125;', term.red, term.bold)def debug(target): term.writeLine(f'🔨 &#123;time.strftime("%b%d-%H:%M", time.localtime()) &#125; ➜ &#123;target&#125;', term.blue) # 锤子图案不太好康也可以用✱2. 使用装饰器调试函数import os terminal_width = os.get_terminal_size()[0]def log(func): def new_func(*args, **kargs): call_info = f' call &lt;&#123;func.__name__&#125;&gt; ' split1 = '-'*((terminal_width - len(call_info))//2) print(f'&#123;split1&#125;&#123;call_info&#125;&#123;split1&#125;') print(f'* Args: &#123;args&#125;\n* Key-Args: &#123;kargs&#125;\n* Output:') res = func(*args, **kargs) ret_info = f' return &lt;&#123;res&#125;&gt; ' split2 = '-'*((terminal_width - len(ret_info))//2) print(f'&#123;split2&#125;&#123;ret_info&#125;&#123;split2&#125;\n') return res return new_func]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[whyのUnixShell学习记录！]]></title>
    <url>%2F2018%2F07%2F21%2Fwhy%E3%81%AEUnix-Shell%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%81%2F</url>
    <content type="text"><![CDATA[shell脚本 需要注意的事情 脚本必须以#!/bin/bash开头，表示指定运行脚本的解释器。 如果脚本不能运行，记得赋予运行权限chmod +x xxx.sh 也将脚本所在文件加入环境变量，以便在各处运行。 基本函数&amp;控制语句 可以直接写命令作为一条语句。 echo &quot;str1&quot; &quot;str2&quot;...：打印些东西 io重定向： command &gt; file把输出重定向到文件，覆盖写文件。 command &gt;&gt; file输出重定向到文件，追加写文件。 command &gt; file从文件获取输入。 变量：声明：a=1，表达式内部 不能有空格。强迫症表示不开心（哼 使用readonly a=1来声明一个只读变量。 使用unset a来删除一个变量 用${}引用变量：&quot;I am ${a}&quot;，花括号可省略。 变量声明之后，所有要使用变量的地方都必须用${}的形式。 把命令赋值给变量时，使用反引号：a=`sudo -s`。在命令中 数组：array=(1 2 3 4)，用空格分隔，用array[0]索引。 引用数组的所有元素：$\({array[@]}\) 引用数组长度：$\({\#array[@]}\) （这里直接写代码的话博客会报错…我也不造为啥只好用\(LaTeX\)写了 命令行参数：使用$0, $1, $2 ...表示对应的参数。 特殊的内置变量: $#: 传递到脚本的参数个数 $*: 以一个单字符串显示所有向脚本传递的参数。如&quot;$*&quot;用&quot;括起来的情况、以&quot;$1 $2 ...$n&quot;的形式输出所有参数。 $$: 脚本运行的当前进程ID号 $!: 后台运行的最后一个进程的ID号 $@: 与$*相同，但是使用时加引号，并在引号中返回每个参数。如&quot;$@&quot;用&quot;括起来的情况、以&quot;$1&quot; &quot;$2&quot; … &quot;$n&quot; 的形式输出所有参数。 $-: 显示Shell使用的当前选项，与set命令功能相同。 $?: 显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。 运算符：不知直接做运算，必须以命令的形式返回运算式的值。如c=a+b应写成c=`expr a + b` 关系运算符：没有&gt;,&lt;之类的符号，只有类似命令的表示： [ $a -eq $b ]：a==b [ $a -ne $b ]：a!=b [ $a -gt $b ]：a&gt;b [ $a -lt $b ]：a&lt;b [ $a -ge $b ]：a&gt;=b [ $a -le $b ]：a&lt;=b 方括号里边的空格不能省略！py选手表示不开心（哼 布尔&amp;逻辑运算符：格式和关系运算一样。 布尔运算：!非，-o或，-a与 逻辑运算：&amp;&amp;与逻辑，||或逻辑 文件判断、字符串运算符：啊啊啊好多啊直接贴链接 条件语句： 123456789if [ $a == $b ]then 我是代码elif then 我是代码else 我是代码fi for循环： 1234for x in list:do 我是代码done while循环： 1234while condition:do 我是代码done until循环：执行代码直到条件为真时停止： 1234until conditiondo 我是代码done break和continue用法与其他语言一致。 自定义函数 定义一个名为fx的函数： 1234function fc () &#123; 我是代码 [return value]&#125; 定义开头的function可省略。 传参给函数时，函数内部使用$1, $2 ...来表示参数。 删除函数：unset.f fx 导入外部文件 source filename或者. filename expect命令 API 一种可以完成交互操作的shell，使用tcl语法。脚本需要在开头注明#!/bin/expect。基本命令有： set timeout x：设定执行命令若x秒没有回应，就跳过执行下一条。 spawn 命令：启动一个新的进程来执行spawn后的命令。 expect &quot;Regex&quot;：表示终端接收到符合匹配的输出时，执行expect之后的命令。 send &quot;str&quot;：以交互的形式向终端发送字符串。如果要模拟用户输入，要在最后加入换行符\r。 expect &quot;Regex&quot; {send &quot;str&quot;}：只有正则匹配成功时才执行send。 那个表达式匹配成功，就执行相应的send。可以所有的send都执行。 12345expect&#123; "Regex1" &#123;send "str1"&#125; "Regex2" &#123;send "str2"&#125; "Regex3" &#123;send "str3"&#125;&#125; 控制语句 函数，其中a,b,c是参数： 123456#定义：proc fx &#123;a b c&#125; &#123; 我是代码&#125;#调用:fx a b c 要注意大括号两边必须有空格。 条件：注意大括号位置都是固定的！不能随便换行！ 1234567if &#123;condition&#125; &#123; [我是代码]&#125; elseif &#123;condition&#125; &#123;&#125; else &#123; [我是代码]&#125; 命令行参数： [lindex $argv 0]表示第一个命令行参数（不包含脚本名字本身） $argv0表示脚本名字本身 $argc表示参数的个数]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[whyの学习笔记：python网络爬虫]]></title>
    <url>%2F2018%2F07%2F12%2Fwhy%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9Apython%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[使用Urllib 打开并读取一个url ： htmlName = urllib.request.urlopen('要打开的url').read().decode('编码方式')： 返回值是一个html格式的对象 为了正确显示中文，一般都在后面加上.decode('utf-8') 下载文件 ： 使用urllib.request.urlretrieve(DOWNLOAD_URL, './img/image1.png')，DOWNLOAD_URL是网络上的文件地址，第二个参数是本地的存储地址。 美丽の汤（BeautifulSoup） 是一种代替正则表达式检索网页内容的工具。导入：from bs4 import BeautifulSoup。 把一个html对象（就是urlopen返回的那个对象）变成美丽の汤（包装成一个soup类）：soup = BeautifulSoup(html, features='lxml')，features表示解析方法，一般就用lxml。 寻找某个特定标签（tag）名：soup.find('a')，返回值是tag名a第一次出现的tag，美味の汤会把这个tag包装为soup类（而不是tag类，和下面作区分） 寻找所有特定的标签（tag）名：result = soup.find_all('a')，返回所有标签名是a的tag。 返回值是一个可迭代对象，它的每一个元素是一个tag对象。 一个tag对象包含多个属性。可以直接检索包含特定属性内容的特定tag：result = soup.find_all('tag名', {'属性名:'用来匹配属性值的正则表达式'})。例如，想要找到所有叫做img的tag，并且要求这些tag的src属性的末尾是.jpg，可以使用result = soup.find_all('img', {'src:'.*\.jpg'})。 一个tag对象包含多个属性。检索特定属性信息方法和字典相同：tag['属性名']，一般比较关心该tag指向的链接，那么就用tag['herf']。 也可以直接通过tag.get_text()获取该tag的正文内容（如果有的话） requests模块 使用requests访问网站 网络数据收发方式： get：获取网页内容。用户发送的get请求一般会直接显示在url里。 post：把本地的数据传到服务器（例如账号登录，上传文件），用户post的数据不会显示在url中。 使用requests发送get请求：requests.get('要get的url', params=用字典表示的参数)。 返回值是符合get请求的一个 Response类（与之前urlopen返回的的html对象、美丽汤返回的soup类似） 。 例如调用百度搜索可以requests.get('http://www.baidu.com/s', params={&quot;wd&quot;: &quot;搜索内容&quot;})，就可以返回搜索页面对应的 Response类 。 post数据到服务端（账户登录等）：r = requests.post('要post'的url,data=要post的信息) 要post的信息用字典表示，例如{'username':'why', 'passwod':'233'} 返回值r也是一个 response类 。 以账户登录为例，登录之后会生成一个该账户登录状态的cookie，使用r.cookies获取 在向已登录状态的网页发送get请求，需要用到之前post得到的cookie：requests.get('要get的url', cookies=r.cookies) 可以使用r.content来访问 Response类 存储的内容。 使用会话：使用会话（session）可以免去每次get都要传递cookie： 创建一个session：s = requests.Session() 使用session进行post：r = s.post(url, data=data) post一次以后，就可以多次使用cookie进行get：s.get(url)，无需手动传入cookie。 使用requests下载数据 使用requests.get获取文件的内容，然后新建一个本地文件，将文件内容写入。 123r = requests.get(IMAGE_URL)with open('./img/image2.png', 'wb') as f: f.write(r.content) 其中stream=False表示先把整个文件下载到内存，然后再写入文件。如果想要实时下载并写入： 12345r = requests.get(IMAGE_URL, stream=True) # stream loadingwith open('./img/image3.png', 'wb') as f: for chunk in r.iter_content(chunk_size=32): f.write(chunk) chunk=32表示将要下载的文件分成多个区块，每个大小为32byte。 网络爬虫 爬取数据的一般步骤： 进入网页：一般使用urlopen函数 搜索数据：使用BeautifulSoup从html中锁定tag，然后用正则表达式搜索想要的内容。总之就是美味汤和正则混合使用… 下载数据：使用urllib或者requests 高级爬虫Selenium 可以使用浏览器自动完成操作。需要安装：python库Selenium，相应的浏览器插件KATALON Recorder（用来记录操作和生成python代码） 只要把生成的代码复制以后执行即可，需要导入的内容： 123from selenium import webdriverdriver = webdriver.Chrome() # 也可以是别的浏览器[复制来的代码] 使用html = driver.page_source来获取网页的html 可以使用driver.get_screenshot_as_file&quot;./img/sreenshot1.png&quot;)来截图 最后要driver.close()关闭浏览器 如果让浏览器在后台执行，作如下修改： 123456from selenium.webdriver.chrome.options import Optionschrome_options = Options()chrome_options.add_argument("--headless") # define headlessdriver = webdriver.Chrome(chrome_options=chrome_options) 多进程爬虫 与普通的多进程程序一样，把要多进程加速的函数放入进程池pool，唯一要注意的是为了防止多个进程爬取到重复的网页，可以定义集合（set）来存储已经爬取过的网页(seenUrl)和未爬取的网页(unseenUrl)。每当某个进程爬取到一个网页之后，就从unseenUrl中删除该网页，向seenUrl中加入该网页，所有爬虫都从unseenUrl中读取网页。 爬虫框架Scrapy 新建一个Scrapy项目： 1scrapy startproject &lt;项目名&gt; 在项目中新建一个爬虫（spider），会建立一个爬虫的模板： 1scrapy genspider &lt;爬虫名字&gt; &lt;要爬的域名&gt; 使用爬虫进行爬取： 1scrapy crawl &lt;爬虫名字&gt; 爬虫名字.py进行数据的爬取，items.py定义爬取到数据的结构，pipelines.py定义处理爬取到的items的方法。 使用交互环境scrapy shell： 1scrapy shell "url" 打开包含该url的Response的scrapy shell item类 一般写在items.py中，用来保存爬取到的数据的结构，类似字典。格式为： 1234class myItem(scrapy.item): 成员1 = scrapy.Field() 成员2 = scrapy.Field() ...... scrapy.spider.Spider类 需要包含以下成员： name: 爬虫的名字，必须是唯一的 allowed_domains：允许爬取的域名列表 start_urls：url列表，在找不到后续连接时，从该列表中爬取 可以有以下方法： start_requests（不必要）：用来创建用于爬取的request对象，可以不写，默认是从start_urls中的url自动创建。如果希望修改最初的爬取对象，需要重写该方法。 parse（必要）：接受并解析response对象（类似之前requests中的response对象），从中提取出item，并进一步生成要跟进的url的request对象。 该方法是一个生成器，通过yeild返回爬取的数据（item） 要解析的网页数据在response.body中。 一般使用response.xpath(&quot;xpath地址&quot;)来筛选元素，返回符合条件的tag列表。xpath简单语法如下所示： xpath 含义 /html/head/title 选择&lt;HTML&gt;文档中 &lt;head&gt; 标签内的 &lt;title&gt; 元素 /html/head/title/text() 选择上面提到的 &lt;title&gt; 元素的文字 //td 选择所有的 &lt;td&gt; 元素 //div[@class=&quot;mine&quot;] 选择所有具有 class=&quot;mine&quot; 属性的 div 元素 执行过程： 为start_urls中的url创建相应的request对象 request对象创建response对象，并传给parse方法进行解析 parse将解析得到的item形式的数据返回。 scrapy.contrib.spiders.CrawlSpider类 一种scrapy内置的常用爬虫，是spider的子类。有额外的属性rules：包含爬取规则的列表，每个元素时一个rule对象，列表越靠前的规则优先级越高。对于一个rule对象，包含： link_extractor：是一个 Link Extractor 对象。 其定义了如何从爬取到的页面提取链接。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AI酱养成计划（六）种一片随机森林！]]></title>
    <url>%2F2018%2F07%2F11%2FAI%E9%85%B1%E5%85%BB%E6%88%90%E8%AE%A1%E5%88%92%EF%BC%88%E5%85%AD%EF%BC%89%E7%A7%8D%E4%B8%80%E7%89%87%E6%A3%AE%E6%9E%97%EF%BC%81%2F</url>
    <content type="text"><![CDATA[植树造林，从我做起 \(Bagging\)集成学习 &amp; 随机森林 集成学习 在机器学习的过程中，常常会遇到过拟合的问题。集成学习是解决过拟合问题的一种优良途径。所谓集成学习，指的是一次训练多个学习器，最后将这些学习器综合在一起进行使用。由于许多弱学习器便于训练，但效果并不理想，因此可以考虑通过弱学习器集成学习的办法，来获得一个强学习器来进行最终的分类或回归任务。有时，也会将不同类型的学习器进行集成（例如决策树和SVM）。 假设现有多个学习器\(t_1,t_2,...t_n\)，现在将这\(n\)个学习器集成学习，得到一个强学习器\(T\)。在具体的操作过程中，每个学习器\(t_i\)的训练过程和原来独立学习时大致相同，但要尽量满足各个学习器“好而不同”的原则——即在保证每个学习器分类/回归正确率的基础上，尽量使得学习器富有多样性（如果所有学习器都一样，那就和没集成一样了）。因此我们在训练子学习器\(t_i\)的过程中，常常希望引入一些随机因素，在我们要介绍的\(Bagging\)集成学习中，常用的方法是在划分训练集时使用一种随机采样法：自助采样法 自助采样 自助采样方法原理十分简单。假设现有的数据集是\(\Bbb D\)，我们想要从中划分出训练集\(S\)，自助采样的做法是： 首先令\(S\)为一空集 从\(\Bbb D\)中随机抽取一个样本\(x\)，加入到\(S\)中，\(x\)要放回\(\Bbb D\)中，以保证之后还可以被继续抽取 重复\(|\Bbb D|\)次抽取，即\(\Bbb D\)有多少元素，就随机抽取多少次 剔除\(S\)中的重复样本（集合中不能有相同元素） 根据采样过程，可以轻易地计算出来，当采样次数足够大时，假设\(|\Bbb D|=m\)，那么\(\Bbb D\)中任意一个元素不被抽取的概率是\(1-\frac{1}{m}\)，于是\(S\)中最终的元素个数是： \[m\lim_{m \to \infty} \left( 1-\frac{1}{m} \right) ^m=\frac{1}{e} \approx 0.37m\] 因此，这样随机抽样的方法得到的训练集大小合适，剩下的部分可以作为验证集，或者用来辅助判断模型的过拟合情况（例如用来给决策树剪枝）。通过对每个子学习器进行自助采样，就可以保证每个子学习器是从不同的数据集训练出来的，也就保证了学习器的多样性。 模型测试 集成模型的测试过程与单一模型不同。由于包含多个子模型，在进行预测时\(n\)个子学习器会给出\(n\)个结果。 对于分类任务，往往直接选取出现次数最多的类别最为预测结果 对于回归任务，采用求均值的方法： 可以直接求算数平均数\(y=\sum_{i=1}^ny_i\) 也可以采用加权均值\(y=\sum_{i=1}^nw_iy_i\)，权重也作为待学习的参数（例如可以采用梯度下降的方法）。 接下来，我们以随机森林算法为例，实现\(bagging\)集成学习。 随机森林 随机森林顾名思义，是由很多棵决策树集成的森林。(什么是决策树？请看我的这篇博客）在进行训练时，单个决策树的训练、剪枝等方法与之前相同，要做出的一些改变是： 训练集的划分采用自助采样法，划分剩下的数据可以用来剪枝。 在决策树分支时，以往的做法是，计算出当前节点处按各个特征划分时的信息增益，并选出增益最大的特征，以该特征为依据进行数据划分。现在我们有了多个决策树，我们希望为每棵树引入一些随机量，来保证集成学习的多样性。因此，在当前节点可供划分的\(p\)个特征中，随机地选出\(k\)个计算信息熵（一般来说，选择\(k={log_2}^p\)效果较好），并在这\(k\)个特征中选出信息增益最大的一个进行划分。 测试 接下来给出随机森林的python代码。首先构建一个类封装随机森林的树结构、数据集以及训练和测试方法，其中数据集使用自助采样的方法： 123456789101112class Forest(object): def __init__(self, trainData, testData, trainLabel, testLabel): self.trainData = trainData self.trainLabel = trainLabel self.testData = testData self.testLabel = testLabel self.trainIndex = [] self.trees = [] for _ in range(treeNum): self.trees.append([]) self.trainIndex.append(set([np.random.randint(0, dataNum) for _ in range(dataNum)])) 接下来调用之前在这篇博客中写好的API，构造最简单的\(ID3\)决策树，并在节点划分的部分加以修改，封装train()和test()方法： 1234567891011121314151617181920def train(self): for i in range(treeNum): featNO = [0, 1, 2, 3, 4, 5] data = np.array([self.trainData[x] for x in self.trainIndex[i]]) label = np.array([self.trainLabel[x] for x in self.trainIndex[i]]) self.trees[i] = API.plant(data, label, featNO)def test(self): count = 0 num = len(self.testLabel) predLabel = np.array([API.pred(self.testData, T) for T in self.trees]) finalPred = [] for i in range(num): pred = [predLabel[j][i][1] for j in range(treeNum)] finalPred.append(max(pred ,key=pred.count)) for i in range(num): if self.testLabel[i] == finalPred[i]: count += 1 acc = count/num return acc 这样就完成了一个随机森林。经过测试，之前的决策树正确率在\(0.87\)左右，而一个包含\(20\)棵树的随机森林，在相同的测试集上，正确率平均可以达到\(0.985\)，可见集成学习的威力。让我们一起植树造林吧！ヽ(✿ﾟ▽ﾟ)ノ \(2018.7.11 \;\; by \; WHY\)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（五）贝叶斯方法]]></title>
    <url>%2F2018%2F07%2F08%2FAI%E9%85%B1%E5%85%BB%E6%88%90%E8%AE%A1%E5%88%92%EF%BC%88%E4%BA%94%EF%BC%89%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[概率论只不过是把常识用数学公式表达了出来。——拉普拉斯 #5 贝叶斯方法&amp;概率论模型 \(\mathbf{Attention:}\)阅读本文需要一些概率论知识(。・・)ノ 贝叶斯理论 贝叶斯方法是一种基于概率论的重要的机器学习方法。概率论试图将复杂的事件和数据分布用数学的语言表示，对于机器学习要处理的数据而言，以分类问题为例，实际上就是求对于给定样本\(X\)，其标签为\(y\)的概率（后验概率）。只要得到标签集合\(Y\)中所有标签的概率，那么出现概率最大的\(y\)就是最终的分类结果，这样就完成了分类任务。 但是对于给定的\(x\)，一般很难求出其后验概率。因此这里使用到贝叶斯公式： \[P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}\] 贝叶斯公式给出了后验概率和先验概率之间的关系，因此可以方便地根据\(P(X|Y)\)计算出后验概率。贝叶斯方法多是基于这个公式而来的。接下来我们以分类问题为例进行讨论。假设给定训练集\(\Bbb D=\{(X_1,y_1),(X_2,y_2)...(X_n,y_n)\}\)，其中\(X_i=[x_1,x_2...x_m]\)是特征向量，\(y_i\)是类别标签。 朴素贝叶斯 朴素贝叶斯是最简单的贝叶斯方法。其简单之处在于两个假设（独立同分布假设，记作\(i.i.d\)假设）： 假设\(X\)的各个特征\(x\)相互独立，互不影响 假设这些特征对于分类的贡献同等重要。 当然，事实上这两个条件都很难成立，但是对于某些问题，特征之间的独立性影响并不大，这样的假设可以大大简化模型，这也正是“朴素”一词的含义。因此朴素贝叶斯模型的训练过程，实际上就是统计和计算各个概率的过程。需要计算的有： \[ \begin{cases} P(X|y)=\prod_{i=1}^n P(x_i|y) \\ P(X)=\prod_{i=1}^n P(x_i) \end{cases} \] \(P(y)\)则直接根据统计得出。要注意，上述计算方法仅适用于特征分布是离散值的情况下（例如人的性别只有男女两种可能取值）。如果特征属性是连续值的情况下（例如人的身高），在计算上述概率时，假设数据都遵从高斯分布，使用极大似然法估计出概率密度函数来进行计算。 贝叶斯网络 概率图模型 在现实中，上述的两个假设很多时候是无法成立的。即样本的各个特征之间并非相互独立，而是有着因果关系（依赖关系）。这个时候独立同分布假设就失去了作用，我们需要想办法表示出这些不同属性之间的依赖关系，这种办法就是贝叶斯网络。 贝叶斯网络使用有向无环图来表示这些依赖关系： 使用图的节点来表示各个特征（相当于概率论中的随机变量）。 使用有向边表示特征之间的相互依赖关系。例如\(A\to B\)表示\(B\)依赖\(A\)。 在各节点处计算出当前节点（假设为\(\alpha\)）与其所有父节点（假设为\(\pi\)）的联合概率，即\(P(\alpha|\pi)\)。 要说明的是，贝叶斯网将样本的标签也当做了一种特征，作为网络的一个普通节点。事实上，贝叶斯网不仅可以通过已知的\(X\)来对标签\(y\)进行推断，也可以根据样本的各个类别和标签中任意个已知量来推断其他未知量的值。假设一个贝叶斯网络的结构已经确定，那么其训练过程就是计算各个依赖关系对应的联合概率的过程。 利用贝叶斯网进行推断 假设\((X,y)\)的\(m\)个特征值和类别标签\(y\)这\(m+1\)个变量中，已经观测到的变量集合的取值为\(E=e\)（称为“证据变量”），其余待推断的变量集合为\(H\)，那么我们要推断的\(H\)的取值\(h\)应该是使得后验概率\(P(H=\mathbf h|E=\mathbf e)\)最大的取值\(h\)。即： \[h=arg\max_h P(H=\mathbf h|E=\mathbf e)\] 这样一来，就需要计算\(H\)的各个可能取值下的后验概率\(P(H=\mathbf h|E=\mathbf e)\)。当未知变量只有类别标签\(y\)时，可以根据\(P(y|X)=\frac{P(X,y)}{P(X)}\)直接计算后验概率。要注意这里联合概率的计算要考虑变量之间的依赖关系，计算方法为： \[P(x_1,x_2...x_n)=\prod_{i=1}^mP(x_i|\pi_i)\] 其中\(\pi_i\)表示\(x_i\)节点的父节点集合。这样根据这个式子就可以容易地计算出： \[P(X|y)=\frac{P(y|\pi_y)\prod_{i=1}^mP(x_i|\pi_i)}{\sum_y\prod_{i=1}^mP(x_i|\pi_i)}\qquad \quad(1)\] 其中分母表示在\(y\)取不同值时，分别对应的\(X\)的概率之和。 马尔科夫链/蒙特卡罗方法（MCMC）&amp;吉布斯采样 然而不幸的是，在对多个未知变量进行推断时，直接计算后验概率是NP难的，复杂度为\(O(2^n)\)，因此在实际的推断中我们常常采用模糊推断的办法。 模糊推断是靠马尔科夫链/蒙特卡罗方法来完成的。这里使用一种简单的蒙特卡洛方法：吉布斯采样。我们的目标是在已经观测到证据变量\(E=\{e_1,e_2...e_p\}\)的情况下，求出未知变量\(H=\{h_1,h_2...h_q\}\)的任意一个取值\(\mathbf h&#39;\)出现的概率，即\(P(\mathbf h&#39;|E=e)\)吉布斯采样的大致流程如下： 初始化计数器\(i=0\)，并事先规定采样轮数\(T\) 假设证据变量的取值为\(\mathbf e\)，那么首先随机产生\(H\)的取值，假设为\(\mathbf h_0\) 对\(H\)中的每个元素\(H_i\)都进行采样，具体做法是：计算出针对\(H_i\)的后验分布\(P(H_i|\text{除}H_i\text{之外的所有变量})=P(H_i|\mathbf e,h_1,h_2...h_{i-1},h_{i+1},...h_q)\)，计算方法与式\((1)\)相同，然后将\(H_i\)的取值\(h_i\)更新为出现概率最大的\(h_i\)。即 \[H_i=arg\max_{H_i}P(H_i|\mathbf e,h_1,h_2...h_{i-1},h_{i+1},...h_q)\qquad(2)\] 将未知变量集合\(H\)整个更新过一遍后，如果此刻恰好\(H=\mathbf h&#39;\)，为\(i\)增加一个计数\((i+=1)\) 经过全部\(T\)轮采样后，要求的\(P(\mathbf h&#39;|E=e)\approx \frac{i}{T}\)。 注意到，MCMC方法其实是在未知特征变量张成的样本空间中进行随机漫步，并且每一步只与前一步的状态有关，这正是一个马尔科夫链。当\(t \to \infty\)时，马尔科夫链将会收敛于一个稳定的分布，因此可以使用这样的方法来逼近\(P(\mathbf h&#39;|E=e)\)。 使用MCMC得出\(H\)的后验分布之后，只要根据式\((2)\)，从其中选取概率最大的分布状态，就可以作为贝叶斯网的推断结果。 期望优化（EM）算法 现在让我们更进一步——在实际的训练中，由于种种原因，训练集经常会有一些变量信息缺失（比如某些特征无法观测）。对于依赖统计的贝叶斯网而言，模型参数\(\Theta\)就是各个节点处的概率分布信息。这么一来，数据的缺失就意味着参数的缺失，导致网络的残缺。为了应对缺失的变量，贝叶斯网络采用的方法是EM算法。假设可观测变量集合为\(X\)，无法观测的变量集合为\(Z\)，EM算法的大致流程如下： 随机初始化参数的未知部分，得到初始参数集合\(\Theta_0\) E步（期望）：根据参数\(\Theta_0\)，推断出缺失数据的期望\(Z\)。具体做法是，对于某个节点\(x\)处的变量取值，求出其概率分布，并求加权和。即\(E(x)=\sum_x P(x)x\) M步（最大化）：根据推断出的新的数据集，寻找最大化似然的参数。具体做法是直接统计并计算出参数，作为新的\(\Theta_1\) 重复E步和M步，直到收敛，就可以得到最终填充的数据，以及最终确定的网络参数\(\Theta\) 可以看出，EM算法类似坐标下降法，每次只优化参数和样本数据这二者中的一个，通过不断迭代来达到收敛，这样就解决了部分数据缺失的问题。 结构学习 接下来让我们再进一步——之前讨论的都是假设网络结构已经确定的情况下，网络的运行方式。但是在实际的训练中，网络的结构往往是最难确定的。在变量个数较少时，可以由领域专家来构造网络结构，但是当数据量很大时，网络结构有多种可能性，不同的网络结构可能对结果造成很与大影响，这个时候就要想办法进行网络结构的学习。 选择网络结构的一般方法是：在已知数据集\(\Bbb D\)的情况下，首先确定一个评分函数，然后在网络的各种可能结构中，选择出使评分函数最大（或最小）的一个结构。我们的选择标准（评分函数）不同，最终得到的网络结构也不同。假设贝叶斯网表示为\(B=(G,\Theta)\)，其中\(G\)表示网络结构，\(\Theta\)表示网络参数。常用的评分函数如下： 贝叶斯评分：贝叶斯网结构的后验概率的的对数似然函数。也即： \[logP(\Bbb D|G)+logP(G)\] 其中第一项是网络结构的对数似然，第二项是结构先验分布（一般假设是均匀分布）。其中\(P(\Bbb D|G)\)可以展开为: \[P(\Bbb D|G)=\int P(\Bbb D|G,\Theta)p(\Theta|G)d\Theta\] 直观上来讲，贝叶斯评分选择的是能够最好地拟合训练数据的网络结构。 贝叶斯信息准则(\(BIC\))：使用拉普拉斯近似，并综合考虑使得网络的编码长度尽量短，可以得到评分准则： \[BIC(\Bbb D|G)=logP(\Bbb D|G,\Theta)-\frac{|B|}{2}logm\] （讲道理这里BIC的推导我也没懂….有明白的同学阔以在评论区留言！笔芯！） 结构优化 在确定评分函数之后，就可以对结构进行优化了。这是一个图搜索的问题，显然如果使用穷举的方法，计算将是\(NP\)难的。因此实际使用中我们常常使用启发式的方法。假设我们从一个已经确定的初始的网络结构\(G_0\)开始（初始的网络最终还是要人工确定…），通过不断迭代的优化方法来逼近评分函数的最值： 使用搜索算子（搜索算子包括加边、减边和转边）对当前模型进行局部修改，得到一系列候选模型 计算所有候选模型的评分函数，取最优的候选模型作为新的模型 重复上述两步，直到收敛。 马尔科夫模型 马尔科夫链 首先要介绍的是马尔科夫链。马尔科夫链指的是一系列随机变量\(x_1,x_2...x_n\)，任意一个随机变量\(x_t\)都只受前一个随机变量\(x_{t-1}\)的影响，\(x_t\)与除了\(x_{t-1}\)之外的其他变量都相互独立。事实上，这就是一个简单的贝叶斯网络。 \[\boxed{x_1}\to \boxed{x_2}\to \boxed{x_3}\to \boxed{x_4}......\to \boxed{x_n}\] 这个模型十分简单有效，在NLP领域有广泛的运用。例如对语料进行情感分析时，语句作为一个序列，恰好符合马尔科夫假设的特点——一般来说，一个句子中的词常常与它的前一个词相关。我们只要根据贝叶斯网络的训练及推断方法，就可以完成马尔科夫链的训练和推断。 隐变量 而隐马尔科夫模型，则是在马尔科夫链中引入了“隐变量”的概念。假设一个马尔科夫链中的随机变量\(x_1,x_2,...x_n\)无法直接观测到，但是每一个随机变量\(x\)都可以产生一个可以观测的信号\(y\)，隐马尔科夫模型就是通过观测\(y\)的值，来进行推断。 \[ \begin{matrix} &amp;\boxed{y_1}&amp; \quad\boxed{y_2}&amp; \quad\boxed{y_3}&amp;\quad\boxed{y_4}&amp;\quad\quad\;\;\boxed{y_n}&amp; \\ &amp; \uparrow &amp;\quad \uparrow &amp; \quad\uparrow &amp;\quad\uparrow &amp;\quad\quad\;\; \uparrow &amp;\\ &amp; \boxed{x_1}&amp;\to \boxed{x_2}&amp;\to \boxed{x_3}&amp;\to \boxed{x_4}&amp;......\quad \boxed{x_n} &amp; \end{matrix} \] 马尔科夫随机场 （未完待续） 突然觉得贝叶斯好难啊啊啊 贝叶斯网引论273页，看完了再更 （溜走]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[whyの学习笔记:Pytorch]]></title>
    <url>%2F2018%2F06%2F10%2Fwhy%E3%81%AE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Pytorch%2F</url>
    <content type="text"><![CDATA[基本类 张量（Tensor）: 一个tensor实例有以下常用成员： Tensor.data: 张量的数据本体 Tensor.grad: 该张量处的梯度（如果有的话） Tensor.grad_fn: 指向在该张量处进行的函数计算（计算图中的计算节点），进行梯度反向传播时会用到。如果是由用户创建的tensor，则grad_fn = None。 Tensor.grad_fn.next_function: 上一级节点处的grad_fn。 tensor有以下常用函数/方法： 创建Tensor torch.eye(n)，返回一个单位阵。 torch.ones(input)，返回一个维度为input的全1的Tensor torch.zeros(input)，返回一个维度为input的全0的Tensor torch.t(input)将input进行转置 troch.full(size, value, device)返回 常用操作 把numpy数组转换为tensor：torch.from_numpy(array) 将Tensor展开为特定的size：tensor.view(x,y)其中参数x,y是展开后的size。 如果x,y中任意一者为-1，则表示该维度自动计算（例如4x4的张量a.view(-1,8)展开为2x8） 如果tensor.view(-1)表示展开为1*n的张量。 tensor.squeeze(n)表示若tensor的第n维度是1，则去掉该维度（例如3x3x1张量a.sqeeze(3)变成3x3） tensor.unsqueeze(n)：sqeueze的逆操作，参数含义相同。 取最大值：torch.max(tensor, axis) axis=1时返回每一行的最大值以及对应的列索引 axis=0时返回每一列的最大值以及对应的行索引 不传入axis，直接用torch.max(tensor)，返回所有元素中的最大值。 Autograd 设置Tensor.requires_grad = True时，pytorch会自动追踪对该张量进行的计算。只要调用Tensor.backward()即可反向计算出所有节点处的梯度。 如果不想追踪对张量的计算， 可以使用： Tensor.detach()，把该张量从计算图中分离。 使用with torch.no_grad():来包装代码。被包装部分定义的Tensor，都不会加入计算图中。使用示例： 12with torch.no_grad(): [我是代码] 使用反向传播Tensor.backward()： Tensor.backward(gradient=grad)： grad是一个张量，用来表示待计算的Tensor的各个元素的计算比例。(???存疑) 注意：使用.backward()方法的张量必须是标量（只有一个元素） nn.Module类 传给nn.Conv2d()的张量size应当为：\(batchSize \times channels \times height \times width\) nn.Conv2d()的输入输出的图像尺寸的关系为： \[output = \frac{input-kernelSize+2\times paddings}{stride}+1\] 常用设置：核尺寸\(3\)，步长\(1\)，\(padding\;1\)。 损失函数使用nn.函数名调用 优化方法（adam什么的）在nn.optim中。 一般的训练过程： 12345678dataloader = torch.utils.data.dataloader.Dataloader(traindata, batchsize, shuffle)for i in range(epoch): for j, (x,y) in enumerate(dataloader): pred = model(x) loss = lossFunc(pred, y) # 计算loss optim.zero_grad() # 将网络中的所有梯度初始化，这一步必须在backward之前。 loss.backward() # 计算所有参数的梯度（仅计算不更新） optim.step() # 更新梯度 （实际训练过程中根据实际情况进行改动） nn.Module中的网络层 nn.Linear(m, n)线性全连接层。接受一个张量，输出一个张量，输入张量的size必须为(*,m)，输出的size为(*,n)。即只对最后一个维度进行全连接计算，再将各个维度拼接起来。这是为了保证在网络中进行随机梯度下降时(假设batchSize = b)，最后传到全连接层的张量size为(b,m)，这样设计可以保证全连接层的输出size为(b,n)，即只对每个样本进行计算，而不会把不同样本之间的数据放在一起计算。 使用1x1卷积，相当于在网络中使用一个全连接层，也可以用来压缩channel数。 使用cuda加速运算 使用t.cuda.is_available()判断cuda是否可用。 把模型/张量放到gpu计算：x=x.cuda() torch.device(&quot;设备名&quot;)定义了计算时使用的设备，例如torch.device(&quot;cuda&quot;)表示使用cuda，torch.device(&quot;cpu&quot;)表示使用cpu。使用x.to(torch.device(&quot;设备名&quot;))可以将模型/张量放到相应的设备上。 数据加载 Dataset 重写torch.utils.data.dataset类。该类必需的方法有： __init__()：初始化数据集，一般传入数据存放位置，存储标签信息文件路径等 __getitem__()：定义给定一个索引，加载相应样本的方法。传入参是索引i，返回数据集中第i个样本的样本文件（tensor格式PIL.image格式(如果是图片的话)）以及该样本的标签。即： 123def __getitem__(self, index) [我是代码] return sample, label __len__()：返回数据集中的样本个数。 加载图片使用PIL模块中的Image.open(path)函数 Dataloader 然后将dataset加载为dataloader：torch.utils.data.dataloader.Dataloader(dataset, batch_size=n, shuffle=True, drop_last=True, num_worker) * batch_size表示每个batch的大小。 * shuffle表示是否在训练时打乱数据。 * drop_last表示当最后剩下的数据不足一个batch时，是否丢弃。 * num_worker表示加载数据使用的线程数。没错仅仅是加载数据的时候用一下多线程… 图像操作（预处理） 都在torchvision.transforms中。 Resize(h, w)：将图片缩放为指定尺寸。只传入一个参数x时，将图像缩放使得其中一条边大小为x。 CenterCrop(h, w)：从图片的中心裁剪下h, w大小。只传入一个参数x时，裁剪正方形。 RandomCrop(h, w)：从图片中随机位置裁剪下h, w大小。只传入一个参数x时，裁剪正方形。 ToTesor()：把PIL_Image或numpy数组变为tensor，同时进行归一化操作。 Normalize()进行归一化操作。 ToPILImage(tensor)：把tensor变为PIL格式 tensor.numpy()：变为numpy数组 使用matplotlib显示图片：plt.imshow(np.transpose(img, (1, 2, 0)))，img是由tensor转化来的数组。之所以要转置，是因为torch和numpy中表示图片的格式不同（分别是channel*h*w和h*w*channel） 保存图像到本地：torchvision.utils.save_image(img, path) 训练网络的技巧 首先检查在训练集上的效果。如果训练集上效果就不好： 试着换个损失函数/激活函数 改变网络结构。很深的网络会导致靠前的层收到的梯度很小，学习很慢，导致梯度消失。可以调整学习率来应对。 使用maxout网络，即让网络自己学习该用什么激活函数 换一种优化方法 如果训练集效果好，测试集效果差（过拟合）： 早停（当训练集正确率上升而测试集正确率下降时） 使用dropout和dropconnect，即删除无用的部分 加入正则项 番外篇 - 论why有多蠢 遇到的坑 CudaRuntimeError 标签索引溢出： 12RuntimeError: cuda runtime error (59) : device-side assert triggered at/opt/conda/conda-bld/pytorch_1524586445097/work/aten/src/THC/generic/THCTensorCopy.c:70 常常是因为label编号的最大值大于了总label数。大概率是因为网络最后的FC层输出没有和label数目匹配….换数据集一定要记得改全连接层啊啊啊啊]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[whyの学习笔记 深度视觉识别（from CS231n & NTU-ML）]]></title>
    <url>%2F2018%2F06%2F06%2Fwhy%E3%81%AE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%B7%B1%E5%BA%A6%E8%A7%86%E8%A7%89%E8%AF%86%E5%88%AB%EF%BC%88from-CS231n%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1.激活函数 sigmoid函数：\(\sigma(x)=\sum_{i=1}^n(w_ix_i+b)\)： 使用了指数，计算量大 由于有饱和区域，在输入较大的\(x\)时，会导致梯度消失（在饱和区梯度为0） 当训练数据全为正或全为负时，会导致梯度更新很慢。由于其公式: \[\sigma(x)=\sum_{i=1}^n(w_ix_i+b)\] 使得计算图中\(\sigma\)节点处总有\(\frac{\partial \sigma}{\partial w}=x\)，使得\(w\)的更新只能向着梯度全为正（或全为负）的方向更新，使得更新效率极低。 解决办法：将数据归一化到均值为\(0\)的区间内，如\([-1,1]\) tanh函数：\(tanh(x)\) 由于有饱和区域，仍然会导致梯度消失（在饱和区梯度为0） ReLU函数：\(f(x)=\max(0,x)\) 收敛更快（大概是sigmoid的6倍） 在正半轴不会出现梯度消失 符合生物学理论 计算成本低 在负半轴处有饱和区，会有梯度消失（dead ReLU） 不是以\(0\)为中心 参数整流器(PReLU)：\(f(x)=\max(\alpha x,x)\) Leaky ReLU：\(f(x)=\max(0.01x,x)\) 在整个实数域上没有饱和区，完全不会出现梯度消失。 同时也具有ReLU的优点 ELU函数： 表达式为：\(f(x)=\begin{cases} x \qquad if \ x&gt;0\\\alpha(e^x-1) \qquad if \ x\leqslant0 \end{cases}\) 具有ReLU的优点 对噪音有更强的鲁棒性 maxout函数：\(\max(W^Tx_1+b_1,W^Tx_2+b_2)\) maxout的本质相当于是让网络自己学习应该使用什么激活函数。maxout将两个神经元合并成一个（只输出最大值），由于两个神经元的参数都是学习得到的，因此通过参数在学习过程中的变化，两个神经元的输出组合也在不断变化，相当于学习了激活函数。 由于两个神经元都是线性的，因此最终maxout学习得到的激活函数是分段的线性函数（例如也有可能学习到ReLU） 在训练时，假设另一个较小的神经元不存在。 Softmax 一种归一化激活函数，将一个给定向量\(K=[k_1,k_2,...k_n]\)压缩到另一个向量\(K&#39;=[k&#39;_1,k&#39;_2,...k&#39;_n]\)，并且使得\(K&#39;\)中的元素都在\((0,1)\)之间，且总和为\(1\)（类似概率的分布）。任意一个元素\(k\)的softmax表达式为： \[k&#39;=softmax(k)=\frac {e^k} { \sum_{i=1}^ne^{k_i}}\] 常常将softmax层放在输出层之前。 权重初始化 当网络很深时，不要将权重初始化为很小的值。这样会导致反向传播时，求导得到的梯度值也很小，容易引起梯度消失。 Xavier初始化：每层的权重\(W_{m,n}\)都从标准高斯分布中随机采样，并将结果除以\(\sqrt{m}\)。即： 1W = np.random.randn(m, n) / np.sqrt(m) 使用Xavier初始化时，如果对应层使用ReLU函数，需要将\(W\)除以\(2\)（因为有一般的输入数据被丢弃） 2. BN（批量归一化） 在网络中加入BN（批量归一化）层。BN层中发生的事情是：对于输入BN层的任意mini-batch，假设该batch中有\(N\)个样本，每个样本维度为\(D\)。也即输入数据集\(X\)的规模为\(N\times D\)。我们先来看看一般的归一化方法：对于每个维度（每个特征）上的\(X\)都求均值和方差，并据此对每个维度上的数据进行归一化。对第\(k\)个维度，归一化之后的数据为： \[\hat x_k=\frac{x_k-\mathrm{E}[x_k]}{\sqrt{\mathrm{Var}[x_k]}}\] 其中\(\mathrm{E}[x_k]\)是第\(k\)维所有\(x\)的均值，\(\mathrm{Var}[x_k]\)是第\(k\)维所有\(x\)的方差。这一过程通常发生在全连接层或卷积层之后，激活层之前。 在原论文中，BN放在激活层之前（猜想可能是因为tanh和sigmoid之类的函数中间部分不是线性区，通过BN可以避免梯度消失），但是后续实践中也有说放在激活层之后更好的？我懵逼了.jpg 也许，还是要根据具体情况来进行判断 这么做虽然可以达成归一化的目的，但是一定程度上破坏了之前学习到的数据分布，因此采用以下方法改进： 对于batch中每个维度的数据，引入参数\(\gamma_k,\beta_k\)。并令这两个参数可以训练。 改进版的归一化中，我们使用\(y_k=\gamma_k\hat{x}_k+\beta_k\)来进行归一化操作。其中\(\hat{x}_k\)是刚才普通归一化得到的结果。之前提到，\(\hat{x}_k\)可能会导致数据分布损失，但是经过\(y_k=\gamma_k\hat{x}_k+\beta_k\)，由于\(\gamma, \beta\)是可学习的，可以通过这两个值的自动学习调整，减少\(y_k\)的损失。这样就既保存了之前的学习成果，又方便了之后的训练。妙蛙！(๑•̀ㅂ•́)و✧ 理解 实际上，BN使得网络内的数据分布变为了具有固定方差和均值的数据，而\(\gamma, \beta\)的存在使得数据具体是什么均值和方差可以被控制，一定程度避免了破坏学习成果。 最后，为什么将某一层的数据改为固定均值和方差，可以加速训练呢？因为对该层来说，其输入数据是由之前的层生成的，而之前的层也在不断训练，它产生的数据的分布就会不断变化，影响了当前层的训练。而BN让这些数据归一化，变得h整整齐齐便于训练。 此外，BN具有一定的正则化效果，因为使用的是一个batch的均值方差，并不精确等于真实的均值方差，相当于引入了噪音。 ### 测试时怎么做？ 测试时，没有了batch的概念，那么均值方差怎么办？方法是记录下训练中所有batch的均值方差，并对这一系列值进行指数加权平均，作为测试使用的均值方差。 3. SGD的优化方法 普通SGD（\(w^{new}=w-\alpha \nabla f(x)\)）缺点： 会陷于局部极小值（鞍点） 梯度方向不与全局方向相同 部分样本的梯度与总体梯度不一定相同 类似一个小球以十分不科学的均匀速度滚下山，而且每个时刻的速度方向都是向着当前的最陡峭方向 动量法(Momentum) 在梯度项后加入一个动量（速度）项，速度的方向就是前一步的梯度方向： \[v^{new}=\nabla f(x)+\rho v,w^{new}=w-\alpha v^{new}\] v的初始值为0，在每次迭代中更新。\(\rho\)是摩擦系数（超参数，通常取\(0.9\)）。类似一个小球滚下山，但速度越来越快，由于引入摩擦系数，可以保证最终停在山脚（感觉科学了一些呢）。这样可以在“惯性”的作用下越过鞍点，收敛速度也更快。 Nesterov动量（Nesterov Momentum） Nesterov动量（右）与普通动量法（左）的比较 在某一点处，假设向着当前速度行进一段时间后，计算出行进后某点处的梯度方向，然后将二者合成，作为当前实际的前进方向: \[v^{new}=\rho v-\alpha \nabla f(x+\rho v),w^{new}=w+v^{new}\] AdaGrad 在迭代过程中把每一步计算得到的梯度平方项累加，并把计算后的梯度除以当前的累加项（对不同的参数，都各自维护一个不同的\(S\)）： \[S^{new}=S+(\nabla f(x))^2,w^{new}=w-\alpha \frac{\nabla f(x)}{S^{new}}\] 优点：可以避免梯度下降时对各个维度的\(w\)敏感程度不同（感觉有些类似归一化） 缺点： 导致步长越来越小 ，容易困在局部极值点。因此一般不使用AdaGrad，而用改进的方法： 改进：RMSprop 在累加平方项的过程中，同时使得每次累加的值不断减小，这样就保证了步长不会随着训练而减小过多： \[S^{new}=\rho S+(1-\rho)(\nabla f(x))^2,w^{new}=w-\alpha \frac{\nabla f(x)}{S^{new}}\] 其中\(\rho\)是衰减率，使得累加项的增幅逐渐减小。 Adam算法（动量+RMSProp） 结合了二者的优点，在每次迭代时计算出动量项和梯度平方的累加项，并综合二者信息进行权重更新。 对参数\(W\)，初始化：\(v = 0, S=0\) 每第\(t\)轮iteration，更新\(v, S\)：\(v = \beta_1 v + (1-\beta_1)dW, S = \beta_2S + (1-\beta_2)dW^2\) 修正\(v, S\)：\(v&#39; = \frac{v}{1-\beta_1^t}, S&#39; = \frac{S}{1-\beta_2^t}\) 更新参数\(W\)：\(W = W - \alpha\frac{v&#39;}{\sqrt{S&#39;}}\) 二阶优化 1.牛顿法 假设loss函数\(E(x)\)是一个\(N\)维的实值函数（从\(N\)维空间到实数的映射），把求该函数的极值点的问题，转化为求\(E&#39;(x)=0\)的问题。在每次迭代中，使用二次函数拟合损失函数（即二项泰勒展开），找到展开后的二次函数的极值点，直接将其当做新的权重，由此不断进行优化。 在实际计算中，假设在\(x_0\)点处进行迭代，\(E&#39;(x)\)是\(E(x)\)关于\(x\)每一个维度求导数，是\(n\times 1\)的向量。\(E&#39;&#39;(x)\)是\(E(x)\)关于\(x\)任意两个维度求导数，因此\(E&#39;&#39;(x)\)是\(n\times n\)的矩阵，称为海森矩阵（设为\(H(x)\)）。泰勒展开并解方程，可以得到在\(x_0\)处牛顿法更新的表达式： \[x=x_0-H^{-1}(x_0)E&#39;(x_0)\] 其中令\(-H^{-1}(x_0)E&#39;(x_0)=d\)称为牛顿法的迭代方向。 2.拟牛顿法 由于牛顿法每一次迭代就要计算二阶导数并求逆，计算较为复杂，因此使用一个近似的矩阵\(B\)和\(D\)来逼近\(H\)和\(H^{-1}\)。 要拟合海森矩阵，就要知道它满足的条件，并以此作为依据来进行拟合。假设在第\(k\)次迭代过程中的\(x\)值为\(x_k\)，那么将\(x\)在\(x_{k+1}\)处泰勒展开，并对等式两边应用哈密尔顿算子（即\(\nabla\)），可以得到关于\(H\)的方程，将\(x_k\)代入方程（即把\(f(x_k)\)在\(x_{k+1}\)处展开），可以得到拟牛顿法需要满足的条件： \[x_{k+1}-x_k=H_{k+1}^{-1}(\nabla f(x_{k+1})-\nabla f(x_k))\] 只要根据这个条件迭代更新矩阵\(B,D\)，就可以认为最终得到的结果与\(H,H^{-1}\)在性质上近似。为了消除牛顿法收敛不稳定的特性，常常采用阻尼牛顿法： 在迭代方向\(d\)上寻找一个最优的步长\(\lambda\)进行迭代。即： \[\lambda=arg\min_{\lambda \in \Bbb R}f(x+\lambda d)\] 常用的拟牛顿法有： DFP算法：\(B,D\)初始化为单位矩阵，通过待定法推导得出迭代公式： \[D_{k+1}=D_k+\frac{s_ks_k^T}{s_k^Ty_k}-\frac{D_ky_ky_k^TD_k}{y_k^TD_ky_k}\] 其中\(s_k=\lambda_k d_k,y_k=\nabla f(x_{k+1})-\nabla f(x_k)\) BFGS算法：思路与DFG算法大致相同，只是\(s_k,y_k\)的位置呼互换，推导过程相似。得出迭代表达式： \[D_{k+1}=(I-\frac{s_ky_k^T}{y_k^Ts_k})D_k(I-\frac{y_ks_k^T}{y_k^Ts_k})+\frac{s_ks_k^T}{y_k^Ts_k}\] 其中\(y_k,s_k\)与之前相同。 L-BFGS算法：由于存储\(D_k\)的开销巨大，因此存储m组\(s,y\)，即\((s_k,y_k),(s_{k-1},y_{k-1}),...(s_{k+1-m},y_{k+1-m})\)，并用它们来表示\(D_k\)。但是由于用到多组\(s,y\)，使得\(D_k\)的表达式十分复杂，因此使用快速计算\(D_k\nabla f(x_k)\)的算法，算法详情见论文：Updating Quasi-Newton Matrices with Limited Storage 4.抗过拟合 正则化 在损失函数后加入一项正则项，作用是作为“惩罚项”，抑制了模型参数的复杂度。常用的有\(L_1\)正则化和\(L_2\)正则化： \(L_1\)正则项：所有参数的绝对值之和，可以表示为参数向量\(W\)的\(L_1\)范数：\(\lambda||W|| _1 = \lambda \sum_{i=1}^n|w_i|\) \(L_2\)正则化：表示为参数向量\(W\)的\(L_2\)范数：\(\lambda||W||_2^2 = \lambda \sum_{i=1}^n w_i^2\)。\(L_2\)正则具有权重衰减的作用，因为正则项的存在，\(W\)的更新公式中会额外减去由于该项带来的梯度，使得W趋向更小。 为什么可以防止过拟合？和dropout有些类似。 \(W\)被设置为趋近0的值，使得一些神经元相当于不起作用了（或者作用变小了），相当于集成学习 很多cell不起作用，网络的复杂度（拟合能力）下降了，而过拟合恰恰就是网络“学得太好”，所以这样可以防止过拟合。 对于tanh这种0附近接近线性的激活函数，权重衰减导致激活值落在线性区，抑制了过拟合。 集成学习 在不同训练集上训练不同的模型，最后对多个模型集成，得到一个平均的模型。（不同模型超参数有时也会不同） Dropout 神经网络每次正向传播经过某一层时，随机将该层中的某些神经元的激活函数置0（相当于暂时丢弃这些神经元），要注意每次传播经过每一层丢弃的神经元都时随机的，并且置零的神经元数目根据dropout的概率\(p\)决定。一般在全连接层进行dropout。另一种解释是，dropout相当于在一个网络中集成学习。 进行测试时，每个神经元表示为\(y=f(w,x)\)，假设输入\(x\)由\(n\)个神经元的输出组成，经过dropout之后，该神经元的输入\(x\)中的\(n\)项有\(m\)项被丢弃了，由于在训练中这些项是被随机丢弃的（假设每个神经元被丢弃的概率为\(p\)），因此在训练时需要记录下被丢弃的神经元，并计算出该神经元\(y\)的输出的期望值，在测试时使用期望值代替实际值（例如，假设每个神经元都有\(p=0.5\)的概率被丢弃，那么\(y=0.5\sum_{i=1}^nw_ix_i\)）。另一种方法是，在训练时除以概率\(p\)，测试过程保持不变，可以达到相同的效果(反转dropout)，而且实现起来更简单。 使用dropout会导致训练时间变长，但是训练后的鲁棒性更好。 随机扰动 dropout和BN使用的其实都是这种思想。其他使用随机性方法的例子还有： 对图片随机裁剪，翻转 使用色彩抖动，例如随机改变对比度和亮度 类似dropout，随机将网络中的一些权重设置为0，相当于暂时切断部分神经元之间的连接。 部分最大池化：随机池化部分区域 随机深度：在训练时随机丢弃一些层，在测试时使用全部层。 5. 偏差与方差 假设训练集和验证集独立同分布，那么模型在二者表现相近（loss相近），表面方差小；相差较大说明方差大（过拟合） 如果模型在二者上的表现都较差（error高），说明偏差大 如果在训练集上表现本身就很差，还过拟合，说明偏差和方差都大。 6.各种熵 \(H(X) = -\sum_i p(x_i)log(p(x_i))\) 自信息\(I(x) = -log(p(x))\)。可见，熵就是自信息的期望值。 相对熵\(KL(P||Q) = \sum_i p(x_i)log\frac{p(x_i)}{q(x_i)}\) \(CE(P, Q) = -\sum_i p(x_i)log(q(x_i))\) 注意到，有\(KL(P||Q) = -H(P) + CE(P, Q)\)，而前一项表示真实分布的熵，在训练中理论上是不变的，因此往往只使用后半部分，即交叉熵作为loss。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（四）使用支持向量机！]]></title>
    <url>%2F2018%2F05%2F28%2FAI%E9%85%B1%E5%85%BB%E6%88%90%E8%AE%A1%E5%88%92%EF%BC%88%E5%9B%9B%EF%BC%89%E4%BD%BF%E7%94%A8%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%81%2F</url>
    <content type="text"><![CDATA[也许是现成的最好的分类器 #4 支持向量机（SVM） \(\mathrm{Warning:}\) 以下内容包含大量数学公式，若有不解之处，请反复阅读。 线性可分问题 要了解SVM的概念，让我们先从 线性可分 的问题谈起。什么是线性可分问题呢？以二分类问题为例，当一个分类器对数据集\(\Bbb D=(x_1,y_1),(x_2,y_2),...(x_n,y_n)\)进行分类时，假设其中任意样本\((x_i,y_i)\)的特征向量 \(x_i\) 都是 \(m\) 维向量，那么数据集 \(\Bbb D\) 的特征就可以表示为 \(m\) 维向量空间中的一组点集。进行分类的过程其实就是找到一个 *超平面** ，可以把表示两种不同类别数据的点划分成相应的两部分 \(\{C_{+1},C_{-1}\}\) 。假如我们找到的超平面是线性的（例如二维空间的直线，三维空间的平面），那么这个分类问题就是线性可分问题 事实上，对一个线性可分的数据集而言，这样的线性超平面不止一个。以上图中的数据为例，即使图中的直线略微左右倾斜，仍然可以正确地划分数据集。在这些不同的直线中，我们需要找的就是一条最优的直线\(L&#39;\)。 所谓“最优”，指的是如果我我们向数据集中增添新的数据，直线仍然能很好的划分出两个类别，这就要求 $L’ $ 正好处在两个类别的数据的“正中间”，换句话说，就是要求 $ { C_{+1},C_{-1}} $ 中最靠近边界的那些点（这些点称作 支持向量点 ）距离 \(L&#39;\) 最近，也即间隔最大。因此最优超平面，指的也就是 最大间隔超平面 。这就将求解 \(L&#39;\) 的问题转化为一个极值问题。 求解最大间隔超平面，就是支持向量机（SVM）要解决的核心问题。 求解最大分隔超平面 函数间隔与几何间隔 假设我们要求的超平面为\(L:f(x)=w^Tx+b\)（要注意这里的\(x\)可能是高维向量，这取决于\(m\)的值），其中\(b\)是截距，\(w\)是参数向量，且\(w\)的方向是超平面的法矢量方向，\(x\)表示点坐标（也即是特征值向量）。那么\(\Bbb D\)中的任一点\(x_i\)到\(L\)的距离为: \[d_i=\frac{|w^Tx_i+b|}{||w||}\] 由于是二分类问题，我们使用\(+1\)和\(-1\)来标记样本的正反类。如果分类器能够正确分类的话，对样本\((x_i,y_i)\)，有: 对于\(y_i=+1\)，有\({w^Tx_i+b}&gt;0\) 对于\(y_i=-1\)，有\({w^Tx_i+b}&lt;0\) 也就是说，无论\(y\)的取值如何，分类器能够正确分类的充要条件是： \[y_if(x_i)=y_i(w^Tx_i+b)\geqslant0 \qquad(1)\] 令\(\gamma_i=y_if(x_i)\)，\(\gamma_i\)称为点\(x_i\)到\(L\)的函数间隔。由于\(|y|=1\)，因此\(\gamma_i=|w^Tx_i+b|\)，进而得出： \[d_i=\frac{\gamma_i}{||w||}\] （这也是为什么要把\(y\)设定为\(1\)和\(-1\)，而不是像逻辑回归中一样设置为\(0\)和\(1\)的原因。） 这样就完成了表示距离的工作，接下来考虑求解超平面\(L\)的最优解\(L&#39;\)。 拉格朗日算子法 还记得我们的目的吗？我们要求的是到支持向量点的距离最近的超平面\(L&#39;\)。首先我们要表示出支持向量到\(L\)的距离。假设支持向量到\(L\)的函数间隔为\(\gamma_v\)，那么要求解的极值问题就是： \[\max_{w,b} \frac{\gamma_v}{||w||},\qquad(2.1)\] \[ s.t.\;\ y_i(w^Tx_i+b)\geqslant\gamma_v,\; (x_i,y_i)\in\Bbb D \qquad(3.1)\] 其中式\((2.1)\)是目标函数，式\((3.1)\)是限制条件。式\((2.1)\)的由来是：由于\(\gamma_v\)是支持向量到\(L\)的函数间隔，应当是所有点到\(L\)的函数间隔中最小的，因此任一点\(x_i\)到\(L\)的函数间隔都应当满足： \[y_i(w^Tx_i+b)\geqslant\gamma_v\] 由于\(\gamma_v\geqslant0\)，因此式\((3.1)\)同时也隐含了条件\((1)\)，也即保证了我们求得的\(L&#39;\)是一个正确的分类器。 接下来对该极值问题进行简化： \(w\)是最终要求的变量，这里有\(\max \limits_{w,b}\frac{\gamma_v}{||w||}\iff \min \limits_{w,b}\frac{1}{2}{\gamma_v}||w||^2\) 由于\(\gamma_v\)的取值并不影响最终求到的 \(L&#39;\) ，因此令\(\gamma_v=1\)。 （这一条可以试着自己证明一下呦！才不是因为我懒哼） 得到简化后的极值问题： \[\min_{w,b}\frac{1}{2}||w||^2,\qquad(2.2)\] \[ s.t.\;\ y_i(w^Tx_i+b)\geqslant1,\; (x_i,y_i)\in\Bbb D \qquad(3.2)\] 这是一个凸二次优化问题。为了求解这类极值问题，我们可以使用拉格朗日算子法。构造拉格朗日函数： \[\mathcal L(w,b,\alpha)=\frac12||w||^2+\sum_{i=1}^n \alpha_i(1-y_i(w^Tx_i+b))\] 其中\([\alpha_1,\alpha_2,...\alpha_n]\)是拉格朗日算子，这里我们令\(\alpha=[\alpha_1,\alpha_2,...\alpha_n]\)。拉格朗日算子法在这里有以下等式关系： \[ \begin{cases} \frac{\partial \mathcal L}{\partial w}=0 \\[2ex] \frac{\partial \mathcal L}{\partial b}=0 \end{cases} \;\Rightarrow\; \begin{cases} w=\sum_{i=1}^n\alpha_ix_iy_i \\[2ex] 0=\sum_{i=1}^n\alpha_iy_i \end{cases} \qquad(4.1) \] 根据式\((4.1)\)，可以消去\(w\)得到\(L:f(x)=\sum_{i=1}^n\alpha_i y_i\left \langle x_i,x\right\rangle+b\)，其中\(\left \langle x_i,x\right\rangle\)表示向量\(x\)与\(x_i\)的内积。这个表达式与之前的\(L\)表达式完全等价，之后就不必再考虑\(w\)，只要求出最优解对应的\(\alpha\)，就可以根据\((4.1)\)计算得到\(w\)。另一方面，注意到式\((3.2)\)是不等式约束关系。在拉格朗日算子法的使用中，出现不等式约束关系时，要求必须要满足KKT条件(Karush-Kuhn-Tucker)。在这里，对应的KKT条件是： \[ \begin{cases} \alpha_i\geqslant0,\\[2ex] y_if(x_i)-1\geqslant0,\qquad\qquad(5.1)\\[2ex] \alpha_i(y_if(x_i)-1)=0. \end{cases} \] 可以看到，对于不同取值的\(\alpha_i\)，要满足的KKT条件也不同。因此对于不同取值的\(\alpha_i\)，根据式\((5.1)\)可以得到： \[ \begin{cases} y_if(x_i)\geqslant1,\qquad\text{if }\;\alpha_i=0 \\ y_if(x_i)=1,\qquad\text{if }\;\alpha_i\gt0 \end{cases} \qquad(5.2) \] 可以看到，当\(\alpha_i&gt;0\)时，必有\(\gamma_i=1\)，也即对应的\(x_i\)必然是支持向量。事实上，\(L&#39;\)的取值也仅与支持向量有关，其他数据的分布并不影响超平面的选取（这也是SVM的好处之一）。到这里我们就初步得到了求解\(L&#39;\)所需要的条件，包括式\((2.2),(3.2),(4),(5.2)\)。接下来只要根据这些条件求解得到\(w\)和\(b\)，就可以得到\(L&#39;\)的方程。稍后，我们将对这个问题进行进一步优化，然后给出求解的详细步骤。 软间隔与松弛变量 注意到，上述数学模型假设最优超平面\(L&#39;\)必定可以准确无误地把 \(\{C_{+1},C_{-1}\}\) 分隔开，但事实上，我们使用的训练数据中往往会有一些反常样例，这种情况下要求\(L&#39;\)把所有的数据都准确地分出来是不现实的，这么强求反而可能会导致过拟合之类的问题。因此，我们可以允许少数样本点落在\(L&#39;\)不属于它所在类别的另一侧。 为此，引入软间隔（或者称为“松弛变量”）的概念。所谓软间隔，也就是不太严格的分类器，它允许少数样本不满足约束条件\((3.2):y_i(w^Tx_i+b)\geqslant1\)。为了达到这一目的，对每一个样本点\((x_i,y_i)\)都引入松弛变量\(\xi_i \geqslant0\)，使得对\((x_i,y_i)\)的约束条件变为\(y_i(w^Tx_i+b)\geqslant1-\xi_i\)，此时\(\xi_i\)可以反映样本\((x_i,y_i)\)允许偏离\(L&#39;\)的程度。这样对于那些靠近\(L&#39;\)的在危险的边缘试探的点，分类器就有了容错的空间。 当然，这些不满足约束条件的点也应当尽可能少。因此，我们在最小化目标函数\(\frac{1}{2}||w||^2\)时，也应当同时最小化\(\xi\)的值。为此，在目标函数中添加一项来描述各个样本的\(\xi\)的总和，目标函数和约束条件就变为： \[\min_{w,b}\frac{1}{2}||w||^2+C\sum_{i=1}^n\xi_i,\qquad(2.3)\] \[ s.t.\;\ y_i(w^Tx_i+b)\geqslant1-\xi_i,\; (x_i,y_i)\in\Bbb D,\xi_i\geqslant0 \qquad(3.3)\] 其中\(C\)是一个事先确定好的超参数，用于控制目标函数中的两项(“寻找间隔最大超平面”和“保证数据点偏差量最小”)之间的权重。由于目标函数和约束条件发生了变化，因此构造新的拉格朗日函数： \[\mathcal L(w,b,\alpha,\mu)=\frac12||w||^2+\sum_{i=1}^n \alpha_i(1-y_i(w^Tx_i+b))-\sum_{i=1}^n\mu_i\xi_i\] 分别各个变量求偏导数，得到： \[ \begin{cases} \frac{\partial \mathcal L}{\partial w}=0\\[2ex] \frac{\partial \mathcal L}{\partial b}=0\\[2ex] \frac{\partial \mathcal L}{\partial \xi}=0 \end{cases} \;\Rightarrow\; \begin{cases} w=\sum_{i=1}^n\alpha_ix_iy_i\\[2ex] 0=\sum_{i=1}^n\alpha_iy_i\\[2ex] C=\alpha_i+\mu_i \end{cases} \qquad(4.2) \] 同样的，由于约束条件是不等式，因此要满足KKT条件。此处的KKT条件为： \[ \begin{cases} \alpha_i\geqslant0,\;\mu_i\geqslant0,\\[2ex] y_if(x_i)-1+\xi_i\geqslant0,\\[2ex] \alpha_i(y_if(x_i)-1+\xi_i)=0,\\[2ex] \xi_i\geqslant0,\; \mu_i\xi_i=0. \end{cases} \qquad(5.3) \] 根据式\((5.3)\)可以发现，当\(\alpha_i\)的取值不同时，要满足的条件也不同。与之前类似，不同\(\alpha\)的取值下，要满足的条件分别为： \[ \begin{cases} y_if(x_i)\geqslant1,\qquad\text{if }\;\alpha_i=0 \\ y_if(x_i)=1,\qquad\text{if }\;0&lt;\alpha_i&lt;C \\ y_if(x_i)\leqslant1,\qquad\text{if }\;\alpha_i&gt;C \end{cases} \qquad(5.4) \] \(\alpha\)在不同条件下，相应的\(x_i,y_i\)要满足不同的关系。从这里也可以看到，\(\alpha_i\)和\((x_i,y_i)\)是一一对应的。 核函数 映射到高维空间 还记得我们一直讨论的是线性可分问题吗？之前的内容都是针对线性可分问题而言的。但是在实际的数据处理中，数据分布往往不是线性可分的（例如典型的 “异或问题” ）。这种情况下我们无法通过一个线性超平面来分隔数据。 对于线性不可分问题，不同的机器学习算法有不同的处理方法。而SVM的处理方法是：使用一个映射函数\(\phi\)，把原数据映射到某一个更高维的特征空间，使得在这个空间中，数据变得线性可分。（事实上，必定存在某个维度，使得映射后的数据线性可分。）以下图为例，一组线性不可分的二维数据经过某种变换\(\phi\)映射到三维空间，在新的特征空间中，数据变得线性可分。之后，只要在新的特征空间中列出相应的拉格朗日方程，求解极值问题即可得到\(L&#39;\)。 我们之前在极值问题中得到过消去\(w\)，使用\(\alpha\)表达的\(L\)的方程：\(f(x)=\sum_{i=1}^n\alpha_i y_i\left \langle x_i,x\right\rangle+b\)。在数据映射到高维空间后，方程变为： \[f(x)=\sum_{i=1}^n\alpha_i y_i\left \langle \phi x_i,\phi x\right\rangle+b\qquad(6)\] 其中\(\phi x_i,\phi x\)是映射后的特征向量和自变量。可以看到，其中涉及到了\(\phi x_i\)和\(\phi x\)内积的计算，也即是矩阵的乘法计算。这就面临一个问题：如果映射之后的特征空间维度很高，那么进行矩阵计算将会相当耗时。为了解决这一问题，我们使用核函数的方法。 核方法 核方法的思想是：为了避免在高维空间中进行矩阵运算，那么在原空间中是否可以找到这么一个函数\(k(x_i,x)\)，使得\(k(x_i,x)\)恰好等于高维空间中\(\left \langle x_i,x\right\rangle\)的值呢？如果函数\(k(\dot\;,\dot\;)\)存在的话，就可以直接在低维空间中计算内积，不必计算高维向量的乘法了。这样的函数\(k\)就称为核函数。 \(k\)的确是存在的，而且还不止一种。假设有核函数\(k(\dot\;,\dot\;)\)，那么定义核矩阵\(K\)： \[ K= \begin{pmatrix} k(x_1,x_1) &amp; \cdots &amp; k(x_1,x_n)\\[2ex] \vdots &amp; \ddots &amp; \vdots \\ k(x_j,x_1) &amp; \cdots &amp; k(x_j,x_n) \\[2ex] \vdots &amp; \ddots &amp; \vdots \\ k(x_n,x_1) &amp; \cdots &amp; k(x_n,x_n) \\ \end{pmatrix} \] 可以证明，如果对于任意的数据集\(D\)而言，\(K\)都是半正定矩阵，那么\(k(\dot\;,\dot\;)\)就可以作为核函数。此外，两个核函数的线性组合、两个核函数的直积也是核函数。下面给出一些常用的核函数： 函数名称 表达式 参数 线性核 \(k(x,y)=x^Ty\) \(None\) 多项式核 \(k(x,y)=(x^Ty)^d\) \(d\geqslant1\)，是多项式的次数 高斯核 \(k(x,y)=exp(-\frac{\|x-y\|^2}{2\sigma^2})\) \(\sigma&gt;0\)，是高斯核的带宽 拉普拉斯核 \(k(x,y)=exp(-\frac{\|x-y\|}{\sigma})\) \(\sigma&gt;0\) Sigmoid核 \(k(x,y)=tanh(\beta x^Ty+\theta)\) \(tanh:\)双曲正切函数，\(\beta&gt;0,0&gt;\theta\) 上述函数都满足核函数的条件，也即\(k(x_i,x)=\left \langle x_i,x\right\rangle\)。使用核函数的好处在于，当我们在低维空间中使用特定的核函数\(k\)时，就已经隐式地把数据映射到了某一个高维空间中去了，不必再进行复杂的映射变换，也无需关注映射后的数据是怎样分布的。如果使用不同的核函数，就可以把数据映射到不同的高维特征空间。因此，核函数的选取也就成为了影响SVM性能的一个重要因素。 进行核函数映射之后，极值问题中所有涉及到向量内积的计算，就可以全部用核函数代替（换言之，只要用核函数代替内积，就相当于完成了高维映射），接下来，就可以进行对极值问题的具体求解了。 训练模型：序列最小化优化算法(SMO) 对偶问题 接下来，我们就要对这个复杂的极值问题就行求解了。首先再次梳理该极值问题的条件。我们要求的目标函数和约束条件分别为： \[\min_{w,b}\frac{1}{2}||w||^2+C\sum_{i=1}^n\xi_i,\qquad(2.3)\] \[ s.t.\;\ y_i(w^Tx_i+b)\geqslant1-\xi_i,\; (x_i,y_i)\in\Bbb D,\xi_i\geqslant0 \qquad(3.3)\] 由该问题构造拉格朗日函数并对各项求偏导数，以及求出相应的KKT条件，再进行相应化简和推导，这些条件综合起来，得到的是式\((4.2),(5.4)\)： \[ \begin{cases} w=\sum_{i=1}^n\alpha_ix_iy_i\\ 0=\sum_{i=1}^n\alpha_iy_i\\ C=\alpha_i+\mu_i \end{cases}(4.2)\quad\&amp;\quad \begin{cases} y_if(x_i)\geqslant1,\quad\text{if }\;\alpha_i=0 \\ y_if(x_i)=1,\quad\text{if }\;0&lt;\alpha_i&lt;C \\ y_if(x_i)\leqslant1,\quad\text{if }\;\alpha_i&gt;C \end{cases} \quad(5.4) \] 并且由于引入了核函数，消去\(w\)，得到用\(\alpha\)表示的超平面\(L\)表达式为： \[f(x)=\sum_{i=1}^n\alpha_i y_ik(x_i,x)+b\qquad(6)\] 这就是我们得到的所有条件。将式\((4.2)\)代入\((2.3)\)，可以消去\(w,b\),得到该极值问题的对偶问题： \[\max_{\alpha}\sum_{i=1}^n\alpha_i-\frac12 \sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_jk(x_i,x_j),\qquad(2.4)\] \[ s.t.\sum_{i=1}^n\alpha_iy_i=0,\; (x_i,y_i)\in\Bbb D,\;0\leqslant\alpha\leqslant C \qquad(3.4)\] 该问题和原来的极值问题完全等价。同时，根据已有条件也可以得到\(w\)和\(b\)关于\(\alpha\)的表达式： \[ \begin{cases} w=\sum_{i=1}^n\alpha_ix_iy_i \\[2ex] b=\frac{1}{|\Bbb S|}\sum_{i=1}^{|\Bbb S|}\left( y_i-\sum_{i=1}^{|\Bbb S|}\alpha_iy_ik(x_i,x_j)\right) \end{cases} \qquad(7) \] 其中，\(\Bbb S\)表示所有支持向量的集合。\(b\)表达式的由来是：对任意支持变量，都有\(y_if(x_i)=1\)成立，因此根据该式求出一个\(b_i\)，然后对所有支持变量求得的\(b_i\)求和并取平均值。可以看到，在对偶问题中，只有向量\(\alpha\)一个变量，因此只要求出\(\alpha\)，就可以根据式\((7)\)得到\(w\)和\(b\)，进而求出最优超平面\(L&#39;\)。 求解\(\alpha\) 接下来我们就要开始求解对偶问题\((2.4),(3.4)\)了。事实上，这个问题是一个凸二次规划问题，有许多现成的算法库可以调用。但是，直接求解这个问题会十分复杂——我们使用的训练数据量有时候会很大。因此我们需要一种简便的优化算法来求\(\alpha\)的值，这个算法就是序列最小化优化算法(SMO)（其实梯度下降法也可以呦(。・・)ノ）。 SMO算法的思想是，不直接去求解这个二次规划问题，而是先将向量\(\alpha\)初始化，然后然后再通过不断迭代，使得目标函数的值不断增大，最终逼近最大值。每一次迭代中，都只把\(\alpha\)的其中两个元素\(\alpha_i,\alpha_j\)视作变量，其他的\(\alpha\)视作常数，只优化这两个\(\alpha\)。具体的迭代步骤是： 首先初始化\(\alpha=[\alpha_1,\alpha_2...\alpha_n]=[0,0,0...0]\) 在\(\alpha\)的\(n\)个元素中一选择一个\(\alpha_i\)作为第一个迭代对象（要怎么选呢？稍后会谈到） 在剩下的\(n-1\)个元素中选择一个\(\alpha_j\)作为第二个迭代对象（要怎么选呢？稍后会谈到） 暂且不管其他\(\alpha\)，根据已知条件求解得到\(\alpha_i,\alpha_j\)的值 选取两个新的\(\alpha\)继续进行迭代，直到所有\(\alpha\)都更新完毕 SMO算法看起来似乎比直接求解优化问题要更麻烦——AI酱可不这么认为o(￣ヘ￣o＃)！事实上SMO算法得益于可以快速收敛的优点，成为了SVM中的经典算法。SMO算法每次只选择两个\(\alpha\)进行更新，因此将多变量的优化问题变成了单变量优化问题（没错，虽然有两个\(\alpha\)但它的确是个单变量问题！稍后你就会看到这两个\(\alpha\)之间是有等式关系的），这样极大降低了优化问题的计算难度。SMO算法只要迭代进行单变量二次优化问题的求解，直到满足停止条件，就可以得到所有的\(\alpha\)值。 接下来以其中一次迭代为例，给出求\(\alpha_i\)和\(\alpha_j\)的步骤： 启发式方法 首先你会好奇，为什么每次要选择两个变量\(\alpha\)进行优化呢？一个不阔以吗？是的当然不阔以ヽ（≧^≦）ノ！我们要求优化之后的\(\alpha\)都满足约束条件\((4.2),(5.4)\)，其中有一条： \[0=\sum_{i=1}^n\alpha_iy_i\] 如果每次都只针对一个\(\alpha\)进行优化的话，无论如何也无法保证每次选取的\(\alpha\)之间都满足这个条件。因此，每次选择两个\(\alpha\)（假设用\(\alpha_1\)表示第一个优化对象，\(\alpha_2\)表示第二个优化对象），只要保证\(y_1\alpha_1+y_2\alpha=\varsigma\)，其中\(\varsigma\)是一个常数，就可以使得\(0=\sum_{i=1}^n\alpha_iy_i\)成立。 那么，在每一次迭代之前，要选择哪两个变量作为\(\alpha_1,\alpha_2\)呢？我们当然可以随机地进行选择，但我们总希望先选出当前情况下使得优化之后效果最好的两个变量，为此，我们采用启发式的选取方法。 首先回忆一下 支持向量的概念。之前曾经提到，超平面\(L&#39;\)的表达式事实上只和支持向量有关。因此，优化支持向量\(x_i\)对应的\(\alpha_i\)带来的收益更大；而如果优化其他点对应的\(\alpha\)，带来的收益就小很多。我们知道，对支持变量\(x_i\)，有\(y_if(x_i)=1\)，而根据式\((5.4)\)，有： \[y_if(x_i)=1,\quad\text{if }\;0&lt;\alpha_i&lt;C\] 也就是说，当\(\alpha\)的取值在\((0,C)\)之间时，对应的数据点是支持向量。因此，对于\(\alpha_1,\alpha_2\)的选取，都应当优先在\((0,C)\)中选取，如果找不到\((0,C)\)中的\(\alpha\)，再从其他区间选择。 对于\(\alpha_1\)和\(\alpha_2\)的选择，也要使用不同的方法。接下来进行分别讨论： \(\alpha_1\)的选择 我们为所有的\(\alpha\)都赋予了初值\(0\)，SMO算法的目的是通过优化\(\alpha\)使得最终所有的\(\alpha\)都满足约束条件，并且在满足约束条件的前提下，使得目标函数最大。因此，我们只需要选择违背约束条件最严重的变量\(\alpha_i\)作为\(\alpha_1\)，并使得它在优化之后满足约束条件，那么就可以认为，优化这个\(\alpha_i\)的效果最好，或者说…优化程度最高（这有些类似梯度下降的做法）。 \(\alpha\)要满足的条件之前已经给出了(式\((4.2,(5.4)\))，其中： \((4.2)\)是优化过程中要用到的条件，以及\(w\)和\(\alpha\)的关系，在这里无法作为判断依据； \((5.4)\)是KKT条件，是关于\(\alpha\)的不等式约束条件，可以作为判断依据。因此，我们要选取的就是违反KKT条件最严重的变量\(\alpha\)作为\(\alpha_1\)。 衡量\(\alpha\)违反KKT条件的程度也很简单，例如对于\(\alpha_i\in(0,C)\)，要求\(y_if(x_i)=1\)，因此只要使得\(y_if(x_i)\)和\(1\)之间的距离最大就可以啦。据此，得出\(\alpha_1\)的选取方法： \[ \alpha_1= \begin{cases} \mathop{argmax}_{\alpha_i} \;1-y_if(x_i),\quad\text{if }\;\alpha_i=0 \\ \mathop{argmax}_{\alpha_i} \;|y_if(x_i)-1|,\quad\text{if }\;0&lt;\alpha_i&lt;C \\ \mathop{argmax}_{\alpha_i} \;y_if(x_i)-1,\quad\text{if }\;\alpha_i&gt;C \end{cases} \qquad(8) \] 在这三种情况中，我们又比较优先选择情况2（因为恰好是支持向量对应的\(\alpha\)），这样，就可以选出每一轮迭代的\(\alpha_1\)。 \(\alpha_2\)的选择 选取得到\(\alpha_1\)之后，根据\(\alpha_1\)来选取\(\alpha_2\)。因为要优化的\(\alpha_1\)已经固定，我们只要选择使得优化收益最大的\(\alpha_2\)（也即使得目标函数上升最快的一个\(\alpha\)）就可以了。此时，这个问题就变成了一个单变量优化问题。 稍后我们进行的具体优化步骤会保证我们向着使得目标函数增长最快的方向优化\(\alpha_1\)和\(\alpha_2\)，因此选取\(\alpha_2\)时不必担心优化后目标函数不增反降，只要使得优化之后\(\alpha_1\)和\(\alpha_2\)的改变尽可能大就可以了。因此引入一个判断标准\(E\)。对样本\((x_i,y_i)\): \[E_i=\underbrace{\sum_{i=1}^n\alpha_i y_ik(x_i,x_i)+b}_{f(x_i)}-y_i\] 当\(\alpha_j\)对应的\(|E_1-E_j|\)最大时，就能使得优化之后的变量\(\alpha\)改变最大，此时\(\alpha_j\)就是我们要选择的\(\alpha_2\)。也即： \[\alpha_2=argmax_{\alpha_j}\; |E_1-E_j|\] 这样就完成了待优化参数\(\alpha\)的选择。选择\(\alpha\)的python代码如下所示： 1234567891011121314151617181920212223242526272829def chooseAlpha(trainData, trainLabel, b, alpha, index): E = 0 num = len(index) predLst = [pred(trainData[x], b, alpha, kernel, trainData, trainLabel) for x in index] for x in range(num): ix = index[x] if alpha[ix] &gt; 0 and alpha[ix] &lt; C: preE = E E = max(E, np.abs(1-predLst[x]*trainLabel[ix])) if preE != E: i = x if E == 0: for x in range(num): ix = index[x] preE = E if alpha[ix] == 0: E = max(E, 1-predLst[x]*trainLabel[ix]) elif alpha[ix] == C: E = max(E, predLst[x]*trainLabel[ix]-1) if preE != E: i = x if E == 0: i = 0 E = [predLst[x]-trainLabel[index[x]] for x in range(num)] j = np.argmax(np.abs(np.array(E) - E[i])) i = index[i] j = index[j] return i, j 优化\(\alpha_i\)和\(\alpha_j\) 接下来就可以对\(\alpha_i\)和\(\alpha_j\)进行优化了。首先根据条件\((4.2)\)，得到\(\alpha_i,\alpha_j\)之间的关系式： \[y_1\alpha_1^{old}+y_2\alpha_2^{old}=y_1\alpha_1^{new}+y_2\alpha_2^{new}=\varsigma\qquad(9.1)\] 其中\(\alpha_1^{old}\)和\(\alpha_2^{old}\)已知，要求的是\(\alpha_1^{new},\alpha_2^{new}\)，我们就可以将\(\alpha_1^{new}\)用\(\alpha_2^{new}\)表示（当然反过来也是可以的），只要求出\(\alpha_2^{new}\)就解决了对\(\alpha\)的优化问题。由于\(y\)的值只能是\(\pm 1\)，所以根据\(y_1,y_2\)的正负情况，上述条件变成： \[ \begin{cases} \alpha_1^{new}=-\alpha_2^{new}+\alpha_1^{old}+\alpha_2^{old} ,\quad if\ y_1=y_2\\ \alpha_1^{new}=\alpha_2^{new}+\alpha_1^{old}-\alpha_2^{old} ,\quad if\ y_1\ne y_2 \end{cases} \qquad(9.2) \] 可以认为\(\alpha_1^{new}\)和\(\alpha_2^{new}\)是一次函数关系。对应上述两种情况，\(\alpha_2^{new}\)与\(\alpha_1^{new}\)之间的函数关系可以用图形表示为： 上图中的\(\alpha\)之所以落在\([0,C]\)的盒子内，是因为我们之前选取的\(\alpha_1,\alpha_2\)都是优先从\([0,C]\)区间内选取的。根据图像可以看出，由于\(\alpha_1^{new},\alpha_2^{new}\)之间的约束关系\((9.2)\)，使得\(\alpha_2^{new}\)是有上下界的。假设\(\alpha_2^{new}\)的上下界分别为\(H,L\)，根据图像有： \[ \begin{cases} L=max(0,\alpha_2^{old}-\alpha_1^{old}),H=min(C,C+\alpha_2^{old}-\alpha_1^{old})\quad if\ y_1\ne y_2\\ L=max(0,\alpha_2^{old}+\alpha_1^{old}-C),H=min(C,\alpha_2^{old}+\alpha_1^{old})\quad if\ y_1=y_2\\ \end{cases} \] 因此，稍后我们求出\(\alpha_2^{new}\)之后，还要对其进行 剪切：如果\(\alpha_2^{new}\)的值不在\([L,H]\)之间，需要把多余部分剪掉，即： \[ \alpha_2^{new}= \begin{cases} H\qquad if\ \alpha_2^{new}&gt;H \\ \alpha_2^{new} \quad if\ \alpha_2^{new}\in [L,H]\\ L \qquad if\ \alpha_2^{new}&lt;L \end{cases} \] 这样得到的就是最终的\(\alpha_2^{new}\)。 求解\(\alpha_2^{new},\alpha_1^{new}\) 此刻，我们已经将最初的优化问题变成了仅与\(\alpha_2^{new}\)有关的单变量优化问题。事实上，这个优化问题就是一个简单的二次函数求最值问题，只不过目标函数稍微复杂一些。经过漫长的推导（真的很漫长…），就可以得到最终的只含有\(\alpha_2^{new}\)的目标函数，接下来求出目标函数在\(\alpha_2^{new}\)处的梯度，然后找到梯度为\(0\)的点，就可以使得目标函数\((2.4)\)最大，得到\(\alpha_2^{new}\)的值。最终导出的方程式是： \[(k(x_1,x_1) +k(x_2,x_2)-2k(x_1,x_2))\alpha_2^{new} = y_2((k(x_1,x_1) +k(x_2,x_2)-2k(x_1,x_2))\alpha_2^{old}y_2 +y_2-y_1 +g(x_1) - g(x_2))\] \[\;\;\;\qquad \qquad = (k(x_1,x_1) +k(x_2,x_2)-2k(x_1,x_2)) \alpha_2^{old} + y2(E_1-E_2)\] 要注意，其中\(x_1,x_2\)表示的是\(\alpha_1,\alpha_2\)对应的\(x\)，\(E_1,E_2\)与之前提到的\(E_i\)计算方法一致。由上式可以得出更新\(\alpha_2^{new}\)的表达式： \[\alpha_2^{new,unc} = \alpha_2^{old} + \frac{y2(E_1-E_2)}{k(x_1,x_1) +k(x_2,x_2)-2k(x_1,x_2))}\] 得到\(\alpha_2^{new}\)，就可以根据式\((9.2)\)求出\(\alpha_1^{new}\)，这样，第一轮选择出来的\(\alpha_1,\alpha_2\)就优化完毕了。接下来只要循环这个过程，直到所有的\(\alpha\)都被更新。得到更新后的\(\alpha\)之后，只要根据式\((7)\)，就可以得到\(w,b\)，进而得到最优超平面 $ L ^* $ ，这样就完成了SVM分类器。 接下来给出使用SMO算法进行训练的python代码（注意：以下代码封装的是一个连贯的函数，请连起来看0v0）。首先确定要使用的核函数，并将所有的\(\alpha\)值初始化： 12345def train(trainData, trainLabel, num, kernel): # alpha = np.random.randint(0, C, (num, )) print('*** data training start ***') func = K[kernel] if kernel else np.dot alpha = np.zeros(num) 接下来开始循环选取适当的\(\alpha_i,\alpha_j\)进行优化，选取方法是直接调用之前的chooseAlpha函数。选取完毕后，首先计算出对应的\(E_i,E_j\)： 123456789for t in range(loopTimes): index = [x for x in range(num)] b = 0 while len(index): i, j = chooseAlpha(trainData, trainLabel, b, alpha, index) Ei = pred(trainData[i], b, alpha, kernel, trainData, trainLabel)-trainLabel[i] Ej = pred(trainData[j], b, alpha, kernel, trainData, trainLabel)-trainLabel[j] 接下来根据之前的内容，计算得出\(\alpha_2^{new}\)的临时解（未进行剪切）： 123456789101112131415preI = alpha[i].copy()preJ = alpha[j].copy()yi = trainLabel[i]yj = trainLabel[j]xi = trainData[i]xj = trainData[j]if yi == yj: L = max(0, alpha[j]+alpha[i]-C) H = min(C, alpha[j]+alpha[i])else: L = max(0, alpha[j]-alpha[i]) H = min(C, C+alpha[j]-alpha[i])# c = -sum([trainLabel[k]*alpha[k]# for k in range(num) if k != i and k != j])eta = 2*func(xi, xj.T)-func(xi, xi.T)-func(xj, xj.T) 如果求得的上下界相等，或者得到的\(\alpha_2^{new}\)与原值几乎相等（无需更新），则直接跳过本轮循环。否则，对\(\alpha_2^{new}\)进行剪切，求出\(\alpha_2^{new}\)的解析解： 123456789if L != H and eta &lt; 0: alpha[j] -= yj * (Ei-Ej)/eta if alpha[j] &gt; H: alpha[j] = H if alpha[j] &lt; L: alpha[j] = L if np.abs(alpha[j]-preJ) &lt; 1e-3: index.remove(i) continue 接下来对\(\alpha_1\)进行更新，并根据式\((7)\)计算\(b\)。如果当前更新的样本点是支持向量点，则求出\(b\)在本轮循环的平均值。最后从索引列表中删除当前\(\alpha_i,\alpha_j\)对应的索引，表示该索引对应的数据已经更新完毕。不断循环该过程，就可以完成训练，最终返回更新后的\(\alpha,b\)，训练结束。 123456789101112131415161718 alpha[i] += yi*yj*(preJ-alpha[j]) b1 = b-Ei-yi*(alpha[i]-preI)*func(xi, xi.T) -\ yj*(alpha[j]-preJ)*func(xi, xj.T) b2 = b-Ej-yi*(alpha[i]-preI)*func(xi, xj.T) - \ yj*(alpha[j]-preJ)*func(xj, xj.T) if alpha[i] &gt; 0 and alpha[i] &lt; C: b = b1 elif alpha[j] &gt; 0 and alpha[j] &lt; C: b = b2 else: b = (b1+b2)/2 index.remove(i) else: index.remove(i) continue print('training: epoch No.', t)print('*** data training finish ***')return alpha, b 数据测试 完成分类器之后，数据测试就变得十分简单。在最开始，我们已经给出了SVM正确分类的充要条件，即条件\((1)\)。因此只要将测试集数据输入SVM，判断式\((1)\)是否成立即可。以下为数据测试的python代码： 1234567891011def test(testData, testLabel, trainData, trainLabel, alpha, b, kernel): print('testing...') count = 0 for x, y in zip(testData, testLabel): if not kernel: w = cal_w(x, alpha, trainData, trainLabel) predLabel = np.dot(w, x.T)+b else: predLabel = pred(x, b, alpha, kernel, trainData, trainLabel) count += 1 if y*predLabel &gt; 0 else 0 return count/len(testLabel) 至此，我们终于完成了一个支持向量机！撒花 ★,°:.☆(￣▽￣)/..°★* 这篇博客从5.28开始，已经过去快半个月了…终终终终于写完啦！（泪流满面 \[2018.6.12\;by \; \mathcal{WHY}\]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（三）多层全连接网络与BP算法]]></title>
    <url>%2F2018%2F05%2F20%2FAI%E9%85%B1%E5%85%BB%E6%88%90%E8%AE%A1%E5%88%92%EF%BC%88%E4%B8%89%EF%BC%89%E5%A4%9A%E5%B1%82%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%E4%B8%8EBP%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[科学是通过一次又一次的葬礼前进的 ——普朗克 #3 多层全连接网络与BP算法 神经元模型 当我们研究机器学习时，我们希望机器可以变得更加智能——自然的想法是通过模拟人脑的真实结构来达成这一点。但是要知道，人脑的结构非常非常复杂，直接模拟大脑似乎是一件不可能的事。所以让我们从最简单的单位——神经元开始吧。与生物体中的神经元类似，神经元模型之间会互相连接并传递信号，神经元可以认为是一个完成 “接受信号-&gt;对接受的信号进行处理-&gt;把处理后的信号发出” 的工作单元。这样看来，一个神经元的本质就是一个函数，其输入参数为传入的信号，输出值为传出的信号。这样就可以得到简单的神经元模型： 神经元模型示意图 这就是经典的M-P神经元。我们记图中表示的神经元为\(c\)，可以看到\(c\)接受\(n\)个输入值，将输入值经过特定的计算后，发出一个输出值。其中要注意： 神经元之间的连接是加权的——也就是说，上一个神经元的输出值\(x_i\)在传递给\(c\)之后，需要先乘以一个对应权重值\(w_i\)，这用来描述不同输入值的重要程度。 神经元本身有一个偏置值，在图中表示为\(b\)，用来描述神经元被激活的难易度。假设\(c\)的激活阈值是\(\theta\)，那么有\(-b=\theta\).也就是说，神经元接受的输入超过\(-b\)时就被激活。 神经元\(c\)将接受到输入值累加起来作为总输入值，并与阈值\(-b\)进行比较，还要将得到的值放入一个激活函数\(f\)中，再输出最终的输出值。\(f\)的选取有很多种，其中比较常用的有sigmoid函数： \[\sigma(x)=\frac{1}{1-e^{-x}}\] 之前提到过，一个神经元的本质就是一个函数。于是，可以得出M-P神经元所表示的函数就是： \[c(x_1,x_2...x_n)=f\left(\sum_{i=1}^{n}w_ix_i+b \right)\qquad(1)\] 从式\((1)\)可以看到，M-P神经元表示的函数中，直到传值给激活函数之前，进行的都是简单的线性运算。这样除了计算方便以外，还有一个最大的好处：可以使用矩和向量来进行计算。 多层全连接神经网络 有了神经元模型，我们就可以在此基础上构建神经网络了。虽然叫做神经网络，但事实上它和真正的神经系统仍有很大差别。因为是由神经元构成的网络，自然就叫做“神经网络”了。 神经网络的种类极其丰富，不同的神经网络可以用来完成各种各样的机器学习任务。在这里，我们使用最简单的神经网络：多层全连接网络。多层全连接网络的结构像下图那样，由若干层组成(假设共有\(n\)层)，每一层又有若干个神经元，神经元之间彼此连接，其中： 相邻两层之间的任意两个神经元均有连接（全连接） 同一层之间的神经元没有连接 不相邻层之间的神经元没有连接 最左边的一层是输入层，最右边的一层是输出层，其他各层称为隐层。 神经网络示意图 我们知道，每一个神经元都有若干个输入值和一个输出值。对第\(i\)层中第\(j\)个神经元\(c_{ij}\)来说： 它的输入值是\(i-1\)层的所有神经元的输出值（因为\(c_{ij}\)与上一层的神经元全连接） 输出值作为\(i+1\)层中所有神经元的输入值的一部分。 于是，以数据分类任务为例，我们将一个样本的各项特征值作为输入层各个神经元的输入值（也就是整个神经网络的输入值。这也要求输入层神经元个数要与样本的特征数相等），样本数据从第一层开始，沿着网络中的神经元不断传递，最终在输出层输出的值就是最终的预测结果（这就要求输出层的神经元个数要与分类的类别数相同），以此来完成预测任务。 如果我们从宏观上以向量和矩阵的思维来看这一步骤，又会有一些新的发现。假设第\(i\)层有\(m_i\)个神经元，第\(i-1\)层有\(m_{i-1}\)个神经元，那么： 将第\(i\)层各个神经元的输入值看作\(m_{i-1}\times 1\)的向量\(x\)。 每两层之间的神经元的连接是带有权重的，由于第\(i\)层的神经元和第\(i-1\)层的神经元之间全连接，在这两层之间就有\(m_{i-1}\times m_{i}\)个连接，也就有\(m_{i-1}\times m_{i}\)个对应的权重，我们把这些权重看作\(m_{i-1}\times m_{i}\)的权重矩阵\(W\)。 对第\(i\)层的\(m_i\)个神经元而言，共有\(m_i\)个偏置值，把这些偏置值看作\(m_i\times 1\)的偏置矩阵\(b\) 我们把神经网络的第\(i\)层看作函数\(L_i\)，根据神经元函数式\((1)\)，可以得到： \[L_i(x)=f\left(Wx^T+b^T\right)\] 因此，一个样本数据在神经网络中的传递，就是不断进行矩阵乘法和加法，以及调用激活函数计算的过程。这个过程称为前向传播（forward）。接下来的问题是：我们该如何得到适当的权重矩阵和偏置矩阵呢？在网络建立之初，网络的参数一般使用随机赋值的方法来初始化，之后再通过训练不断修正。这里我们使用python类来描述网络的结构，并写出前向传播的方法： 123456789101112131415class Net(object): def __init__(self, size): self.weights = [np.random.randn(x, y) for x, y in zip(size[:-1], size[1:])] self.bias = [np.random.randn(num) for num in size[1:]] self.num = len(size) self.size = size def pred(self, feat): tmp = feat nerve = [feat] for i in range(self.num-1): tmp = sig(np.dot(tmp, self.weights[i])+self.bias[i]) nerve.append(tmp) return nerve 准备：数据导入及处理 有了网络结构，接下来进行数据的导入。将数据划分为训练集和测试集，导入成为numpy矩阵，并进行归一化处理。这里使用min-max归一化方法。进行归一化处理的代码如下所示： 123maxMat = np.row_stack((np.max(data, axis=0),)*num)minMat = np.row_stack((np.min(data, axis=0),)*num)data = (data-minMat)/(maxMat-minMat) 代价函数与梯度下降 代价函数 之前提到，神经网络的参数\(\Theta\)（包括所有的权重矩阵和偏置矩阵）的初始化是随机的。我们要想训练网络模型，就需要知道如何对参数进行调整。假设对于一个训练样本\((x,y)\)（其中\(x\)是样本特征值，\(y\)是样本标签），我们希望通过调整参数\(\Theta\)，使得网络的预测结果能向着正确结果逼近。那么，假设有一个函数\(E(y,y&#39;)\)，可以描述当前的预测值\(y&#39;\)和实际标签\(y\)之间的差距，我们只要向着使\(E(x,y&#39;)\)减小的方向调整参数就可以了。这个函数\(E\)就被称为代价函数。 在实际训练过程中，代价函数有很多选择，可以根据不同的任务来选取，在这里我们使用比较简单的均方误差来作为代价函数。假设网络的输出值\(y&#39;\)是长度为\(l\)的向量，用\(y_i&#39;\)表示\(y&#39;\)的第i个元素，则： \[E(y,y&#39;)=\frac{1}{2}\sum_{i=1}^l(y_i-y_i&#39;)^2\] 优化参数 代价函数表面看来只有\((y&#39;,y)\)两个参数，但在计算\(y&#39;\)的过程中，需要用到神经网络\(N\)的所有参数\(\Theta\)以及样本特征值向量\(x\)，也即\(y&#39;=N(\Theta,x)\)。因此，\(E(y,y&#39;)=E(\Theta,x,y)\)。为了使得\(E\)的取值尽可能小，比较可行的方法是，向着“使得代价函数更小”的方向调整参数，就可以使神经网络的预测值向着正确值不断靠近。这里“使得代价函数更小的方向”用更加数学的语言来表示，也就是“代价函数的负梯度方向”。 我们知道，函数的梯度给出了函数增长最快的方向，因此它的反方向也就是下降最快的方向。向着负梯度方向调整参数，这就是传说中的梯度下降法。使用一个样本进行训练时，首先计算出代价函数，并求出代价函数对每一个参数（包括所有的权重和偏置值）的偏导数，所有参数的偏导项合起来就构成了梯度向量，梯度向量的每一项指示了每一个参数应该如何调节。 梯度下降法有很多不同的变种，比如动量法，adagrad等改进方法，此处我们以普通的梯度下降法为例，进行参数的优化。 误差逆传播算法(BP算法) 对于神经网络而言，梯度下降这个过程就变得有些复杂——因为神经网络有很多层，需要对大量的参数求偏导。而且从整体来看，不同层上不同参数的偏导数表达式也是不同的，如果直接盲目计算的话，将会耗费大量精力。因此引入了BP算法来解决求偏导这一问题。 计算图与链式法则 所有复杂的函数都可以分解为若干简单函数的组合，例如四则运算，指对函数，幂函数等等。我们可以用计算图模型将这些组合表示出来，图中的每一个结点表示一个简单函数。例如下图表示的复杂函数，可以用计算图简单地表示出来： （什么？你觉得还是原来更简单？计算机可不这么想…） 将函数分解为\(+\)或\(*\)这样的简单运算之后，求导和计算都变得很容易。计算图沿着箭头方向传播，是函数求值的过程；逆着箭头方向传播，就是函数求导的过程。这也是“逆传播”的含义。计算图的求导规则是： 对末端结点，可以根据函数值和该节点表示的简单函数直接求出偏导，并将偏导数传给下游结点。 中间节点接受从上游结点传来的偏导值，并乘以该结点对应的简单函数的偏导（本地偏导），将结果传递给下游节点。 初始结点接受上游传来的偏导值，并乘以本地偏导，得到最终关于某个参数的偏导。 可以发现，计算图运用的求导方法正是函数求导的链规则。通过将这一过程流程化，只要逆向遍历计算图，就可以得到对任何参数的偏导（无论该参数是在何处输入的），这也正是许多深度学习框架中所使用的方法。神经网络本身也是一个复杂函数。只要将它的计算图表示出来，就可以很容易地求出关于各个参数的偏导。 这样我们就解决了神经网络求导的问题。根据求导得到的梯度向量，就可以决定每个参数该增减多少，这样就实现了神经网络在一个样本下的训练过程。对一个训练样本进行参数优化的代码如下： 123456789101112def update(self, label, nerve, alpha): sigma = (label-nerve[-1]) deltaW = [x*0 for x in self.weights] deltaB = [x*0 for x in self.bias] for i in range(self.num-1): grad = nerve[-1-i]*(-nerve[-1-i]+1)*sigma gradMat = np.row_stack((grad,)*len(nerve[-2-i])) Bn = np.column_stack((nerve[-2-i],)*len(nerve[-1-i])) deltaW[-1-i] += alpha*gradMat*Bn deltaB[-1-i] += -alpha*grad sigma = np.sum(grad*self.weights[-1-i]) return deltaW, deltaB 接下来只要不断重复这个过程，每输入一个训练样本，神经网络就进行一次参数调整，如此循环，就可以完成网络的训练。整个BP算法训练过程的代码如下： 12345678910111213141516171819def BP(self, trainData, trainLabel, epoch, batch, alpha): for j in range(epoch): for i in range(int(len(trainLabel))): deltaW = [x*0 for x in self.weights] deltaB = [x*0 for x in self.bias] batchData = trainData[i] batchLabel = trainLabel[i] nerve = [self.pred(x) for x in batchData] predLabel = [x[-1] for x in nerve] loss = np.sum((batchLabel-np.array(predLabel))**2)/2 for x, y in zip(batchLabel, nerve): w, b = self.update(x, y, alpha) for k in range(self.num-1): deltaW[k] += w[k] deltaB[k] += b[k] for k in range(self.num-1): self.weights[k] += deltaW[k] self.bias[k] += deltaB[k] print('training: epoch', j, 'loss:', loss) 累积BP算法 按照BP算法，每输入一个样本，就要进行一次参数优化。但是这样的优化只针对当前的一个样本有效——也就是说，并不能提高网络在其他样本上的表现。因此，我们使用累积BP算法来解决这一问题。 所谓累积，指的是将整个训练集中所有样本的梯度向量累积起来，计算出每个样本上的梯度向量后，先不急着调整参数，而是等到把整个训练集中所有样本的梯度都计算过以后，取各个梯度向量的平均值，再根据平均值进行参数优化。这样就保证了每次优化都兼顾到了所有的数据，而不仅仅针对单独的训练数据。使用累积BP算法进行训练的代码如下所示： 1234567891011121314151617def AEBP(self, trainData, trainLabel, epoch, alpha): length = len(trainLabel) for j in range(epoch): deltaW = [x*0 for x in self.weights] deltaB = [x*0 for x in self.bias] nerve = [self.pred(x) for x in trainData] predLabel = [x[-1] for x in nerve] loss = np.sum((trainLabel-np.array(predLabel))**2)/length for x, y in zip(trainLabel, nerve): w, b = self.update(x, y, alpha) for k in range(self.num-1): deltaW[k] += w[k] deltaB[k] += b[k] for k in range(self.num-1): self.weights[k] += deltaW[k]/length self.bias[k] += deltaB[k]/length print('training: epoch', j, 'loss:', loss) 数据测试 数据测试所用到的前向传播方法在之前的BP算法中已经提到了。输入一个测试样本，在网络中进行前向传播，输出层的输出结果就是样本的预测结果。使用测试集测试并计算准确率的代码如下所示： 123456789def test(self, testData, testLabel): count = 0 for x, y in zip(testData, testLabel): tmp = self.pred(x)[-1] Pred = np.argmax(tmp) if Pred == np.argmax(y): count += 1 acc = count/len(testLabel) return acc 至此，我们就完成了一个多层全连接网络！ \(\textit{2018.5.8}\quad by\ why.\)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(二) 聚类]]></title>
    <url>%2F2018%2F05%2F08%2FAI%E9%85%B1%E5%85%BB%E6%88%90%E8%AE%A1%E5%88%92%EF%BC%88%E4%BA%8C%EF%BC%89%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[物以类聚，人以群分。 #2聚类 算法思想 与监督学习不同，聚类代表的是无监督学习(unsupervised learning)。所谓‘无监督’，指的是训练样本没有标签标记。在聚类任务中，模型需要自己找出他们之间的区别，并完成分类的任务–由于没有标签，我们有时无从知道这些类别的代表的含义，但是这种无标记的训练方法，往往可以揭示数据的内在规律。 聚类的思想是，假设训练数据构成集合\(\Bbb D=\{x_1,x_2...x_n\}\)，包含\(n\)个没有标记的样本，每个样本包含的\(m\)个特征构成m维特征向量\(x_i=\{x_{i1},x_{i2}...x_{in}\}\)，聚类的思想是：假设要将数据集划分为k类，首先选取k个样本\(P=\{\lambda_1,\lambda_2...\lambda_k\}\)作为样本中心点（将来其他样本要向这k个中心点聚集），通过某种判断标准（后面会介绍），将每一个样本\(x_i\)与样本中心点集合\(P\)的每个元素之间进行比较，根据该标准选取出合适的中心点\(\lambda\)，并将样本\(x_i\)划分到以\(\lambda\)为中心的子集中。这样不断循环，最终将整个数据集\(\Bbb D\)划分成了k个子集。其中，每一个子集称为一个簇，直观来看，相当于所有样本“聚集”到了相应的簇中。 划分标准 那么，以何种方式对集合进行划分呢？以分类问题为例，我们希望聚类之后的每一个簇中都属于同一类别，因此，需要找到一种衡量样本之间相似程度的方法：这就引入距离度量的概念。 对样本\(x_i=\{x_{i1},x_{i2}...x_{in}\}\)，可以表示为由特征向量张成的n维空间中的一个点。特征越相似的样本，在空间中的点距离就越近。因此，可以通过计算空间中样本点的距离（通常使用欧氏距离），来判断两个样本的相似程度，并以此为依据来进行划分。 距离度量可以根据实际需求有所变化。这里所说的“距离”，实际上指的是闵可夫斯基距离————也即是两个向量的差向量的范数。n维向量\(x=(x_1,x_2,...x_n)\)和\(y=(y_1,y_2,...y_n)\)之间的闵可夫斯基距离定义为： \[D(x,y)=(\sum_{i=1}^n(x_i-y_i)^p)^{\frac{1}{p}}\] 根据范数取法的不同，闵可夫斯基距离也表现出不同的形式： \(p=1\)时，\(D\)表现为曼哈顿距离：\[D(x,y)=\sum_{i=1}^{n}(x_i-y_i)\] \(p=2\)时 \(D\)表现为欧氏距离：\[D(x,y)=\sqrt{(\sum_{i=1}^n(x_i-y_i)^2)}\] \(p\to\infty\)时，\(D\)是向量\(x-y\)的无穷范数：\[D=max_i(|x_i-y_i|)\] 要注意的是，对于连续属性，可以方便地进行距离计算，对于离散属性（特别是没有大小之分的离散值），样本间的距离就变得难以计算。并且对离散属性而言，往往距离不满足直递性（《机器学习》p201）。因此对于离散型变量，我们采用另外的处理办法。 （待续） 准备：数据导入和处理 这里我们使用西瓜数据集4.0来进行训练，该数据集的聚类任务是通过西瓜的含糖量和密度，推断西瓜是否是好瓜。数据集如下所示： 编号 密度 含糖量 编号 密度 含糖量 编号 密度 含糖量 1 0.697 0.460 11 0.245 0.057 21 0.748 0.232 2 0.774 0.376 12 0.343 0.099 22 0.714 0.346 3 0.634 0.264 13 0.639 0.161 23 0.483 0.312 4 0.608 0.318 14 0.657 0.198 24 0.478 0.437 5 0.556 0.215 15 0.360 0.370 25 0.525 0.369 6 0.403 0.237 16 0.593 0.042 26 0.751 0.489 7 0.481 0.149 17 0.719 0.103 27 0.532 0.472 8 0.437 0.211 18 0.359 0.188 28 0.473 0.376 9 0.666 0.091 19 0.339 0.241 29 0.725 0.445 10 0.243 0.267 20 0.282 0.257 30 0.446 0.459 将数据集导入为numpy数组。此处以k-means算法为例，进行聚类的实现。由于数据都是连续值，性质十分优良，无需进行特殊处理。导入数据的python代码如下所示： 123456def load(): with open("/media/why/DATA/why的程序测试/AI_Lab/Task/Task_week2/melon4.0.csv", "r") as f: reader = csv.reader(f) data = [x for x in reader] data = np.array(data, dtype="float64") return data k-means算法 有了数据集和划分标准，接下来就可以进行聚类了.我们知道了划分标准是选择样本点之间的最近距离，那么问题来了：该把哪一个点作为样本中心点呢？k-means算法给出的答案是：先随机找k个点，再通过不断优化修正中心点的位置，得出最终的k个中心点。之所以叫做k-means，是因为该算法把各个簇中样本的均值向量作为中心点。对于蔟\(C\)，其均值向量\(\mu\)定义为： \[\mu=\frac{1}{|C|}\sum_{x\in C}{x}\] 对于集合\(\Bbb D\)划分得到的蔟集合\(\{C_1,C_2...C_k\}\)，k-means算法的目的是通过优化，使得其平方误差最小化。平方误差\(E\)定义为： \[E=\sum_{i=1}^k\sum_{x\in C_i}||x-\mu_i||_2^2\] 可以用来衡量蔟内样本围绕中心点的紧密程度（越小越好）。 据此得到k-means的算法的步骤如下： 1. 初始化：从\(\Bbb D\)中随机选取\(k\)个样本作为中心点 2. 聚集：以其他样本和各个中心点之间的距离为标准，找出每个样本离哪个中心点最近，将样本划分到该中心点所在的簇中 3. 优化：划分结束后，计算出每个蔟的均值向量作为新的样本中心点，重复步骤2，直到均值向量不再变化为止。 k均值算法的代码如下所示： 1234567891011121314151617181920212223def divide(data, k): num = len(data) ave = np.ones((k, 2)) category = [] for i in range(k): category.append([]) ix = np.random.choice(range(num), replace=False) ave[i] = data[ix] for j in range(100): for i in range(k): category[i] = [] for x in data: d = [np.linalg.norm(x - y) for y in ave] category[d.index(min(d))].append(x) new_ave = np.ones((k, 2)) for i in range(k): new_ave[i] = np.mean(category[i], axis=0) tmp = np.mean(new_ave-ave) if np.fabs(tmp) &lt; 1e-60: break else: ave = new_ave return ave, category 我们使用matplotlib库对聚类之后的样本数据进行绘制，不同的类别用不同颜色表示，s使用蓝色表示样本中心点。当k=2时，聚类结果如下所示： 性能度量 由于聚类的数据集没有进行标记，因此无法像之前的机器学习算法一样计算AUC或ACC。对于聚类，我们有单独的度量标准。 ###外部标准 有时候，我们也会使用有标记的数据进行聚类。但是不使用acc等评价标准———因为他们评价的是模型的预测能力，而我们需要一些单独针对聚类的评价指标。假设带有标记的样本真实的类别划分为：\(C^*=\{C^*_1,C^*_2,...C^*_s\}\)，共\(S\)类。我们把训练集中的样本两两配对，对于每一个样本对\((x,y)\)，分别用\(a,b,c,d\)来表示： 在\(C\)和\(C^*\)中都属于同一个簇的样本对的数目 在\(C\)中属于同一个簇，但在\(C^*\)中属于不同簇的样本对的数目 在\(C^*\)中属于同一个簇，但在\(C\)中属于不同簇的样本对的数目 在\(C\)和\(C^*\)中都属于不同簇的样本对的数目 并令\(a+b+c+d=\frac{m(m-1)}{2}\)，那么，有以下几种度量标准： jaccard系数： \[\mathrm {JC}=\frac{a}{a+b+c}\] FM指数： \[\mathrm {FMI}=\sqrt{\frac{a}{a+b}\cdot\frac{a}{a+c}}\] Rand指数： \[\mathrm {RI}=\frac{2(a+d)}{m(m-1)}\] 这些指标的取值范围都在\([0,1]\)区间，值越大越好。以JC指数为例，给出计算聚类外部评价标准的代码： 12345678910111213141516171819202122def cal_JC(category, cate_label, label, k): index = [] num = len(label) est_label = [0]*num for x in cate_label: index.append(max(x, key=x.count)) for i in range(k): for y in category[i]: for j in range(num): if (data[j]).all()==y.all(): ix=j; break est_label[ix] = index[i] a = b = c = 0 for i in range(num): for j in range(num): if i == j: continue if label[i] == label[j]: if est_label[i] == est_label[j]: a += 1 else: b += 1 elif est_label[i] == est_label[j]: c += 1 JC = a/(a+b+c) return JC 内部标准 对于集合\(\Bbb D\)划分得到的蔟集合\(\{C_1,C_2...C_k\}\)，定义以下参数： \(avg(C)=\frac{2}{|C|(|C|-1)}\sum_{1\leqslant i&lt;j\leqslant |C|}dist(x_i,x_j)\) \(diam(C)=max_{1\leqslant i&lt;j\leqslant |C|}dist(x_i,x_j)\) \(d_{min}(C_i,C_j)=min_{x_i\in C_i,x_j\in C_j}dist(x_i,x_j)\) \(d_{cen}(C_i,C_j)=dist(\mu_i,\mu_j)\) 则有下列内部评价指标： DB指数： \[\mathrm {DBI}=\frac{1}{k}\sum_{i=1}^kmax_{j\neq i} \left(\frac{avg(C_i)+avg(C_j)}{d_{cen}(C_i,C_j)}\right)\] Dunn指数： \[\mathrm {DI}=min_{1\leqslant i \leqslant k}\left\{\max_{j\neq i} \left(\frac{d_{min}(C_i,C_j)}{max_{1\leqslant i&lt;j\leqslant k}diam(C_l)}\right) \right\}\] 其中，DBI的值越小越好，DI的值越大越好。以DBI为例，计算内部评价标准的代码如下所示： 123456789101112131415def cal_DBI(category, ave, k): avg = [] DBI = 0 for ix in range(k): num = len(category[ix]) sum_dist = 0 for x in category[ix]: for y in category[ix]: sum_dist += np.fabs(np.linalg.norm(x - y)) avg.append((sum_dist)/(num*(num-1))) for i in range(k): DBI += max([(avg[i]+avg[j])/np.fabs((np.linalg.norm(ave[i]-ave[j]))) for j in range(k) if j != i]) DBI /= k return DBI 到这里，我们就完成了k-means聚类的所有工作。 其他：原型聚类 我们这里介绍的k-means算法，事实上属于原型聚类的一种。所谓原型聚类，就是“基于原型的聚类”。这种算法相信数据集中的不同类别可以用一组原型来刻画（比如k-means中的均值向量）。原型聚类的算法具有一致性：通常都是先进行初始化，然后对原型进行迭代优化，得出最终结果。原型表示方法不同，优化方法不同时，可以产生不同的原型聚类算法。除了k-means之外，还有学习向量量化法（LVQ）以及高斯混合聚类，密度聚类等原型聚类算法。这些算我们法将在今后深入研究。 \({2018.4.30}\) \({by}\) \({WHY}\)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（一）种一棵决策树！]]></title>
    <url>%2F2018%2F04%2F26%2FAI%E9%85%B1%E5%85%BB%E6%88%90%E8%AE%A1%E5%88%92%EF%BC%88%E4%B8%80%EF%BC%89%E7%A7%8D%E4%B8%80%E6%A3%B5%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%81%2F</url>
    <content type="text"><![CDATA[大家一起愉快地种树吧！OvO #1 决策树 算法思想 决策树(decision tree) 是一种常用的机器学习算法。既然叫做决策树，自然是通过树结构来完成决策的。这种树结构很类似人类们思考时，通过不断提出问题来对事物进行判断。例如判断电影的好坏，我们常常会这么想:这部电影的音乐好不好?如果回答是“好”，我们又会考虑“画面怎么样?”，如果回答是“优秀”，再看“演员演技如何?”如果回答仍然是“精湛”，我们最终判断“这是一部好电影w”。 决策树的思想与之类似。我们用来训练的数据包含多个特征，而树结构包含一个根节点，若干叶节点和内部节点。在每个节点处，决策树都会根据样本的某个特征作出一次判断(正类似于“电影的画面怎么样”之类的判断)，通过不断地作出判断，最终得到预测结果。从功能上来看，决策树可以分为: 用于预测连续值(完成回归任务)的回归树(如CART树)， 用于预测离散值(完成分类任务)的分类树(如ID3，C4。5)。 此处以ID3算法为例，用python进行决策树的构建。另外，由于采用了树结构，决策树也使用到了分治的思想。 准备:数据导入及处理 此处使用的数据集是关于汽车购买的数据：通过汽车的各项特征来预测消费者对汽车的评价。使用numpy库中的np.readtxt()函数进行导入，将测试集导入到一个array中。整个程序中我们要用到的python库有numpy以及matplotlib。 划分验证集 除了测试集和训练集之外，这里使用到验证集，用于验证模型的有效性。此处采用k折交叉验证法来进行验证集的划分：首先将待划分的数据集平均分成k个子集(k一般取10)，然后循环进行k次训练，每次训练时分别将k个子集中的一个当作验证集，其余的当作测试集。 数据的导入和处理函数如下所示： 12345678910def load(f_name， divide， delimiter): data = np.loadtxt("/media/why/DATA/why的程序测试/AI_Lab/Task/Task_week2/tree/" + f_name， dtype=np.str， delimiter=delimiter) data = data[1:， :] if divide: label = data[:， -1:] data = data[:， :-1] return data， label else: return data 另外，有时也会用到留一交叉验证法:即每次只把一个样本当作验证集。这样可以充分利用每个样本，但是会造成训练次数增多，因此常常在样本数量较少时使用。这里数据集数目较多，所以不采取这种方法。 树结构实现(模型训练) 递归生成树结构 接下来，我们要构建决策树的主体部分了。决策树的构建是通过递归来完成的：从树的根节点开始，使用训练集，递归进行树的分支(划分)。通过不断划分，训练集的数据也会从根节点开始被不断分类，最终分布到各个叶节点上。当达到叶节点时停止划分。达到叶节点的情况有三种: 当前节点的数据类别都属于同一类\(X\)，无需再进行划分，此时将当前节点的类别标记为类别\(X\); 没有任何一个数据符合当前节点的条件(即当前节点上数据为空)，无法继续划分，此时将当前节点的类别标记为该节点处数据中出现最多的类别; 当前用来划分的依据(样本特征值)已经用尽，无法继续划分，此时将当前节点的类别标记为其父节点的类别。 当一个节点满足上述条件之一时，我们将它标记为叶节点，并返回该节点下的数据类别。由于使用递归的方法，叶节点的返回值会被其父节点接收，父节点接收返回值后，将该叶节点和其返回值加入到当前的子树中，不断重复这个过程，最终在根节点处返回一棵完整的决策树。 值得注意的是，在pyhon中无法使用指针来构造树形结构，而且每个节点的分叉数目事先无法确定，因此这里我们使用嵌套的 字典（dict） 来实现树结构。字典从外到内层层嵌套，分别代表从根节点到叶节点的逐层分叉。这样，在测试数据时，只要递归地从外到内遍历字典，就可以实现对树的遍历。 使用python进行递归构造的代码如下所示： 12345678910111213def plant(data， label， feat_label， valiData， valiLabel): values， counts = np.unique(label， return_counts=True) if np.shape(values)[0] == 1: global leafNo leafNo += 1 return (leafNo， label[0][0]) if np.shape(data[0])[0] == 1: leafNo += 1 return (leafNo， values[np.argmax(counts)]) uqFeat = optimize(data， label， valiData， valiLabel) if type(uqFeat).__name__ != 'int': leafNo += 1 return (leafNo， uqFeat) 另外，在标记叶节点时，除了标记叶节点下的数据标签以外，同时为每个叶节点分配一个编号，之后画ROC曲线的时候会用到。 节点划分 构造树的过程，实际上就是使用训练集进行训练的过程。在节点处，训练集数据被不断划分。那么接下来的问题是：在某一个节点处，使用样本的哪一个特征作为划分的依据呢？这就需要引入信息熵的概念。 为了对当前结点上的数据进行划分，我们一般希望划分之后，每个子结点上的数据类型尽可能相同，即每个子结点上的数据「纯度」要尽可能高，信息熵可以认为是一种描述信息纯度的指标。假设数据集\(\Bbb D\)包含n种不同的标签，第k种标签出现的概率为\(p_k\)，那么数据集\(\Bbb D\)的信息熵表示为： \[Ent(\Bbb D)=-\sum_{k=1}^n p_klog_2p_k\] 计算数据集信息熵的代码如下所示： 123456def cal_ent(label): num = len(label) _， counts = np.unique(label， return_counts=True) prob = counts/num ent = (np.sum(prob*np.log2(prob))) return -ent 因此，假设某一节点处根据特征\(a\)来划分数据集\(\mathbb D\)，划分后的数据集为\(\{D^1,D^2,...D^m\}\)，可以通过计算该节点处划分前后的信息熵，来得出根据特征\(a\)划分数据的信息增益： \[ Gain(\Bbb D,a)=Ent(\mathbb D)-\sum_{k=1}^m \frac{|D^k|}{|\mathbb D|}Ent(D^k) \] 信息增益可以用来表示划分以后信息是否变得“更纯”，以及“纯度”增加了多少。因此，可以以此为依据，找出使得信息增益最大的划分方式。通过计算信息增益来优化划分方式的代码如下所示： 12345678910111213141516def optimize(data, label, valiData, valiLabel): num = len(label) length = len(data[0]) originEnt = cal_ent(label) maxGain = 0.0 uqFeat = 0 # &lt;-作为最佳划分依据的特征 for i in range(length): _, new_label, _ = divide(data, label, i) sigma = 0 for x in new_label: sigma += (cal_ent(x))*len(x)/num gain = originEnt-sigma if gain &gt; maxGain: maxGain = gain uqFeat = i return uqFeat 开始训练 封装好构造树函数以及优化函数之后，将训练集传入树函数，即可进行数据的训练，返回值是用来代表树结构的嵌套字典。到这里，我们就初步得到了一棵决策树。 数据测试 得到决策树之后，需要使用验证集来验证模型的有效性。在测试数据时，对于每一个测试样本，都要从根节点开始，沿着树枝一直走到叶节点，最后到达的叶节点就是该样本的预测结果。在此过程中，仍然使用递归的方法： 从根节点开始，将样本向下传递。对于当前结点: 如果当前节点下的子节点已经是叶节点，那么返回叶节点上的标签值作为预测结果； 如果当前节点下仍有子树，那么递归调用函数自身，将测试样本传递给子树。 对一个样本进行测试的代码如下所示： 1234567891011def classify(data, tree): for x in tree.keys(): ix = x dict = tree[ix] for key in dict.keys(): if data[ix] == key: if type(dict[key]).__name__ == 'dict': label = classify(data, dict[key]) else: label = dict[key] return label 处理: 预剪枝与后剪枝 在构造决策树过程中，树的结构是由训练集决定的。决策树力图对训练集的数据分布尽可能地进行模拟，因此往往会出现过拟合的问题：决策树将样本学习得太好了，以至于对未曾见过的数据的泛化能力较差。为了减少过拟合问题，需要对决策树进行剪枝操作。 预剪枝 预剪枝是在构建树结构时，在划分节点之前进行的。对节点进行划分前，先假设不划分该节点，即将该节点标记为叶节点，并比较一下划分前后，在该节点上使用训练集测试的正确率哪个更大。如果不划分时正确率更高，那么直接将该节点标记为叶节点，不对其进行展开，这样就实现了预剪枝工作。 要进行预剪枝工作，需要对之前的优化函数optimize进行改动，在optimize函数后添加代码： 12345678910111213141516171819202122values, counts = np.unique(label, return_counts=True) maxValue = values[np.argmax(counts)] values, counts = np.unique(valiLabel, return_counts=True) acc1 = counts[np.where(values == maxValue)[0]]/len(valiLabel) _, new_label, featValue = divide(data, label, uqFeat) n = len(featValue) tmpLabel = [] for i in range(n): values, counts = np.unique(new_label[i], return_counts=True) value = values[np.argmax(counts)] tmpLabel.append(value) count = 0 for i in range(len(valiLabel)): for j in range(n): if featValue[j] == valiData[i][uqFeat] and tmpLabel[j] == valiLabel[i]: count += 1 acc2 = count/len(valiLabel) if acc1 &lt;= acc2: return uqFeat else: return maxValue 要注意，对于我们使用的汽车购买的数据集，由于数据集标签中‘unacc’标签占了绝大多数，因此在预剪枝时，内部节点被标记为叶节点’unacc’之后，得到的正确率甚至比划分节点之后的正确率还要高，因此决策树的大部分树枝都被剪掉了——结果导致决策树变成了决策树桩，这显然不是我们想要的，因此，可以根据数据集的实际情况，来决定是否进行预剪枝。 后剪枝 后剪枝是在决策树构造完毕后，再对其进行剪枝。首先将用来剪枝的数据集传给决策树，使得数据集沿着数值被不断分类，最终分布到各个叶节点上。对于所有的叶节点，我们考察是否要进行剪枝——剪枝的含义是，将该叶节点的父节点标记为叶节点，并将父节点下的所有数据中，出现次数最多的标签作为父节点的标签。 判断是否进行剪枝操作的标准依然是数据集的正确率。对于该父节点下的样本，如果进行剪枝后正确率比剪枝前的更高，那么就进行剪枝操作。 * 剪枝前的正确率计算：将该父节点及其分支作为一棵子树，将该父节点下的所有数据分别作为测试数据，传递给classify函数，函数返回预测值，将预测值和样本的实际标签比较并计算出正确率。 * 剪枝后的正确率计算：假设该父节点下的所有数据为集合\(\mathbb D\)，\(\mathbb D\)中出现次数最多的标签集合为\(L\)，那么正确率为： \[Acc=\frac {|L|}{|\mathbb D|}\] 后剪枝也使用了递归的方法：将用于剪枝的数据集data和其对应的标签集label传给函数，递归遍历整棵树，当到达叶节点时，对其父节点计算剪枝前后的样本正确率，并判断是否进行剪枝操作。若不剪枝，则返回原有的树结构，并继续寻找下一个叶节点;若选择剪枝，则返回剪枝后的叶节点。直到遍历过所有的叶节点为止，返回剪枝后的整棵决策树。 进行后剪枝的代码如下所示： 1234567891011121314151617181920212223242526272829def cut(tree, data, label): global newLeafNo if len(label) == 0: return tree for x in tree.keys(): ix = x dict = tree[ix] for key in dict.keys(): tmpData = [] tmpLabel = [] for i in range(len(data)): if data[i][ix] == key: tmpData.append(data[i]) tmpLabel.append(label[i]) tmpData = np.array(tmpData) tmpLabel = np.array(tmpLabel) if type(dict[key]).__name__ == 'dict': new_tree = cut(dict[key], tmpData, tmpLabel) tree[ix][key] = new_tree else: value, count = np.unique(label, return_counts=True) tmp = np.argmax(count) newAcc = count[tmp]/len(label) oldAcc = test(data, tree, label) if newAcc &gt; oldAcc: tree = (newLeafNo, value[tmp]) newLeafNo -= 1 return tree return tree 模型评估: 树模型绘制ROC曲线 一般情况下的ROC曲线 ROC曲线常常用来评估二分类模型的性能。对于一个二分类任务，标签只有正例和反例存在（分别用1和0表示）。对于一个样本，实际的类别和预测结果有以下四种情况： 实际类别为1 实际类别为0 预测类别为1 真正例(记作TP) 假正例(记作FP) 预测类别为0 假反例(记作FN) 真反例(记作TN) 规定真正例率(TPR) 和假正例率(FPR) 分别为： \[ TPR=\frac{TP}{TP+FN},FPR=\frac{FP}{FP+TN}\] ROC曲线就是由TPR和FPR分别作为x轴和y轴作出的图像。对于一个测试集，只计算出一组对应的TPR和FPR值，此时的ROC曲线只包含一个点。为了得到一条曲线，我们需要多组TPR和FPR值。 在二分类任务中，模型最终给出的结果往往不是0或1的准确预测值，而是一个0～1的数值。为了得到0或1的结果，往往会人为规定一个阈值（通常是0.5），当预测结果大于这个值的时候，就认为结果是1，反之是0。同一个模型，当阈值不同时可以认为是不同的分类器。因此，我们可以通过改变阈值来得到多个分类器，以从一个测试集上得到多组TPR和FPR值，从而画出ROC曲线。 决策树的ROC曲线 而对一个二分类的决策树，ROC曲线的绘制就有些困难：决策树返回的是0或1的离散值，而不是0～1的连续值。因此无法通过改变阈值来得到多组TPR和FPR。 对此，论文 Learning Decision Trees Using the Area Under the ROC Curve 给出了一种方法：假设决策树有n个叶节点。训练数据时，每个叶节点处的数据都会有2种类别(正例或反例)。而我们为该叶节点标记的标签的也只有0和1两种可能。因此整棵树的叶节点的标记的可能性有\(N=2^n\)种，这N种不同的标记方法也就对应了\(N\)种不同的决策树分类器，决策树的ROC空间中的\(N\)个点就由这\(N\)种不同的分类器(决策树)产生。 对一每种分类器(决策树)，都可以计算出它对应的TPR和FPR，从而得到在ROC空间上的一个点，这样得到的\(N\)个点就构成了决策树的ROC曲线。 优化ROC 这样得到的ROC空间中，一共有\(2^n\)个点，计算量较大。事实上，这其中很多点都是不必要的，因此可以对其进行优化，优化之后的点只有\(n+1\)个，即只需要考虑叶节点上\(2^n\)中个分类器中的\(n+1\)种，大大减少了计算量。我们将这n+1种分类器记作\(\\{S_0,S_1,S_2...S_n\\}\)。 假设一棵决策树有3个叶节点(也即\(n=3\))，并假设将数据集传给决策树后，每个叶节点上的数据分布如下所示： 正例数 反例数 leaf1 3 5 leaf2 5 1 leaf3 4 2 首先按照 局部正精度(local positive accuracy) 来对各个叶节点降序排列。（某一叶节点的局部正精度\(=\frac {\text {当前叶节点正例数}}{\text {当前叶节点总样本数}}\)）排列好之后，我们假定分类器S在每个叶节点上的标签为+或-（分别代表正例或反例），优化后的所有S在各个叶节点上的分布符合阶梯形，如下所示： 正例数 反例数 \(S_0\) \(S_1\) \(S_2\) \(S_3\) leaf1 5 1 - + + + leaf2 4 2 - - + + leaf3 3 5 - - - + 于是，我们最终得到的优化后的分类器为： \[S_0=\{-,-,-\},S_1=\{+,-,-\}S_2=\{+,+,-\},S_3=\{+,+,+\}\] 根据这n+1个分类器，就可以得到ROC曲线上的点，从而作出ROC曲线。绘制ROC曲线的代码如下所示(在此省去计算ROC曲线上点的过程)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def draw_ROC(data, test_label, tree, leafNum): values = np.unique(test_label) num = len(test_label) k = 0 for x in values: label = [] pred_label = pred(data, tree) pred_leaf = [] for i in range(num): label.append(1 if test_label[i] == x else 0) pred_leaf.append(pred_label[i][0]) uqLeaf = np.unique(pred_leaf) posAcc = [] for l in uqLeaf: index = np.where(pred_leaf == l)[0] posNum = 0 for ix in index: if label[ix] == 1: posNum += 1 posAcc.append([l, posNum/len(index)]) maxLeaf = sorted(posAcc, key=lambda temp: temp[1], reverse=True) maxLeaf = [y[0] for y in maxLeaf] leaves = [0]*(leafNum+1) # 按照排序后的叶节点顺序,表示该分类器下,对应所有叶节点的分类情况 x_ROC = [] y_ROC = [] x_PR = [] y_PR = [] for i in range(leafNum+1): leaves[i] = 1 newPredLabel = [] for j in range(num): tmp = pred_leaf[j] ix = maxLeaf.index(tmp) newPredLabel.append(leaves[ix]) Tp, Fp = cal_ROC(newPredLabel, label) p1, p2 = cal_PR(newPredLabel, label) x_ROC.append(Fp) y_ROC.append(Tp) x_PR.append(p1) y_PR.append(p2) fig = plt.figure(k) ax = fig.add_subplot(1, 1, 1) ax.set_xlabel('False Postive Rate') ax.set_ylabel('True Postive Rate') ax.set_title('ROC Curve of Decision Tree (pre-pruning)') plt.plot(x_ROC, y_ROC) plt.scatter(x_ROC, y_ROC, alpha=0.6) plt.show() k += 1 对于多分类决策树（比如我们这里构建的就是一棵多分类决策树），绘制ROC曲线时可以将其转化为二分类问题：假设数据的可能类别有\(\\{L_1,L_2,L_3...L_N\\}\)，那么需要绘制\(n\)条ROC曲线，每次绘制分别以\(L_k\)为正例，其他类别为反例。这里的汽车数据集中，样本有4种不同类别，因此可以绘制出4条ROC曲线。绘制好的曲线大概是这个样子： 至此，我们就完成了ID3决策树的所有工作。大家来一起愉快地种树吧！ \(\bf 2018.4.26\) \(\bf by\) \(\bf WHY\)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[whyの学习笔记:各种常用的Python库]]></title>
    <url>%2F2018%2F04%2F11%2Fwhy%E3%81%AE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-numpy%2F</url>
    <content type="text"><![CDATA[whyのnumpy学习记录！ 导入： 1import numpy as np 数据类型 数组（array）： 建立数组：np.array(列表)，将列表变成array。 API np.zeros((m,n))：建立一个元素均为0的m*n矩阵。 np.ones((m,n))：建立一个元素都是1的m*n矩阵。 np.eye(m)：建立一个m阶对角线方阵，主对角线元素均为1。 np.full((m,n),p)：建立一个元素均为p的m*n矩阵。 np.random.random((m,n))：建立一个m*n的随机数值的矩阵。 Array相关操作 切片： array本质上是多维数组，因此对每一个子数组均需要指定切片。如a[1:2,5:6]表示将a矩阵的12行，56行切片。切片时:表示复制整个数组。（基本和py一样啦…） gpu加速： a=a.cuda()，注意要对a进行赋值。 求最值： 对a的每一列求最值：使用np.max(a,axis=0)和np.min(a,axis=0)函数 对a的每一行求最值：使用np.max(a,axis=1)和np.min(a,axis=1)函数 返回值是一个1* n的array 返回列表a中出现的所有不同元素:np.unique(a,return_index=Falise,return_counts=False).当return_index为true时,同时返回每个unique元素出现的第一个index;return_counts为true时,同时返回每个unique元素出现的次数. 从数组a中随机抽取元素(被抽样的数组必须一维的):使用np.random.choice(),例如np.random.choice(a,size=(2,3),replace=True),size表述要抽取的元素的大小,replace为true表示允许重复抽取. 删除矩阵a的特定行行/列: 删除行: np.delete(a,index,axis=0) 删除列: np.delete(a,index,axis=１) index表示要删除的行/列的索引,可以为数字(删除一行/列),也可以为list(删除多个指定行/列) 获取矩阵a的行列数:np.shape(a),返回值是一个tuple. 获取数组a中沿着轴x方向,最大值/最小值的索引（如果是一维数组,则不必考虑方向问题）： 沿行方向最值 沿列方向最值 最大值 np.argmax(a,axis=1) np.argmax(a,axis=0) 最小值 np.argmin(a,axis=1) np.argmin(a,axis=0) 对array进行扩展：np.concatenate((x, y), axis=1)， 表示把x和y按照axis=1的方向进行合并（axis的含义同上） 行向量变列向量：np.array([x]).T，方括号不能省略。 计算 范数计算 计算向量x和y的L2范数(欧式距离):np.linalg.norm(x - y) 统计&amp;概率 对矩阵a进行均值计算:np.mean(a,axis=0),axis=0时表示对列求均值,axis=1时表示对行求均值. 文件I/O np.loadtxt('文件名',dtype='',delimiter='分割符'),从指定的文本文件中读取数据,返回值是array类型.可以指定分隔符读取,也可以读取csv文件. 保存&amp;加载array: np.savetxt('path', array) 当数组列数\(n\)过大，或格式不匹配时可能出错，此时使用np.savetxt('path', array, fmt='%s, %s, %s')(\(n\)个%s，若为其他数据类型，使用相应的转换说明。)手动指定保存格式。 np.save('path', array)，以二进制形式保存。path中的文件名后缀应该是.npy。相应的加载方法是array = np.load('path') np.savez('path', name1 = array1, name2 = array2...)用来保存多个数组。文件后缀应当是.npz。也用a = np.load(path)加载，但返回的a是一个字典。使用a['name1']来获取相应的数组。 whyのpandas学习记录！ 导入：import pandas as pd 数据结构 基本结构DataFrame 索引：df['name']索引某一列 df.iloc[index]或df.loc[index]索引某一行 df[a:b]切片（同numpy） 常用API 文件IO： 从csv文件读取：pd.read_csv(path) 保存到csv: df.to_csv(path) 转换为numpy.ndarray: df.values 表拼接：pd.concat((df1, df2...), axis)，axis含义同numpy。 whyの正则表达式学习记录！ 不说废话直接上图 导入：import re pattern类 pattern类指的是用于匹配的模式。 将字符串形式的正则表达式编译为pattern类：p = re.compile('正则表达式',匹配模式)，返回的p是一个pattern类。 使用已有的模式p进行匹配：match = p.match('要匹配的字符串',匹配模式)，返回值match是匹配的结果（是一个SRE_Match对象），如果没有匹配项，则返回NoneType。 对匹配结果match（SRE_Match对象），使用match.group()来查看成功匹配的字符串。 正则表达式也可以不编译直接匹配，使用re.match(正则表达式,待匹配字符串,匹配模式)函数。 注： 上述匹配模式指的是忽略大小写、匹配多行之类的选项： re.I 忽略大小写 re.M 多行模式 re.S 即为.并且包括换行符在内的任意字符（.不包括换行符） re.L 表示特殊字符集 \w, \W, \b, \B, \s, \S 依赖于当前环境 re.U 表示特殊字符集 \w, \W, \b, \B, \d, \D, \s, \S 依赖于Unicode 字符属性数据库 re.X 为了增加可读性，忽略空格和#后面的注释 在python中使用正则表达式时，由于python中的字符串本身也用\进行转义，因此在字符串形式的正则表达式前一般加上前缀r，表示该字符串不转义。 API 使用正则表达式分割字符串：re.split(用来匹配分割部分的正则表达式,待分割字符串) re.findall(正则表达式,待匹配字符串)以列表形式返回所有匹配的子串。 使用Matplotlib数据可视化！ 二维函数图像 导入：import matplotlib.pyplot as plt 建立一个图像窗口plt.figure(num, figsize=(a, b)) 其中num表示编号，figsize表示画布大小。 figure是一次性的，在plt.show()之后就会销毁。 画二维函数线图：plt.plot(x, y, color='red', linewidth=w, label='name', linestyle='--')，其中x, y都是可迭代对象（如列表，数组等）。 为了画出函数图形，一般使用x = np.linspace(a, b, n)来产生区间\((a,b)\)之间n个均匀的值，然后将y表示成x的函数关系。 color='red'表示图像的颜色。 linestyle='--'表示线的类型，'--'表示虚线。 linewidth表示线宽 label表示线的名称（标签），做图例时会用到。 添加图例：plt.legend(loc=‘upper right’) loc表示图例位置。loc='best'可以自动放到最佳位置。 调整坐标轴 plt.xlim((a, b))表示x轴显示范围是\((a,b)\)，使用plt.xlabel('name')调整x轴名称。y轴同理。 调整刻度&amp;给刻度添加名称：plt.xticks(ticks, names)，ticks是刻度值组成的列表，names是相应名称组成的列表 axis = plt.gca()获得坐标轴当前的状态，axis的成员xaxis和yaxis表示两个坐标轴，apines表示边框。axis有以下方法： axis.spines['top'].set_color('red')设置上边框颜色，下、左、右同理。 axis.spines['top'].set_position(('data', 0))设置上边框的位置,(data,0)表示放在y=0处。 axis.xaxis.set_ticks_position('top')设置坐标刻度名称的位置，可以有top，bottom，both，default，none 例如，把x，y轴都调整到原点处： 1234567ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position('bottom')ax.spines['bottom'].set_position(('data', 0))ax.yaxis.set_ticks_position('left')ax.spines['left'].set_position(('data', 0)) 使用plt.xticks(())隐藏x轴，y轴同理。 添加标注 plt.plot([x0, x0,], [0, y0,], 'k--', linewidth=2.5) 在某一点\((x_0,y_0)\)处画出一条垂直于x轴的虚线. 为某一点设置样式plt.scatter([x0, ], [y0, ], s=50, color='b')，其实plt.scatter是画散点图用的… 显示网格 plt.grid(True, linestyle = &quot;--&quot;, color = &quot;r&quot;, linewidth = &quot;3&quot;) 参数含义同上。 填充 plt.fill_between(x_range, lower_bound, upper_bound, facecolor, alpha) x_range表示要填充的x范围；lower_bound， upper_bound表示填充上下界。例如，想在因变量y和x轴之间填充，可以写：plt.fill_between(x, 0, y) 散点图 scatter(x, y, s=75 c=None, marker=None, cmap=plt.cm.hot, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, edgecolors=None, hold=None, data=None) x,y分别表示各个点的横纵坐标，都是可迭代对象 s表示size，点的大小，int c表示颜色，颜色可以是一个长度等于散点数的列表，每个元素用来描述每个点的颜色。 cmap表示颜色组，设置颜色组可以自动配色。cmap=plt.cm.hot表示暖色组。所有的cmap在这里 如果想要设置渐变色，需要手动设置一个颜色数组，长度与数据的数目相同，常常使用cmap来实现渐变效果，如： 12colors = [plt.cm.viridis(x) for x in range(N)]plt.scatter(X, Y, c=colors) aplpha表示透明度 marker表示形状 edgecolors表示描边颜色 条形图 plt.bar(x, y, edgecolor, facecolor) x是各个条形的横轴值，一般是np.arange(10) y时条形的柱高，正在上，负在下。 edgecolor表示描边颜色， facecolor表示柱体颜色。 部分参数（如透明度之类的）和散点图一致。 条形图不能使用颜色组。 等高线图 plt.contourf(X, Y, Z, cmap, alpha)将三维空间的点画成等高线图。 这次的X,Y,Z不是普通的数组。生成方式如下： 1234x = np.linspace(-3, 3, n)y = np.linspace(-3, 3, n)X,Y = np.meshgrid(x, y)Z = f(X, Y) 等高图可以使用颜色组cmap aplpha表示透明度 使用C = plt.coutour(X, Y, Z, 8, colors='black', linewidth=0.5)进行描边。其中数字8表示等高线密度。 使用plt.clabel(C, inline=True, fontsize=10)为描边添加高度数字。 C是之前coutour返回的对象。 inline表示是否在线上，fontsize是字体大小。 极坐标图 使用plt.subplot(111, projection = 'polar')绘制极坐标图 直方图 plt.hist(x, bins= 10, range= None, normed= False, weights= None, cumulative= False, bottom= None, histtype= 'bar', align= 'mid', orientation= 'vertical', rwidth= None, log= False, color= None, label= None, stacked= False) x：指定要绘制直方图的数据； bins：指定直方图条形的个数； range：指定直方图数据的上下界，默认包含绘图数据的最大值和最小值； normed：是否将直方图的频数转换成频率； weights：该参数可为每一个数据点设置权重； cumulative：是否需要计算累计频数或频率； bottom：可以为直方图的每个条形添加基准线，默认为0； histtype：指定直方图的类型，默认为bar，除此还有’barstacked’, ‘step’, - ‘stepfilled’； align：设置条形边界值的对其方式，默认为mid，除此还有’left’和’right’； orientation：设置直方图的摆放方向，默认为垂直方向； rwidth：设置直方图条形宽度的百分比； log：是否需要对绘图数据进行log变换； color：设置直方图的填充色； label：设置直方图的标签，可通过legend展示其图例； stacked：当有多个数据时，是否需要将直方图呈堆叠摆放，默认水平摆放； 显示图片 plt.imshow(img)来把图片加入画布，img可以是numpy数组 使用plt.colorbar()来添加一个颜色指示条。 记得隐藏坐标轴 3D图形 导入额外模块from mpl_toolkits.mplot3d import Axes3D 定义一个画布fig = plt.figure() 添加3D坐标轴：ax = Axes3D(fig) 准备x,y数据（任意方法），得到等长的一维数组x,y 把x，y编制成格栅X, Y = np.meshgrid(x, y) 获取Z值，注意Z必须从X，Y得到，而不是x，y 使用ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.get_cmap('rainbow'), edgecolors)来绘制曲面 restride和cstride是颜色网格的大小 cmap是颜色组 edgecolors描边颜色 子图 plt.subplot(a,b,c)表示将整个figure分成a行b列，当前正在第c个子图上。 在子图上的绘制过程和之前相同。 可以使用plt.savefig('path')来保存图像。 tqdm 一个终端进度条工具。 使用方法 1234form tqdm import tqdmfor _ in tqdm(range(1000)): [我是代码] prettytable 打印出好康的表格。示例： 12345678910111213141516171819import prettytable as pttb = pt.PrettyTable()tb.field_names = ["City name", "Area", "Population", "Annual Rainfall"]tb.add_row(["Adelaide",1295, 1158259, 600.5])tb.add_row(["Brisbane",5905, 1857594, 1146.4])tb.add_row(["Darwin", 112, 120900, 1714.7])print(tb)输出：+-----------+------+------------+-----------------+| City name | Area | Population | Annual Rainfall |+-----------+------+------------+-----------------+| Adelaide | 1295 | 1158259 | 600.5 || Brisbane | 5905 | 1857594 | 1146.4 || Darwin | 112 | 120900 | 1714.7 |+-----------+------+------------+-----------------+]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[whyの学习笔记:Python]]></title>
    <url>%2F2018%2F04%2F10%2Fwhy%E3%81%AE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Python%2F</url>
    <content type="text"><![CDATA[why的Python学习记录! 大家一起愉快地写代码叭! 目录 linux下的Python配置 python运算符 控制流与API 数据类型 自定义函数 高级操作 文件I/O 面向对象(OOP) 多进程&amp;多线程 Linux下的Python配置 atom安装Python插件，或者使用IDE (spyder,pycharm)。程序运行的两种模式：命令行模式（直接运行）与Python交互模式（逐句执行）。 Python运算符 +,-,* ,/,//（取商的整数部分）,x**y （表示x^y ）,&gt;&gt;,&lt;&lt;,&amp;,|,^,~,&gt;,&lt;,&gt;=,&lt;=,==,!=,not,and,or 控制流与API 需要注意的事情： 句末不加分号（当然如果你想加的话也可以23333），函数块用缩进表示，字符串可以用单引号引用。 标准输入输出函数： print(),input()。Print末尾自动换行，可以通过在末尾添加逗号来取消末尾的换行符。print语句可以像c语言一样使用转换说明来打印变量。如 1print(“hello %s”%(“world!”))。 打印彩色&amp;粗体&amp;高亮字体： 1print("this is a \033[显示方式;字体颜色;背景颜色m text \033[0m other text") 相关参数如下： 字体色 背景色 颜色描述 30 40 黑色 31 41 红色 32 42 绿色 33 43 黃色 34 44 蓝色 35 45 紫红色 36 46 青蓝色 37 47 白色 显示方式 效果 0 终端默认设置 1 高亮显示 4 使用下划线 5 闪烁 7 反白显示 8 不可见 特别地，%s除了表示字符串以外，也可以表示list。a=input(“xxxxxxx”)表示把输入的数据赋值给a，并在输入前显示一行提示语句。在python3中input函数默认将键盘输入作为str看待，若想接受int类型，应当使用int(input()). 返回字符串长度/列表元素个数：len() 条件函数if： 12345678if &lt;条件判断1&gt;: &lt;执行1&gt;elif &lt;条件判断2&gt;: &lt;执行2&gt;elif &lt;条件判断3&gt;: &lt;执行3&gt;else:&lt;执行4&gt; 循环语句： for x in…循环：把…(一个数组)的每一个元素赋值给x（即x的值在循环体中表示的是数组的相应元素）执行循环体。 range(101)： 生成一个元素为整数0-100的序列（数组）。Range(m,n)生成从m到n前一个数字的数组。Range(1,5,2)中2表示步长（缺省为1）生成list[1,3] while循环： while x，同c语言。可使用break退出循环，或用continue结束本轮，执行下次循环。 注：在while或for语句块后可以跟随else。若在循环体中使用break语句，else部分也会被跳过。 尾递归： 函数的最后一步只调用函数本身，且返回语句不含表达式的递归。如： 12345def tailrecsum(x, running_total=0):if x == 0:return running_totalelse:return tailrecsum(x - 1, running_total + x) 是尾递归，而： 12345def recsum(x): if x == 1: return x else:return x + recsum(x - 1) 不是尾递归。因为函数的最后一步return语句中有表达式。使用尾递归，使得循环时内层函数不必使用外层函数中的值，无需储存上一层函数中的数据，只占用一个栈帧，可以避免栈溢出风险。 暂停1s输出： time.sleep(1) 数据类型 整形数据（int），浮点（float），字符串（str），布尔值（bool），none： 同c语言 复数（complex）： a+bi表示为complex(a,b)或a+bj，其中a和b是浮点型。 列表（list）： 是可变的数据类型。定义：A=[a,b,c]，列表的内容可以是不同的类，甚至可以是list，若这么做，相当于二维数组。 列表通过下标访问。A[-n]表示倒数第n个元素。 A.append(‘xxxxx’)表示向列表末尾加入元素 A.insert(n,’xxxxx’)表示向A[n]处插入新元素。 删除list的元素:使用A.pop()删除末尾元素，A.pop(i)删除指定位置的元素（也可使用del(A[i])来删除指定位置的元素）。 对list的元素进行排序：使用A.sort()语句。使用该语句后，List本身的内容会被改变。 元组（tuple,const）： 是不可变的数据类型。一旦初始化就不能修改。其内容也可以是不同的类，包括list。成员中有list时，list中的元素可变，但不能对list整体修改。元组中的元素通过下标访问。定义：A=(a,b,c)。元素只有一个时，应定义为A=(a,)来消歧义。 字典（dict）： 是可变的数据类型。查找速度快，但内存浪费多（空间换取时间）。声明语句如 1d = &#123;'Michael': 95, 'Bob': 75, 'Tracy': 85&#125; 每个元素的名称（key）和值一一对应，key不能重复，且必须为不可变对象。字典中的某个key代表的元素表示为：d['Jack']，对应值为88。 通过’x’ in d语句判断该元素是否存在（是否被定义）。存在则该语句的值为真，反之为假。 也可以使用d.get(‘Michael’)表达式判断，若该键没有对应的值则返回none。该表达式可指定不存在时的返回值：d.get(‘Michael’，-1) 使用d.keys()单独提取dict中的key为一个数组； 使用d.values()单独提取值为一个数组； 使用d.items()将key和对应的值同时提取为数组，数组的每个元素是一个tuple。 注意： 此处的“数组”是不严谨说法，其实他们的数据类型既不是list也不是tuple。 Set： 一组key的集合，但不存储值。声明：s = set([1, 2, 3])。 Key不可变，且不能重复，因此set的元素都是不可变对象，并且没有相同元素。Add(key)可以加入新元素， * remove(key)可以删除元素。 * 两个set可以进行数学上集合的交并操作，使用&amp;和|。 字符串(str,const)： 大致同c，不可变的类型。若a=’abc’ * 想要改变a的值，可以使用a=a.replace(‘a’,’A’)。也只有字符串类型可以使用replace函数。Replace函数的返回值是改变后的串内容，但a本身并没有改变。要想改变，需要进行赋值操作，即a=a.replace(‘a’,’A’)。 * 将字符串内容两边加上空格：s.center()。 * 将字符串s中的所有大写字母变成小写，使用s.lower()方法。 * 变成大写：s.upper()。 字符串中可以用转义字符\’表示单引号，\\表示反斜线，在行末使用\可以在下一行续写字符串。 可迭代对象： 包括集合数据类型，如list、tuple、dict、set、str等； 以及generator，包括生成器和带yield的generator function。可以使用函数isinstance()来判断一个对象是否是可迭代对象。可以被next()函数不断返回下一个值的对象称为迭代器（iterator）。For循环本质就是不断调用next函数来实现的。 自定义函数 函数定义语句： 12def 函数名(参数): 函数体 Python中的自定义函数使用return语句返回值。可以返回多个值，如return 0,1。返回多个值的本质是返回了一个tuple。若不写返回语句，则返回值为none。 与含有函数定义的py文件的同级目录下的文件可以直接导入函数。导入语句：import导入对象。 自定义函数不会自动检查实参和形参类型是否一致，因此函数定义中最好检查下实参的数据类型是否正确，使用内置函数isinstance(参数名,(类型1,类型2…))实现。该函数的返回值：若参数是类型1，2，3…中的某一种，返回true，否则false。 pass语句： 什么都不做的空函数函数体为pass，可以用作占位符。（如果还没想好这个函数要怎么写的话2333） 全局变量： 函数体内的变量是局部变量（同c语言），若要使用函数体外定义的变量，需要实现声明该变量是global变量。如global x 闭包结构： 可以在函数中定义内部函数，并返回这个函数。内部函数可以引用外部函数的局部变量和参数。这种结构称为闭包，即内部函数保存了外部函数的变量和参数。调用外部函数时，内部函数并没有被调用。多次调用内部函数时，内外部函数的变量如果已经在第一次调用内部函数时就被改变过，那么后续调用时使用的都是改变后的值。 参数类型： 默认参数： 定义有多个参数的函数时，如果在定义函数fx(a,b)时就给参数b进行赋值fx(a,b=2)，则这个值会成为参数b的默认值。当调用时可以选择省去参数b，若这样做，则b的值缺省为2.函数定义时，默认参数要放在各项参数的最后。 调用函数时，给默认参数赋值的问题：可以不赋值（缺省），可以按顺序赋值，也可指定默认参数赋值。如fx(m,b=n)。 注意：python自定义函数内默认参数修改的可变对象（如list），在函数之外也会改变。可以使用不变对象none和str来避免这一问题。即默认参数必须指向不可变对象。 好处： 降低了参数调用的难度。 可变参数： 参数数目可变，本质是接收一个list或tuple。函数定义为： fx(*数组名)。函数调用：fx(元素1，元素2，元素3…) 传递的元素自动组装成数组。也可直接传递数组：fx(*要传递的数组名); 也可以将普通参数和可变参数一起传入, 如def fx(a,b,*c)，传递参数时fx(1,2,3,4)，则a=1,b=2,c=(3,4)。 关键字参数： 传入任意个带有名称(key)的参数，传入后参数自动组装成dict。函数定义：fx(**dict名)，函数调用：fx(key1=值1,key2=值2……)。注：key名不带引号！ 也可直接传递dict：fx(**dict名)。关键字参数是可选的，可以缺省（不传入）。 命名关键字参数： 只接受特定key作为关键字参数，需要在定义时对关键字参数命名：fx(*,key1,key2……)，即在分隔符*,之后指明只接受那些key名作为关键字参数。在函数调用时，也只能传入（且必须传入，除非缺省时）这些key名对应的dict。传递错误或没有传入则会报错。如果在命名关键字参数前有了可变参数，则无需加上*,：如Fx(*数组名,key1,key2……)。 命名关键字参数也可以在定义时赋予默认值（缺省值）。如： 1def person(name, age, *, city=&apos;Beijing&apos;, job) 里面，city是缺省值。 命名关键字参数与其他参数混用：fx(a,b,*,ket1,key2),fx(a,b,*数组名,key1,key2) 参数组合调用： 对于任意函数，都可以通过类似func(*args, **kw)的形式调用它，无论它的参数是如何定义的。即必选参数也可以通过tuple或list传入。 高级操作 整体赋值： 如a,b,c=1,2,3，结果为a=1,b=2,c=3。本质是转化为元组整体赋值。 对数组（list，元组，str）切片（Slice）: 使用操作符“[:]”，L[0:3]表示从第0个元素开始取到第3个元素之前（取到第二个元素）. 第一个索引是0时可以省略为L[:3] 第二个索引代表最后一个元素时也可省略，如L[1:]。 倒数切片：L[-2:-1]， 若省略两个索引，则L[:]表示L的全部元素。 遍历数组： 使用for x in…语句。对dict而言，x可以是key（遍历所有的key） 默认情况下，dict迭代的是key。 如果要迭代value，可以用for value in d.values()， 如果要同时迭代key和value，可以用for k, v in d.items()。 判断一个对象是否可迭代，使用collections模块中的Iterable类型判断。使用函数isinstance(对象,Iterable)来判断。可迭代则返回true，否则false。若想像c语言一样，对list进行下标循环，可以使用函数enumerate(list)，可以把该list中的每个元素变成（下标，元素）对。可以在for语句中使用两个索引，分别代替对应的变量，如： 1for i, value in enumerate(['A', 'B', 'C']): 列表生成式： 用来创建list或从已知list推出新的list的式子。格式为：[x的表达式 for x in 数组名]如[x * x for x in range(1, 11)] 这样可以生成一组数列； 也可以加入条件：[x * x for x in range(1, 11) if x % 2 == 0] 或者加入嵌套循环：[m + n for m in 'ABC' for n in 'XYZ'] 也即是[元素 条件]的格式。 生成器（generator）： 一种动态的结构。与列表生成式直接生成所有数据不同生成器是记录下列表中元素的规则，动态生成，这样节省了存储空间。定义方法： 元素的生成规则比较简单，可以用列表生成式表示的时候，直接将列表生成式最外层的[]变为()。 调用next(生成器)或者生成器.__next__()时，生成器返回一个值，直到所有值都返回完毕，抛出异常。 生成器是可迭代对象，也可以使用for x in 生成器遍历。 元素的生成规则比较复杂时（如斐波那契数列），可以使用定义函数形式的生成器：只要在在函数中使用yield value，在调用next执行生成器内的代码时，每次执行到yield语句，生成器就会中断，并返回出value。下一次调用next时，会从中断处继续执行。 res = yeild value语句的返回值默认是None，即res == None。在外部调用生成器.send(x)时，会先将res置为x，再执行代码。可以用这种方法来进行通信。 也就是说，生成器.send(None)和生成器.__next__()是等价的。另外，在第一次获取元素时，send的参数必须是None（因为没有yield语句的返回值可以让你更改） 模块&amp;库 导入：import 模块名 as 缩写名，单独导入部分模块：from 模块名 import 子模块/子类/子函数 导入自己的模块： 在模块中添加main()函数，并加入以下语句： 12if __name == '__main__': main() 这表示当该程序直接启动时，执行main()，作为模块导入该程序时，不执行main() 在模块中定义__all__=[x1, x2, x3....]列表，使得在__all__中的变量才有权限被外部访问。 在含有多个模块的文件夹下添加__init__.py文件（空文件即可），使得该文件夹成为一个包。 高阶函数 函数本身可以赋值给变量（类似指向函数的指针），函数名也是变量。函数也可以接受另一个函数名作为参数。函数名的数据类型实际上是function类。 python的高级操作： Map函数： 可以迭代使用函数。函数原型为：map(函数名f，可迭代对象a, 可迭代对象b...)，map的作用是多次调用f，a，b...是多个可迭代对象，每一组a[i], b[i]...构成传给f的参数，如果a,b...不等长，会自动截断，map会返回由各个返回值构成的可迭代对象c。 Reduce函数： 可以递归使用函数。函数原型为reduce(函数名f，可迭代对象a)，假设a=[x1,x2,x3,x4]，那么reduce的作用相当于:f(f(f(x1, x2), x3), x4) 匿名函数： 格式为(lambda x: 关于x的表达式)，例如(lambda x: x*x)，一般用于只使用一次的函数，以及担心函数名冲突的时候。可以把匿名函数表达式赋值给变量，然后通过该变量来调用函数。 Filter函数： 用于过滤序列，函数原型为filter(函数名f,序列a)，filter将传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。 闭包 即一个函数返回的，是它内部定义的另一个函数。 1234567def f(x): y = 10 def g(x): print(x+y) return gfunc = f(5)# [out] 15 要在内层函数中修改外部变量时，要求被修改的变量必须是可变类型（如list） 多次执行闭包时，内层函数的内存空间一直不被释放，因此每一次迭代使用的变量都是同一个（会有累加效应） 作用：外层函数可以参数，并传给内层函数，即外层函数可以根据接受到的不同参数，来定制返回的函数。在需要许多相似结构的函数时，可以用这种方法来简化代码。 装饰器 一种不需要修改函数的源代码就可以为函数增加功能的高级方法，要使用到闭包。假设要给函数func增加功能，装饰器的做法是：写一个闭包closure，传入func，并把要增加的功能作为内函数，然后在原来func的定义前一行加入@closure即可： 12345678def closure(func): def wrapper(*args, **kargs): # 参数应该包含func的参数，或者也用*args, **kargs来应对任意形式的参数传递 [要增加的功能] return func(*args, **kargs) # 这里的参数应该与func原来的参数完全相同，或者使用*args, **kargs来应对任意形式的参数传递 return wrapper、@closure def func(): [函数定义] 文件I/O Open函数可以打开一个文件（同c语言）。函数原型为open(‘文件路径’,’打开方式’)。打开方式(只读，只写)等同c语言）open函数的返回值可以理解为c语言中的文件指针。对p: 有p.write(‘字符串’)方法，在文件末尾添加文字； p.close()方法以关闭文件。 P.read()可以读取到对应文件的内容，该方法的返回值是包含文件所有内容的字符串。 使用p.readline()进行逐行读取。第i次使用该函数读取到的内容是文件的第i行。 使用p.readlines()可以读取所有行。该函数的返回值是一个list，该；list的每个元素也是list，包含每一行的内容。 面向对象(OOP) 定义一个类： 123456class Class1(object): #要求首字母大写且要有冒号。 Name = ....... Value = ....... ...... #类的属性。 def method(self, a, b): [我是代码] 其中object表示该类是从哪个类继承下来的。类的定义中可以有def语句定义函数。在创建该类的一个实例时（对象） 常用操作： 定义__init__(self)方法：在定义对象时进行的初始化方法。 定义__str__(self)方法：在打印该类时，会打印该方法的返回值。 定义__iter__(self)方法：需要返回一个可迭代对象，定义该方法可以使得该类可以被迭代。 定义__getitem__(self, ix)方法：该方法传入一个索引（整型)，返回一个值。定义该方法后，可以使得该类使用下标索引。 定义__getattr__(self, method)方法：当对象调用不存在的方法时，会试图调用该方法，有机会避免报错。 定义__slot__ = ['name1', 'name2'...]属性，是一个列表。定义后，该类不能添加__slot__列表以外的方法。 要注意__slot__对该类的子类没有作用。 使用@property， 把一个方法变成属性： dd 多进程&amp;多线程 多线程 需要导入threading模块 threading.active_count()可以返回当前的活跃线程数目 threading.current_thread()查看正在运行的线程。 使用多线程 添加线程thread = threading.Thread(target = func, args = (args))，func是该线程要运行的工作。 为线程添加功能：定义上面的func函数，参数args是由要传给func的参数组成的可迭代对象。 所以只有一个参数时，为了保证传入可迭代对象，使用args=(a, )。 如果函数需要返回值，不能直接返回，一般使用队列(queue)的方法来保存多线程的返回值。 队列对象使用queue.Queue封装 使用queue.put(x)来把x放入队列 使用queue.get(x)来输出队列中的内容 使用队列的多线程如下： 1234567891011from queue import Queuedef func(queue): [我是代码] queue.put(返回值)queue = Queue()for _ in range(n): t = threading.Thread(target=func, args=queue) t.start()print(queue.get()) 运行线程thread.start()。 调度 正常情况下，各个多线程时同时工作的。 使用thread1.join()语句，将thread1加入主线程，即主线程运行到该语句处，会等待thread1完成任务，主线程再运行之后的代码。 使用锁控制线程的运行顺序lock = threading.Lock()，在线程对应的函数中，使用: 123456lock = threading.Lock()def func(): global lock lock.acquire() [我是代码] lock.release() 多进程 需要multiprocessing模块，import multiprocessing as mp 创建进程process = mp.Process(target=func, args=args)，参数含义和多线程相同。 func的返回值也需要队列来完成。这里使用的队列直接使用multiprocessing中的队列对象，即queue = mp.Queue()，其他操作与多线程相同。 这里的参数 开始进程process.start() 加入主进程process.join() 注意：多进程必须在if __name__ == '__main__'下运行。 全局锁 多进程执行中，如果有需要多个之间共享的变量，需要使用mp.Value()封装。具体细节见文档。 如果不加锁，各个进程之间使用共享内存会起冲突。建立一个全局锁：l = mp.Lock()。使用全局锁就是在某个进程对应的func中按如下所示： 123456l = threading.Lock()def func(): global l l.acquire() [我是代码] l.release() 进程池Pool 普通的多进程操作：创建进程-&gt;运行进程-&gt;加入主进程-&gt;使用队列得到返回值 使用进程池，把要运行的任务放入池中，可以自动实现多进程计算. 创建进程池：pool = mp.Pool(processes=n)，processes表示进程数，默认和cpu核心数相等 把函数放入池： 如果需要重复执行某个函数，并且该函数只有一个参数，可以使用return_value = pool.map(func, 可迭代对象)。这里的可迭代对象是由函数参数构成的（与python中的map函数类似） 如果只执行一次函数，但函数有多个参数，也可以使用异步非阻塞运行：res = pool.apply(func, args=(args1, args2...))，args是由各个参数组成的 可迭代对象。 一般使用效率更高的map和apply异步方法res = pool.map_async()和res = pool.apply_async()。除了名字不同以外，参数传递完全相同。异步方法不能直接获取返回值，需要用res.get() 来获取。 未完待续….以后慢慢填坑叭 0v0 by WHY,2018.4.10]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>编程</tag>
      </tags>
  </entry>
</search>
